[
    {
        "summary": "Pull Request '25802' titled 'better error message when lowering a primitive with a custom_partition rule' was authored by a User, who is associated as a NONE. \nIt was created at 2025-01-09T12:38:22Z, and was closed at 2025-01-09T15:56:08Z by a User.\nIt has a body of 'Hello,\r\n\r\nA quick PR to improve an error message I got when I was using `custom_partitionning`\r\n\r\nHere is a MWE\r\n\r\n```py\r\nimport os\r\n\r\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\r\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\r\n\r\nfrom jax import core, lax\r\nimport jax.numpy as jnp\r\nimport jax\r\nfrom jax.experimental.custom_partitioning import custom_partitioning\r\nfrom jax.interpreters import mlir\r\nimport jax.extend as jex\r\n\r\nfrom jax.sharding import NamedSharding\r\nfrom jax.sharding import PartitionSpec as P\r\n\r\npdims = (8,)\r\nmesh = jax.make_mesh(pdims, axis_names=(\"x\"))\r\nsharding = NamedSharding(mesh, P(\"x\"))\r\n\r\n# ================================\r\n# Double Primitive Rules\r\n# ================================\r\n\r\n# Step 1: Define the Primitive\r\ndouble_prim_p = jex.core.Primitive(\"double_prim\")\r\n\r\n\r\n# dispatch.prim_requires_devices_during_lowering.add(double_prim_p)\r\n# Step 2: Define the Implementation\r\n@custom_partitioning\r\ndef double_prim_impl(x):\r\n    return 2 * x  # Linear operation\r\n\r\n\r\ndef infer_sharding_from_operands(mesh, arg_infos, result_infos):\r\n    return arg_infos[0].sharding\r\n\r\n\r\ndef partition(mesh, arg_infos, result_infos):\r\n    input_sharding = arg_infos[0].sharding\r\n    output_sharding = result_infos.sharding\r\n    input_mesh = input_sharding.mesh\r\n\r\n    def impl(operand):\r\n        return 2 * operand\r\n\r\n    return input_mesh, impl, output_sharding, (input_sharding,)\r\n\r\n\r\n# Step 3: Define Abstract Evaluation\r\ndef double_prim_abstract_eval(x):\r\n    return core.ShapedArray(x.shape, x.dtype)\r\n\r\n\r\n# Step 4: Register the Primitive\r\ndouble_prim_p.def_impl(double_prim_impl)  # Implementation\r\ndouble_prim_p.def_abstract_eval(double_prim_abstract_eval)  # Abstract Eval\r\ndouble_prim_impl.def_partition(\r\n    infer_sharding_from_operands=infer_sharding_from_operands, partition=partition\r\n)\r\nmlir.register_lowering(\r\n    double_prim_p, mlir.lower_fun(double_prim_impl, multiple_results=False)\r\n)  # Lowering\r\n\r\n\r\n# Define a Python wrapper for the primitive\r\n@jax.jit\r\ndef double_prim_call(x):\r\n    return double_prim_p.bind(x)\r\n\r\n# Test Forward Computation\r\nx = jnp.arange(8).astype(jnp.float32)\r\nx = lax.with_sharding_constraint(x, sharding)\r\nprint(\"Double Primitive Forward:\\n\", double_prim_call(x))\r\n```\r\n\r\nI get this error message \r\n`Please file a bug at https://github.com/jax-ml/jax/issues`\r\n\r\nIf I uncomment this line \r\nit is fixed\r\n\r\n`dispatch.prim_requires_devices_during_lowering.add(double_prim_p)`\r\n\r\nI added a more explicit error message.'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/jax-ml/jax/pull/25802/checks?check_run_id=35370052235) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2025-01-09T12:38:26Z\n'Thanks for this! But, I don't think that this is quite the right change. This new error message would expose JAX internal APIs which we definitely don't want to do. I would argue that this is actually really a misuse of `custom_partitioning`, so I'd say that the error message is actually perfect :D \r\n\r\nHere's how I would recommend refactoring the code. Instead of wrapping your primitive impl in `custom_partitioning`, move that to `double_prim_call`:\r\n\r\n```diff\r\n...\r\n\r\n# Step 2: Define the Implementation\r\n- @custom_partitioning\r\ndef double_prim_impl(x):\r\n    return 2 * x  # Linear operation\r\n\r\n...\r\n\r\n+ @custom_partitioning\r\ndef double_prim_call(x):\r\n    return double_prim_p.bind(x)\r\n\r\n...\r\n\r\n+ double_prim_call.def_partition(\r\n+     infer_sharding_from_operands=infer_sharding_from_operands, partition=partition\r\n+ )\r\n```\r\n\r\nAnd then everything works as expected!\r\n\r\nHope this helps.' by a COLLABORATOR of type User on 2025-01-09T13:35:49Z\n'Thank you for your answer\r\n\r\nThe problem is custom_partition lowering is not vmappable nor differentiable\r\nthis is an example of what you suggest\r\n\r\n```py\r\nimport os\r\nos.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\r\nos.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\r\n\r\nfrom jax import core, lax\r\nfrom jax.interpreters import mlir, ad, batching\r\nimport jax.numpy as jnp\r\nimport jax\r\nfrom jax.experimental.custom_partitioning import custom_partitioning\r\nfrom jax._src import dispatch\r\nimport jax.extend as jex\r\n\r\nfrom jax.sharding import NamedSharding\r\nfrom jax.sharding import PartitionSpec as P\r\nfrom functools import partial\r\n\r\npdims = (8,)\r\nmesh = jax.make_mesh(pdims , axis_names=('x'))\r\nsharding = NamedSharding(mesh, P('x'))\r\n\r\n# ================================\r\n# Double Primitive Rules\r\n# ================================\r\n\r\n# Step 1: Define the Primitive\r\ndouble_prim_p = jex.core.Primitive(\"double_prim\")\r\ndispatch.prim_requires_devices_during_lowering.add(double_prim_p)\r\n# Step 2: Define the Implementation\r\n#@custom_partitioning\r\ndef double_prim_impl(x):\r\n    return 2 * x  # Linear operation\r\n\r\ndef infer_sharding_from_operands(mesh , arg_infos , result_infos):\r\n    return arg_infos[0].sharding\r\n\r\ndef partition(mesh , arg_infos , result_infos):\r\n\r\n    input_sharding = arg_infos[0].sharding\r\n    output_sharding = result_infos.sharding\r\n    input_mesh = input_sharding.mesh\r\n\r\n    def impl(operand):\r\n        return 2 * operand\r\n\r\n    return input_mesh , impl , output_sharding , (input_sharding,)    \r\n\r\n@partial(custom_partitioning , static_argnums=(1,))\r\ndef vmapped_double_prim_impl(x, batch_dims):\r\n    return jax.vmap(lambda x: 2 * x , in_axes=batch_dims)(x)\r\n\r\ndef v_infer_sharding_from_operands(batch_dims , mesh , arg_infos , result_infos):\r\n    return arg_infos[0].sharding\r\n\r\ndef v_partition(batch_dims , mesh , arg_infos , result_infos):\r\n    input_sharding = arg_infos[0].sharding\r\n    output_sharding = result_infos.sharding\r\n    input_mesh = input_sharding.mesh\r\n\r\n    def impl(operand):\r\n        return jax.vmap(lambda x: 2 * x , in_axes=batch_dims)(operand)\r\n\r\n    return input_mesh , impl , output_sharding , (input_sharding,)\r\n\r\n\r\nvmapped_double_prim_impl.def_partition(infer_sharding_from_operands=v_infer_sharding_from_operands, partition=v_partition)\r\n\r\n# Step 3: Define Abstract Evaluation\r\ndef double_prim_abstract_eval(x):\r\n    return core.ShapedArray(x.shape, x.dtype)\r\n\r\n# Step 4: Define JVP Rule\r\ndef double_prim_jvp_rule(primals, tangents):\r\n    x, = primals\r\n    t, = tangents\r\n\r\n    # Forward computation\r\n    primal_out = double_prim_call(x)\r\n\r\n    # Tangent computation (reuse the primitive itself)\r\n    tangent_out = double_prim_call(t)\r\n    return primal_out, tangent_out\r\n\r\n# Step 5: Define Transpose Rule\r\ndef double_prim_transpose_rule(ct_out, x):\r\n    ct_x = 2*ct_out if ad.is_undefined_primal(x) else None\r\n    return ct_x ,\r\n\r\n# Step 6: Define Batch Rule\r\ndef double_prim_batch_rule(batched_args, batch_dims):\r\n    x, = batched_args\r\n    bx, = batch_dims\r\n    # Apply vmapped double operation\r\n    res = vmapped_double_prim_impl(x, bx)\r\n    return res, 0\r\n\r\n# Step 7: Register the Primitive\r\ndouble_prim_p.def_impl(double_prim_impl)  # Implementation\r\ndouble_prim_p.def_abstract_eval(double_prim_abstract_eval)  # Abstract Eval\r\nmlir.register_lowering(double_prim_p, mlir.lower_fun(double_prim_impl, multiple_results=False))  # Lowering\r\nad.primitive_jvps[double_prim_p] = double_prim_jvp_rule  # JVP Rule\r\nad.primitive_transposes[double_prim_p] = double_prim_transpose_rule  # Transpose Rule\r\nbatching.primitive_batchers[double_prim_p] = double_prim_batch_rule  # Batch Rule\r\n\r\n\r\n# Define a Python wrapper for the primitive\r\n@custom_partitioning\r\ndef double_prim_call(x):\r\n    return double_prim_p.bind(x)\r\n\r\ndouble_prim_call.def_partition(infer_sharding_from_operands=infer_sharding_from_operands, partition=partition)\r\n\r\n# ================================\r\n# Linear Double Primitive Testing\r\n# ================================\r\n\r\n# Test Forward Computation\r\nx = jnp.arange(8).astype(jnp.float32)\r\nx = lax.with_sharding_constraint(x, sharding)\r\nprint(\"Double Primitive Forward:\\n\", double_prim_call(x))\r\n\r\n# Test Reverse-Mode Autodiff\r\nprint(\"Double Primitive Grad:\\n\", jax.jacrev(double_prim_call)(x))\r\nprint(\"Double Primitive VJP:\\n\", jax.vjp(double_prim_call, x)[0])\r\n\r\n## Test Forward-Mode Autodiff \r\n# print(f\"Double Primitive Forward diff:\\n\", jax.jacfwd(double_prim_call)(x)) THIS IS CRASHING IN ALL CASES BECAUSE THE SHARDING IS NOT PROPAGATED CORRECTLY IN JACFWD\r\nprint(\"Double Primitive jvp:\\n\", jax.jvp(double_prim_call, (x,),( jnp.ones_like(x),)))\r\n\r\n# Test Batch Rule\r\nbatched_x = jnp.stack([x, 2* x])\r\nprint(\"Double Primitive Batched:\\n\",jax.vmap(double_prim_call, in_axes=0)(batched_x))\r\n```\r\n\r\nYou can see that there is no longer any differentiation rules nor batch rules when wrapping with `custom_partitionning`\r\n\r\nThe whole goal of what I am doing is implementing the batching and diff rules \r\n\r\nA working example is then\r\n```py\r\n@custom_partitioning\r\ndef double_prim_impl(x):\r\n    return 2 * x  # Linear operation\r\n    \r\n@jax.jit\r\ndef double_prim_call(x):\r\n    return double_prim_p.bind(x)\r\n\r\ndouble_prim_impl.def_partition(infer_sharding_from_operands=infer_sharding_from_operands, partition=partition)\r\n```\r\nThe custom_partitinniong needs to wrapped with a primitive that defines all rules\r\n\r\nThere is still no way to do a jacfwd .. the vmap however works the way I did it\r\n\r\nPlease tell me if I am going the wrong way' by a NONE of type User on 2025-01-09T13:51:47Z\n\nPR has review comments:\n'I don't think we want to put private APIs in the error message.\r\n\r\nNow I know that's suboptimal but maybe we can document it in some other way in the docs somewhere' by a COLLABORATOR of type User on 2025-01-09T15:42:58Z\n'It was mentioned in the changelog: https://github.com/jax-ml/jax/blob/main/CHANGELOG.md#jax-0424-feb-6-2024\r\n\r\nmaybe that's enough for now?' by a COLLABORATOR of type User on 2025-01-09T15:47:02Z\n'Ok good for me\r\nI will close this PR' by a NONE of type User on 2025-01-09T15:56:03Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 3,
        "reason_for_closure": "Inconsistent behavior change: PR closed as the proposed change to raise an error was deemed unnecessary and against JAX's principle of extending NumPy functionality."
    },
    {
        "summary": "Pull Request '25143' titled 'Disallow platform aliases for get_topology_desc' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-11-27T08:54:52Z, and was closed at 2024-12-12T07:52:29Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nPR has comments:\n'What is the status? It seem approved, but not merged.' by a COLLABORATOR of type User on 2024-12-04T15:39:30Z\n'Abandoning (since deviceless AOT is already working without this).' by a CONTRIBUTOR of type User on 2024-12-12T07:52:29Z\n\nPR has review comments:\n'`if platform not in expand_platform_alias(platform):`. Avoid using `if not ... in ...` pattern.' by a COLLABORATOR of type User on 2024-11-27T16:42:17Z\n'Done.' by a CONTRIBUTOR of type User on 2024-11-27T18:25:52Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Abandoned: Feature already works without this change."
    },
    {
        "summary": "Pull Request '25107' titled 'Track mapping of platform aliases to compile-only backends' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-11-26T08:48:39Z, and was closed at 2024-12-03T07:58:16Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'The patch tracks the mapping of aliases to compile-only backend platform names. The mapping enables canonicalizing platform names correctly ('gpu' -> 'cuda') when we only have compile-only backends for the platform.'\nPR has comments:\n'This is an alternative to  https://github.com/jax-ml/jax/pull/25033 to address https://github.com/jax-ml/jax/issues/23971.' by a CONTRIBUTOR of type User on 2024-11-26T08:53:54Z\n\nPR has review comments:\n'Do not update config like this without a context manager. Can you set the config just for this test or in setUp and revert it back in tearDown?' by a COLLABORATOR of type User on 2024-11-27T16:46:35Z\n'The function runs in a different process, so it should not matter, no?' by a CONTRIBUTOR of type User on 2024-11-27T18:19:05Z\n'No, pytest can run stuff in the same process which can affect other tests too. Also, it's a good practice in general to scope global updates to what you need.' by a COLLABORATOR of type User on 2024-11-27T18:28:50Z\n'WHy global_config_context? Looks like this should be local?' by a COLLABORATOR of type User on 2024-11-28T16:15:42Z\n'I thought you asked me to use a context manager that will remember the original state of the config and restore it when the test is done. This seems to be exactly what global_config_context is doing: https://github.com/jax-ml/jax/blob/aff7714dc0f49cc0097e4db08e028b68182c8ab9/jax/_src/test_util.py#L1167 (I assumed the name implies that it is temporarily updating the *global* JAX config). Is there some local version of this context manager?' by a CONTRIBUTOR of type User on 2024-12-02T16:53:51Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 5,
        "reason_for_closure": "Inconclusive discussion: Author misunderstood reviewer feedback regarding global vs local config context."
    },
    {
        "summary": "Pull Request '24438' titled 'Alternative abs() formula for `sph_harm()` with certain GPU/CUDA combinations' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2024-10-21T21:04:26Z, and was closed at 2024-11-15T05:25:39Z by a User.\nIt has a body of 'It works around a known ptxas optimization bug, which causes abs() inside array indices to be lost and leads to incorrect clamping of negative indices at 0. The bug causes the `jax.scipy.special.sph_harm` function to produce incorrect results on CUDA GPUs of compute capability 9.0. This issue only affects CUDA Toolkit versions 12.5.0 to 12.6.2 due to a known compiler bug, which has been resolved in subsequent releases.\r\n\r\nMinimal reproducer for the bug (on a C.C. 9.0 device such as H100):\r\n```\r\n$ docker run -it --gpus all --shm-size=1g ghcr.io/nvidia/jax:jax-2024-10-20 bash\r\n# python <<EOF\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax.scipy.special import sph_harm\r\nfrom scipy import special\r\nm = jnp.arange(-3, 3)[:, None]\r\nn = jnp.arange(3, 6)\r\nn_max = 5\r\ntheta = 0.0\r\nphi = jnp.pi\r\nprint(sph_harm(m, n, theta, phi, n_max=n_max))\r\nprint(special.sph_harm(m, n, theta, phi))\r\nEOF\r\n```\r\nexample output:\r\n```\r\n[[ 0.7463527 -0.j -0.84628445+0.j  0.9356027 -0.j]\r\n [-0.7463527 +0.j  0.84628445+0.j -0.9356027 +0.j]\r\n [ 0.7463527 -0.j -0.84628445+0.j  0.9356027 -0.j]\r\n [-0.7463527 -0.j  0.84628445+0.j -0.9356027 -0.j]\r\n [ 0.        +0.j  0.        +0.j  0.        +0.j]\r\n [-0.        -0.j  0.        +0.j  0.        +0.j]]\r\n[[ 0.        -0.j -0.        +0.j -0.        +0.j]\r\n [ 0.        +0.j  0.        +0.j  0.        +0.j]\r\n [-0.        +0.j  0.        -0.j -0.        +0.j]\r\n [-0.74635267+0.j  0.84628438+0.j -0.93560258+0.j]\r\n [ 0.        +0.j -0.        +0.j  0.        +0.j]\r\n [-0.        +0.j  0.        +0.j  0.        +0.j]]\r\n```\r\n\r\n\r\nA distilled the repro which shows that the abs-indices gets incorrectly clamped:\r\n```\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\nA = jnp.arange(10).reshape(5, 2)\r\ni = jnp.arange(-2, 3)\r\nj = jnp.arange(2)\r\nprint(jax.jit(lambda A, i, j: A.at[jnp.abs(i)[:, None], j[None, :]].get(mode='clip'))(A, i, j))\r\n```'\nPR has comments:\n'Closing it as the underlying ptxas bug will be fixed via CUDA 12.6 U3.' by a COLLABORATOR of type User on 2024-11-15T05:25:39Z\n\nPR has review comments:\n'Note this is checking the wrong version. You should be checking the version of `ptxas`, not the version of libcudart. They may be the same, they may not.' by a COLLABORATOR of type User on 2024-10-25T13:36:54Z\n'Is there a jax/jaxlib API somewhere that exposes the ptxas version? If not, what is the recommended way to locate the ptxas binary that XLA uses (assuming if multiple versions of ptxas may co-exist on a system)?' by a COLLABORATOR of type User on 2024-10-28T03:58:25Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Inconsistent behavior :- PR aimed for NumPy consistency but JAX's broader functionality was preferred."
    },
    {
        "summary": "Pull Request '24203' titled 'Drop `complex dtype` support in `jnp.arctan2` to make it consistent with `np.arctan2`' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-10-09T08:03:08Z, and was closed at 2024-10-17T02:14:58Z by a User.\nIt has a body of 'This PR fixes a discrepancy between `jnp.arctan2` and `np.arctan2` by raising a `TypeError` for `complex` inputs, as `np.arctan2` currently does.\r\n\r\nCurrent behavior:\r\n```python\r\n>>> jnp.arctan2(1-2j, 3)\r\nArray(0.4913969-0.6412373j, dtype=complex64, weak_type=True)\r\n```\r\n\r\nNew behavior:\r\n```python\r\n>>> jnp.arctan2(1-2j, 3)\r\nTypeError: ufunc 'arctan2/atan2' does not support complex dtypes.\r\n```'\nPR has comments:\n'Does the function return reasonable results for complex input? If so, I don't think we should do this deprecation, as it would potentially break existing users with very little benefit.' by a COLLABORATOR of type User on 2024-10-09T12:21:18Z\n'@pearu would probably have the best opinion on \"does this function return reasonable results for complex inputs?\".' by a COLLABORATOR of type User on 2024-10-09T14:13:56Z\n'It dispatches to `lax.atan2`, so I suspect the answer is yes for the sake of this discussion. (it's not returning garbage, it's actually attempting a valid computation).\r\n\r\nMy thinking here: in general JAX functionality is a superset of numpy functionality. So just because NumPy returns a TypeError doesn't mean JAX must as well. Does that make sense?' by a COLLABORATOR of type User on 2024-10-09T14:29:37Z\n'Thanks for the clarification @jakevdp. Can I modify this PR to add the docstring for `arctan2`?' by a CONTRIBUTOR of type User on 2024-10-10T05:08:28Z\n'atan2 is implemented in stablehlo, see https://github.com/openxla/stablehlo/blob/main/docs/spec.md#atan2 . Usually, arctan2 for complex inputs is not supported (for example, by numpy, torch, Python array API standard v2023.12, C++ numerics library, etc) as `atan2(y, x)` is associated with the direction angle of the point `(x, y)` in the  Cartesian coordinates. However, numerically, supporting `atan2` for complex inputs does make sense for cases where `x` is complex zero or close to complex zero, and ideally, `atan2(y, x)` should be more accurate than `atan(y / x)`.\r\n\r\nThat said, the current implementation of atan2 on complex inputs in stablehlo is problematic accuracy-wise. For example:\r\n```python\r\n>> x, y = 1+0.00001j, 1-0.00001j\r\n>>> jnp.arctan2(y, x)   # the imaginary part is inaccurate\r\nArray(0.7853981-1.001353e-05j, dtype=complex64, weak_type=True)\r\n>>> jnp.arctan(y / x)   # expected\r\nArray(0.7853982-1.e-05j, dtype=complex64, weak_type=True)\r\n```\r\nand atm I would recommend using `atan(y / x)` instead of `atan2(y, x)` when `x != 0+0j` (I'll add this issue to my todo list).\r\n\r\nOn the other hand, to allow the switch `numpy <-> jax.numpy` in both directions, the current PR makes sense, although, I second @jakevdp point that jax.numpy will likely never be equivalent to `numpy`.' by a COLLABORATOR of type User on 2024-10-11T10:20:42Z\n'I think we should close this PR, because we shouldn't deprecate or raise an error for complex inputs to this function.' by a COLLABORATOR of type User on 2024-10-16T16:13:26Z\n'Thanks! Closing the PR.' by a CONTRIBUTOR of type User on 2024-10-17T02:15:36Z\n\nPR has review comments:\n'Raising an exception does not correspond to deprecation which I would expect to trigger a warning for a few releases. So, I suggest fixing the title of the PR: `Deprecate` -> `Drop` or similar.' by a COLLABORATOR of type User on 2024-10-12T09:43:30Z\n'Thanks! Modified the title.' by a CONTRIBUTOR of type User on 2024-10-16T06:35:55Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 2,
        "reason_for_closure": "Breaking change considered harmful; JAX aims for superset of NumPy functionality, not strict equivalence."
    },
    {
        "summary": "Pull Request '24021' titled '[Mosaic GPU] Query SM and PTX ISA dynamically using driver and LLVM' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-09-30T17:52:07Z, and was closed at 2024-11-06T14:10:27Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Use CUDA driver API to query major and minor compute capabilities, thus arriving at a \"base\" SM string (e.g. `sm_90`).\r\nThen use LLVM to see if we can \"upgrade\" the base SM string to one that enables architecture-specific capabilities (e.g. `sm_90a`).\r\nThen use LLVM to map the SM string to a PTX ISA version that supports the SM.'\nPR has comments:\n'Superseded by #24449.' by a CONTRIBUTOR of type User on 2024-11-06T14:10:27Z\n\nPR has review comments:\n'you should be able to `std::make_pair` to avoid the extra annotations I think' by a COLLABORATOR of type User on 2024-10-01T08:04:05Z\n'Thank you, fixed in the latest push.' by a CONTRIBUTOR of type User on 2024-10-01T14:09:33Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Superseded by another PR (#24449)."
    },
    {
        "summary": "Pull Request '23997' titled '[memories] Enable already-passing ComputeOffload tests on GPU' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-09-30T10:50:17Z, and was closed at 2024-10-03T17:30:27Z by a User.\nIt has a body of 'Enables `ComputeOffload` tests (i.e., tests exercising `@compute_on(...)`) that already happen to pass on GPU, so we don't regress.\r\n\r\ncc @hawkinsp @zhenying-liu @nouiz @yashk2810 '\nPR has comments:\n'@yashk2810 I'm not sure how long these tests have been working -- should we also guard this by an `xla_extension_version` as in https://github.com/jax-ml/jax/pull/23853#discussion_r1777769626 ?' by a CONTRIBUTOR of type User on 2024-09-30T10:55:07Z\n'I don't think these tests should pass on GPU so let's keep it disabled. Any reason you want to enable it?\r\n\r\nAFAIK, there are no passes in XLA that handle this for GPU' by a COLLABORATOR of type User on 2024-09-30T14:44:37Z\n'https://github.com/jax-ml/jax/pull/24011 should move the test so I think you can revert this PR?' by a COLLABORATOR of type User on 2024-09-30T15:41:08Z\n'Sounds good, thanks for the heads-up!' by a CONTRIBUTOR of type User on 2024-10-03T17:30:25Z\n\nPR has review comments:\n'This test has nothing to do with compute offload. I don't know how it got here but I'll move it.' by a COLLABORATOR of type User on 2024-09-30T14:56:53Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 1,
        "reason_for_closure": "Incompatible :- PR closed as the proposed change to match numpy behavior was deemed unnecessary.  JAX generally aims to be a superset of numpy functionality."
    },
    {
        "summary": "Pull Request '23890' titled 'DOC: Improved documentation for jax.numpy.isscalar' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-09-24T22:39:58Z, and was closed at 2024-10-25T17:17:38Z by a User.\nIt has a body of 'Part of https://github.com/jax-ml/jax/issues/21461\r\n\r\n Based on discussion: [19959](https://github.com/jax-ml/jax/discussions/19959)'\nPR has comments:\n'Bullet points rewritten and one example added based on the comments.' by a CONTRIBUTOR of type User on 2024-09-26T00:23:25Z\n'Replaced by #24120' by a COLLABORATOR of type User on 2024-10-25T17:17:38Z\n\nPR has review comments:\n'I think these bullet points are too abstract to be useful. Someone without any background knowledge on NumPy's object structure will have no idea what is meant by \"scalars and zero-dimensional arrays are distinct,\" and what they should expect from `np.isscalar` based on this sentence.' by a COLLABORATOR of type User on 2024-09-24T23:43:14Z\n'Similarly here, I think this will be unhelpful to most people. Most users in my experience, even established researchers who have been using NumPy for years, will think that `np.float32` is a dtype, and won't appreciate that it's a scalar type that can be instantiated to create a scalar object.' by a COLLABORATOR of type User on 2024-09-24T23:44:30Z\n'I'm sorry, but this still seems very muddled and misleading. To me \"plain Python scalars\" means `float`, `int`, or `complex`, but instances of e.g. `np.float32` are also interpreted as scalars.\r\n\r\nThis also seems much longer than it needs to be. We need to make sure the documentation is both accurate and concise.' by a COLLABORATOR of type User on 2024-09-26T12:44:37Z\n'Since the main difference from NumPy is how JAX treats zero-dimensional arrays (which might cause confusion), how about the following statement, or do you have any other suggestions?\r\n\r\n  “JAX version of this function treats zero-dimensional arrays (arrays with no dimensions) as scalars,\r\n  unlike NumPy, which distinguishes between scalars and zero-dimensional arrays.”' by a CONTRIBUTOR of type User on 2024-09-26T22:12:38Z\n'It's not the case that JAX treats zero-dimensional arrays as scalars; for example:\r\n```python\r\n>>> x = jnp.zeros(())\r\n>>> x.ndim\r\n0\r\n>>> jnp.isscalar(x)\r\nFalse\r\n```' by a COLLABORATOR of type User on 2024-09-26T23:16:55Z\n'Yes, in such cases it's not.\r\n\r\nHow about modifying the examples as below and leave out the description?\r\n\r\n```\r\n  Examples:\r\n    >>> jnp.isscalar(1.1)\r\n    True\r\n\r\n    For JAX arrays:\r\n\r\n    >>> jnp.isscalar(jnp.array(1.1))\r\n    True\r\n    >>> jnp.isscalar(jnp.array([1.1]))\r\n    False\r\n\r\n    For ``bool`` and ``string``:\r\n\r\n    >>> jnp.isscalar(False)\r\n    True\r\n    >>> jnp.isscalar('jax')\r\n    True\r\n\r\n    For JAX-specific numeric types:\r\n\r\n    >>> jnp.isscalar(jnp.float32(1.1))\r\n    False\r\n    >>> jnp.isscalar(jnp.complex64(1.1))\r\n    False\r\n```\r\n\r\nDo you have any other suggestion?' by a CONTRIBUTOR of type User on 2024-10-03T20:50:32Z\n'I don't have any great suggestions to be honest – the lack of clarity here maybe implies that we should change the semantics of `jnp.isscalar`. But the semantics of NumPy's version are subtle as well, for example:\r\n```python\r\n>>> np.array(0)\r\narray(0)\r\n>>> np.isscalar(np.array(0))\r\nFalse\r\n>>> np.array([0])[0]\r\nnp.int64(0)\r\n>>> np.isscalar(np.array([0])[0])\r\nTrue\r\n```\r\nThis is because `numpy` distinguishes between scalars and zero-dimensional arrays, whereas in JAX the two are identical. So for JAX it's impossible to replicate NumPy's semantics, but the conventions used in `jnp.isscalar` are probably more confusing than is warranted.' by a COLLABORATOR of type User on 2024-10-03T21:13:50Z\n'Maybe we should redefine the function to be basically this:\r\n```python\r\ndef isscalar(val):\r\n  check_arraylike(\"isscalar\", val)\r\n  return jnp.asarray(val).ndim == 0\r\n```\r\nThat behavior is much simpler to document, and it would be a huge improvement in understandability over the current version. I wonder if anything would break?' by a COLLABORATOR of type User on 2024-10-03T21:24:05Z\n'Yes, I totally agree! It would be clearer and more consistent to change the semantics of `jnp.isscalar`.\r\n\r\nIndeed, in the example below, since the current `jnp.isscalar(jnp.float32(1.1))` returns `False`. This causes the `if jnp.isscalar(phi)` block inside the [`jax.scipy.special.sph_harm`](https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.special.sph_harm.html) function to be skipped.\r\n\r\n```Python\r\nimport jax.numpy as jnp\r\nfrom jax.scipy.special import sph_harm \r\n\r\nm_values = jnp.array([0, 1, 2])\r\nn_values = jnp.array([1, 1, 2])\r\ntheta_values = jnp.array([0.0, jnp.pi/2, jnp.pi])  \r\n# phi_values = jnp.array([jnp.float32(1.1)]) # # This will work\r\nphi_values = jnp.array(jnp.float32(1.1)) # This will cause IndexError: tuple index out of range\r\n\r\nresult = sph_harm(m_values, n_values, theta_values, phi_values) \r\n```' by a CONTRIBUTOR of type User on 2024-10-04T00:19:06Z\n'Shall we change the code semantics in this PR or in a different PR?\r\n\r\nI haven't found any instances within the codebase where the new `jnp.isscalar` semantics would cause breakage.\r\n' by a CONTRIBUTOR of type User on 2024-10-04T00:26:16Z\n'I made the change and ran some internal tests overnight; I found a few downstream breakages that are now fixed. https://github.com/jax-ml/jax/pull/24120 is the version that ended up working.' by a COLLABORATOR of type User on 2024-10-04T13:28:37Z\n'Looks great, Thanks!' by a CONTRIBUTOR of type User on 2024-10-04T17:55:20Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 12,
        "reason_for_closure": "Superseded :- PR closed in favor of another change which addressed the issue differently."
    },
    {
        "summary": "Pull Request '23711' titled 'Disable remat pass' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-09-18T02:04:27Z, and was closed at 2024-09-18T20:50:54Z by a User.\nIt has a body of '--xla_disable_hlo_passes=rematerialization is a flag that is frequently passed to the compiler. This patch adds a configuratin option in JAX to pass that flag by default'\nPR has comments:\n'@nouiz ' by a CONTRIBUTOR of type User on 2024-09-18T02:04:55Z\n'Follow up in https://github.com/google/jax/pull/23738' by a COLLABORATOR of type User on 2024-09-19T14:18:29Z\n\nPR has review comments:\n'It is simpler to understand if all is implemented as enabling that pass.\r\nSo maybe _DISABLE_COMPILER_REMAT_OPTIMIZATION_PASS -> _ENABLE_COMPILER_REMAT_OPTIMIZATION_PASS\r\nAnd set the default to False? If people want to optimize it, they set it to True?' by a COLLABORATOR of type User on 2024-09-18T13:32:43Z\n'I personally prefer when the description is useful for users that have no knowledge at all of that.\r\nWhat about something like:\r\n```\r\nAllow to enable the automatic rematerialization pass. This avoid to pass `--xla_disable_hlo_passes=rematerialization` to XLA. If you have OOM, this will automatically try to trade-off memory usage with extra computation. You can frequently get better results by doing it manually with jax.checkpoint.\r\n```\r\n\r\nDon't hesitate to re-write, I just wanted to give an example of what can be added, so new user don't need to ask any questions.' by a COLLABORATOR of type User on 2024-09-18T13:37:42Z\n'Does JAX has a documentation about that talk about OOM? If so, maybe add the config there?' by a COLLABORATOR of type User on 2024-09-18T13:38:15Z\n'Do you mean a config option related to OOMs?' by a CONTRIBUTOR of type User on 2024-09-18T17:06:04Z\n'Maybe add the new option in that page: https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\r\nThis way, more users will find it?' by a COLLABORATOR of type User on 2024-09-18T18:30:10Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 5,
        "reason_for_closure": "Inconclusive: PR closed after discussion and alternate solutions.  PR deemed unnecessary because JAX should offer more than NumPy."
    },
    {
        "summary": "Pull Request '23627' titled 'Add a \"broadcasting vmap\" helper to custom_batching.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2024-09-13T13:24:54Z, and was closed at 2024-10-10T14:52:22Z by a User.\nIt has a body of 'It has come up a few times (most recently in https://github.com/google/jax/issues/23624) that the \"vectorized\" behavior of `pure_callback` and `ffi_call` is confusing. I'm working on improving that, but in the meantime, it seems like it would be useful to provide a `broadcasting_vmap` similar to the `sequential_vmap` helper that we currently have for vmapping with a `lax.map`.'\nPR has comments:\n'It should also respect the `in_axes` and `out_axes` of `vmap` for my use case. I don't manually construct the vmaps in my case, it's a function that nests vmaps and scans as specified by a signature. https://github.com/Joshuaalbert/DSA2000-Cal/blob/joshs-working-branch/dsa2000_cal/dsa2000_cal/common/jax_utils.py#L395 ' by a CONTRIBUTOR of type User on 2024-09-13T13:36:43Z\n'What do you mean by \"respect the in_axes and out_axes\"?\r\n\r\nIt might make more sense to have this conversation in https://github.com/google/jax/issues/23624 regardless. If the example code I provided over there doesn't do what you want, please explain exactly what behavior you would expect!' by a COLLABORATOR of type User on 2024-09-13T13:41:10Z\n'I mean this should pass.\r\n\r\n```python\r\n@partial(jax.vmap, in_axes=(0, None, None))\r\n@partial(jax.vmap, in_axes=(None, 0, None))\r\ndef cb_vec(x, y, z):\r\n    def add(x, y, z):\r\n        assert x.shape == (4, 5)\r\n        assert y.shape == (4, 5)\r\n        assert z.shape == ()\r\n        return x + y + z\r\n    return jax.pure_callback(add, jax.ShapeDtypeStruct(shape=x.shape, dtype=x.dtype), x, y, z, vectorized=True)\r\n\r\n\r\nif __name__ == '__main__':\r\n    x = jnp.arange(4, dtype=jnp.float32)\r\n    y = jnp.arange(5, dtype=jnp.float32)\r\n    z = jnp.array(1, dtype=jnp.float32)\r\n\r\n    assert cb_vec(x, y, z).shape == (4, 5)\r\n```' by a CONTRIBUTOR of type User on 2024-09-13T14:13:52Z\n'Closing in favor of: https://github.com/jax-ml/jax/pull/23881' by a COLLABORATOR of type User on 2024-10-10T14:52:22Z\n\nPR has review comments:\n'Test with `expand=False`?' by a MEMBER of type User on 2024-09-13T16:42:06Z\n'Yes - on it! But, importantly, I'm not sure I love that name \"expand\". Can you think of something that would better describe the behavior? I also considered `full_arrays` or `ufunc` (which means the inverse).' by a COLLABORATOR of type User on 2024-09-13T16:44:27Z\n'\"full\"?' by a MEMBER of type User on 2024-09-13T17:48:49Z\n'why expose this? Every new API that we add has a cost. Please consider not exposing it or do it via jex.' by a COLLABORATOR of type User on 2024-09-13T17:51:44Z\n'I don't think sticking this alone in `jex` is right, but the rest of your point stands.\r\n\r\nStepping back, all of `custom_batching` is part of an effort to expose (effectively) a coherent custom primitive creation interface. It's still up for debate whether that's best done by:\r\n* instantiating something called a custom primitive; or\r\n* attaching custom derivatives, custom batching, custom etc. to a Python callable; or\r\n* anything else.\r\n\r\nThat overall effort should lead to APIs that either go in `jax.extend` or into `jax`. Note that custom derivatives have long been in `jax`. I think your original question applies to all of `custom_batching` (which is currently in `jax` following custom AD, but not documented as we very slowly \"land and iterate\").\r\n\r\nSeparately, whether to add this particular `broadcasting_vmap` convenience to `custom_batching` is its own question. The reason for it is that the general `custom_vmap` API requires a complex implementation (specifically handling reverse-mode AD). Meanwhile we only know of ~2 concrete use cases, equivalent to `sequential_vmap` and `broadcasting_vmap`, for which AD seems easier to handle. So the idea is to expose _only_ the latter two as the API, _instead_ of a general `custom_vmap` function.' by a MEMBER of type User on 2024-09-13T18:20:58Z\n'Do we need to expose that right now or maybe later when we have full fledged custom_batching support?' by a COLLABORATOR of type User on 2024-09-13T18:24:06Z\n'Great points all around!\r\n\r\nFor the bigger question: I definitely agree that `custom_batching` probably shouldn't be exposed as a top level `jax` API given it's current state of readiness. Maybe it does make sense to migrate it to `jex`, or maybe it's better to just do nothing and wait until we have a better and more complete story around customization.\r\n\r\nAs for this specific function: I do think that it is useful to expose something like this, and it should probably live in the same place as `sequential_vmap`. I think something like `jax.extend.batching` or `jax.extend.custom_batching` could be a reasonable place, and I don't have strong opinions.\r\n\r\nIf the consensus is helpers like this and `sequential_vmap` shouldn't be exposed as part of any API, I'm happy to abandon this PR for `broadcasting_vmap` to just exist as an issue comment. No stress either way!' by a COLLABORATOR of type User on 2024-09-13T19:34:16Z\n'`tile` ?' by a CONTRIBUTOR of type User on 2024-09-14T10:17:54Z\n'I think `convert_to_ufunc` is a clearer name, and is also the driving intention.' by a CONTRIBUTOR of type User on 2024-09-14T10:18:53Z\n'Re: publish in API. People can grab the decorator from the github issue easily enough when they stumble upon this. _But_, downstream I see a lot more people using `pure_callback` especially in the science community. A great deal of untapped JAX potential was released when you enabled JAX to interoperate with preexisting code. So if you care about this segment you should probably endeavour to support that integration with intention. Huge projects are starting to migrate to JAX where underlying simulation software is in some other language, e.g. FORTRAN, which will now get access to ecosystem of ML and probabilistic programming via JAX. Also, worthy other mention is providing pre-allocated output buffers for these simulations might be highly useful.' by a CONTRIBUTOR of type User on 2024-09-14T10:26:29Z\n'Thanks for your comments here @Joshuaalbert! The context here is that the code in `jax.custom_batching` is only about 80% operational and completely undocumented, so it's not clear that _this_ is the right place to provide a \"public\" API like this. (It's also not so obvious that having a function in this undocumented module would have better discoverability than a comment in a GitHub issue.) We have [an official compatibility policy](https://jax.readthedocs.io/en/latest/api_compatibility.html) on all functions that are part of the public API (like this would be), so there's maintenance burden to adding new public functions so we should only do that when we're actually solving the right problem.\r\n\r\nYou're preaching to the choir on the importance of interoperability so please rest assured that that's a high priority even if this specific function doesn't get merged :D ' by a COLLABORATOR of type User on 2024-09-17T19:49:44Z\n'This is great to hear. You know, next week I'll be presenting at MODE conference which this year is basically about using JAX + AD to do experimental design optimisation. If you could drop me a tentative unofficial roadmap for interoperability beween JAX and other codebases that would be really useful. I could include it in my talk. There are many ideas too, like automatic vjp and jvp for linear adjoint pairs of operators. JAXbind is other work focused on this.' by a CONTRIBUTOR of type User on 2024-09-18T11:30:10Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 12,
        "reason_for_closure": "Closed as not useful: PR deprecates functionality without sufficient justification."
    },
    {
        "summary": "Pull Request '23449' titled 'Added `shape` and `dtype` to `AbstractValue`' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2024-09-05T10:48:06Z, and was closed at 2024-09-06T17:35:33Z by a User.\nIt has a body of 'We assume that an `AbstractValue` is a `ShapedArray` in lots of places, and currently most of them generate static type errors, because an `AbstractValue` has neither `shape` nor `dtype` (unlike `ShapedArray`).\r\n\r\nI decided to use `tuple[Any, ...]` instead of `tuple[int, ...]` for the `shape` because dimension are not `int`s in the presence of dynamic shapes.\r\n\r\nWith this\r\n\r\n     python -m pyright jax/_src\r\n\r\nis down to\r\n\r\n     2063 errors, 19 warnings, 0 informations'\nPR has comments:\n'I'd like to hear from @mattjj or @froystig on this. My understanding is that it's intentional that `AbstractValue` has no shape or dtype, and this change seems like it's breaking a core abstraction of the tracing model. Not to mention the fact that having `UnshapedArray` inherit from `AbstractArray`-with-`shape` either makes the name incorrect, or breaks basic OOP assumptions around object attributes and inheritence.\r\n\r\nI wonder if there's a more consistent fix we could use here?' by a COLLABORATOR of type User on 2024-09-05T13:28:14Z\n'This doesn't make sense because we can have non-arraylike types which don't have these attributes, like `AbstractToken` today and `AbstractTuple` in the past/future.\r\n\r\n> We assume that an AbstractValue is a ShapedArray in lots of places\r\n\r\nWe should instead revise those places, as they're mistaken! There are lots of places where we are careful to handle `AbstractValues` in their generality, e.g. `mapped_aval` and other tables-of-handlers.' by a COLLABORATOR of type User on 2024-09-06T16:26:41Z\n'Okay, closing the PR.' by a COLLABORATOR of type User on 2024-09-06T17:35:33Z\n\nPR has review comments:\n'Note that it's important to raise `AttributeError` here so that `hasattr` and `getattr` behave correctly.' by a COLLABORATOR of type User on 2024-09-05T11:04:18Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 1,
        "reason_for_closure": "Inconsistent behavior change: PR closed as it breaks existing users with little benefit and deviates from the superset functionality goal."
    },
    {
        "summary": "Pull Request '23354' titled 'add experimental generic metadata field to dot_general' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2024-08-30T22:50:14Z, and was closed at 2024-09-05T21:03:52Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nPR has comments:\n'Just kidding.' by a COLLABORATOR of type User on 2024-09-05T21:03:52Z\n\nPR has review comments:\n'LGTM' by a NONE of type User on 2024-08-30T23:08:44Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Inconsistent :- PR closed as the change was deemed unnecessary."
    },
    {
        "summary": "Pull Request '23223' titled '[NVIDIA] Reduce number of tests for `jax.nn.dot_product_attention`' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-08-23T21:31:24Z, and was closed at 2024-08-30T17:12:16Z by a Bot.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'While adding the new mask mode, `sliding_window`, I noticed that the number of tests has become quite large. Currently, every time we introduce a new option, it requires testing all possible combinations with existing options, which makes the number of tests increase exponentially. For example, we already have 864 parameterized tests for this API. This PR aims to address this issue by reducing the number of tests through grouping.\r\n\r\nFor the new tests, we categorize them as follows:\r\n\r\n1. **Non-mask tests:** These verify the basic functionality of the API, including data types, `vmap`, groups, etc.\r\n2. **Mask tests:** These cover different masking scenarios, such as causal, padding, or other commonly used combinations.\r\n\r\nAdditionally, we will no longer maintain separate tests for inference and training.'\nPR has comments:\n'@sbodenstein Can you take a look?' by a CONTRIBUTOR of type User on 2024-08-23T21:31:40Z\n\nPR has review comments:\n'How about `uses_cudnn_backend` to reflect that this is a predicate and it will not raise if the condition is not met?' by a COLLABORATOR of type User on 2024-08-28T22:57:25Z\n'Done. PTAL.' by a CONTRIBUTOR of type User on 2024-08-29T20:40:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Intended Behavior Change :- PR reduces the number of tests by grouping them."
    },
    {
        "summary": "Pull Request '22895' titled 'expose docs for custom_vjp.defvjp' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2024-08-06T12:49:43Z, and was closed at 2024-10-10T15:08:03Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of '# Changes\r\n\r\nUses `autoclass` for `custom_vjp` so `defvjp` is properly documented.\r\n'\nPR has comments:\n'This doesn't look quite right to me! `custom_vjp` is now documented inline at https://jax--22895.org.readthedocs.build/en/22895/jax.html#jax.custom_vjp rather than on its own page. I don't think that's what we want, right?' by a COLLABORATOR of type User on 2024-08-12T20:32:47Z\n'I'm doing some housekeeping and I'm going to close this because I think #23069 did the trick. @cgarciae: please re-open if I'm missing something. Thanks!' by a COLLABORATOR of type User on 2024-10-10T15:08:03Z\n\nPR has review comments:\n'Let's make this more specific:\r\n```suggestion\r\nsphinx>=7.3.2,<8.0 # 7.3.0 breaks sphinx-book-theme; 8+ triggers warnings in myst_nb v1.1\r\n```' by a COLLABORATOR of type User on 2024-08-06T16:13:16Z\n'I'm making this change separately in #22896 in order to unbreak doc builds at head.' by a COLLABORATOR of type User on 2024-08-06T16:17:22Z\n'Thanks @jakevdp! Rebased, testing...' by a COLLABORATOR of type User on 2024-08-12T15:29:34Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 3,
        "reason_for_closure": "Superseded :- PR attempts to enforce NumPy behavior, but JAX extends NumPy functionality and shouldn't deprecate existing features."
    },
    {
        "summary": "Pull Request '22719' titled 'Support strided load / store in interpret mode' was authored by a User, who is associated as a MEMBER. \nIt was created at 2024-07-29T13:14:05Z, and was closed at 2024-08-19T16:24:55Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Support strided load / store in interpret mode\r\n\r\nThe aim of the PR is to support this test: https://github.com/google/jax/blob/bb160cf54ef5f69c2a59a4001a3210fd56a7b286/tests/pallas/ops_test.py#L772-L786'\nPR has comments:\n'Will make a better fix instead' by a MEMBER of type User on 2024-08-14T16:07:44Z\n'reopened because I realised that the original code still makes sense' by a MEMBER of type User on 2024-08-19T12:54:29Z\n'Superseded by #23117' by a MEMBER of type User on 2024-08-19T16:24:55Z\n\nPR has review comments:\n'Can we convert any scalar indexer `i` to a single-element slice and then do `squeeze`?' by a COLLABORATOR of type User on 2024-07-29T13:52:24Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 1,
        "reason_for_closure": "Superseded by another PR/fix."
    },
    {
        "summary": "Pull Request '22404' titled '[cuDNN SDPA] fix bias sharding check and dbias calculation' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-07-11T21:03:10Z, and was closed at 2024-07-17T20:07:20Z by a Bot.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of '* only check bias batch/num_head sharding spec if present. Both dims could be broadcast.\r\n* dbias calculation is incorrect in spmd and all_reduce is required.'\nPR has comments:\n'@superbobry could you help review this? Thanks.' by a CONTRIBUTOR of type User on 2024-07-11T21:03:52Z\n'Please squash the commit so that we can merge.' by a COLLABORATOR of type User on 2024-07-17T11:36:21Z\n'Done, thanks!' by a CONTRIBUTOR of type User on 2024-07-17T19:16:36Z\n\nPR has review comments:\n'Could you add a test case for the new condition?' by a COLLABORATOR of type User on 2024-07-15T15:52:23Z\n'Perhaps call `_dot_product_attention_bwd_impl` directly here?' by a COLLABORATOR of type User on 2024-07-15T15:53:06Z\n'I modify the `test_sdpa_broadcast_bias_and_dbias` from single device to sharded test.' by a CONTRIBUTOR of type User on 2024-07-16T21:03:22Z\n'fixed.' by a CONTRIBUTOR of type User on 2024-07-16T21:09:42Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 4,
        "reason_for_closure": "Code Functionality Issue: PR addressed a discrepancy by removing functionality instead of fixing the underlying issue."
    },
    {
        "summary": "Pull Request '22330' titled '[NVIDIA] Remove logic of combining bias and mask' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-07-08T18:34:38Z, and was closed at 2024-07-16T14:19:06Z by a Bot.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'The cudnn API has already supported the combination of bias and mask from [this PR](https://github.com/google/jax/pull/22078). We are removing the logic from the public sdpa API and pass the mask directly.\r\n\r\ncc. @Cjkkkk '\nPR has comments:\n'@superbobry @sbodenstein for review. Thx.' by a CONTRIBUTOR of type User on 2024-07-08T18:36:11Z\n'Gentle ping @superbobry ' by a CONTRIBUTOR of type User on 2024-07-15T17:26:20Z\n'Please remove the `# type: ignore` comment mypy complains about.' by a COLLABORATOR of type User on 2024-07-15T18:36:30Z\n'Please squash the commit, so that we can merge.' by a COLLABORATOR of type User on 2024-07-15T21:29:36Z\n'> Please squash the commit, so that we can merge.\r\n\r\nDone. PTAL.' by a CONTRIBUTOR of type User on 2024-07-15T22:09:32Z\n\nPR has review comments:\n'OOC why do we still need `ignore[arg-type]` here?' by a COLLABORATOR of type User on 2024-07-08T18:52:35Z\n'Sorry, I missed this comment. Right, I don't think we need this anymore. Removed. PTAL.' by a CONTRIBUTOR of type User on 2024-07-15T20:31:58Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as unnecessary.  The PR addressed a NumPy incompatibility that was deemed acceptable in JAX."
    },
    {
        "summary": "Pull Request '22078' titled '[cuDNN SDPA] combine mask with bias to support public SDPA API' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-06-25T06:15:31Z, and was closed at 2024-07-07T21:42:50Z by a Bot.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of '* cuDNN SDPA does not support mask input any more, therefore we combine the bias and mask manually to align with public SDPA API design.'\nPR has comments:\n'@superbobry @kaixih could you help review this? @kaixih could you help make sure it is aligned with public API?' by a CONTRIBUTOR of type User on 2024-06-25T06:18:15Z\n'I think the minimum cuDNN version JAX supports is 9.0. Does it make sense to remove mask entirely from the JAX API, given that?\r\n\r\nCC @hawkinsp ' by a COLLABORATOR of type User on 2024-06-27T16:55:49Z\n'> I think the minimum cuDNN version JAX supports is 9.0. Does it make sense to remove mask entirely from the JAX API, given that?\r\n> \r\n> CC @hawkinsp\r\n\r\nRemoving mask completely should be fine as long as it aligned with the public API.' by a CONTRIBUTOR of type User on 2024-06-28T19:38:52Z\n'Please fix the type checker errors.' by a COLLABORATOR of type User on 2024-06-30T20:28:31Z\n'Thanks, you just need to squash the commits and the PR is good to go.' by a COLLABORATOR of type User on 2024-07-01T17:16:05Z\n'hi, @superbobry is the pr good to go?' by a CONTRIBUTOR of type User on 2024-07-05T17:12:26Z\n\nPR has review comments:\n'Is there a bug we can reference here? If not, can you add more context on why the test is skipped to the message string, please?' by a COLLABORATOR of type User on 2024-06-27T09:55:28Z\n'When can this workaround be removed? Is there a bug you can link to here?' by a COLLABORATOR of type User on 2024-06-27T09:56:12Z\n'Can we use -.7 * finfo.maxn as the previous version of this function did?' by a COLLABORATOR of type User on 2024-06-27T09:56:31Z\n'It looks like the previous version of this function accepted more dtypes. Why is it okay to only allow b?float16 here?' by a COLLABORATOR of type User on 2024-06-27T09:57:01Z\n'cuDNN Flash attention in XLA only support these 2 data type. Fp8 flash attention is not integrated yet in XLA.' by a CONTRIBUTOR of type User on 2024-06-27T16:23:49Z\n'It is still not fixed in cuDNN AFAIK. So when cuDNN subtract two large negative number (softmax for example) will result in nan. There is an internal nvbug but nothing I can link here.' by a CONTRIBUTOR of type User on 2024-06-27T16:25:36Z\n'I tried -.7 * finfo.maxn and it has the nan issue and cuDNN team recommended the -2 << 40 as WAR for now.' by a CONTRIBUTOR of type User on 2024-06-27T16:26:30Z\n'I don't have more info now as I just noticed that with cuDNN 9.2 this test fail with less than 1% value diff larger than 1e05.' by a CONTRIBUTOR of type User on 2024-06-27T16:28:03Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 8,
        "reason_for_closure": "Closed as not useful: PR deprecates existing functionality without sufficient justification."
    },
    {
        "summary": "Pull Request '22006' titled 'better true_divide and divide docs' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-06-20T20:44:43Z, and was closed at 2024-09-03T18:34:15Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'better true_divide and divide docs\r\n\r\npart of https://github.com/google/jax/issues/21461'\nPR has comments:\n'Also, fix the tests by adding `divide` to the aliases listed under `test_lax_numpy_docstrings`' by a COLLABORATOR of type User on 2024-06-21T15:32:47Z\n'There are some conflicts. Can you rebase against the updated main branch?' by a COLLABORATOR of type User on 2024-06-26T19:01:56Z\n\nPR has review comments:\n'No reason to wrap this in `jit` since it's directly calling another jit-compiled function' by a COLLABORATOR of type User on 2024-06-21T15:30:58Z\n'Add a *See Also* section with `floor_divide`' by a COLLABORATOR of type User on 2024-06-21T15:31:24Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Inconsistent with NumPy; dropping complex dtype support breaks existing JAX functionality."
    },
    {
        "summary": "Pull Request '21892' titled 'updating bitwise_right_shift_doc as an alias' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-06-14T20:23:49Z, and was closed at 2024-09-03T18:34:14Z by a User.\nIt has a body of 'Better bitwise_right_shift doc\r\n\r\nPart of https://github.com/google/jax/issues/21461'\nPR has comments:\n'It looks like you synced to an update main branch, then rebased on an older snapshot of the main branch. To fix, you can make sure your local `main` is updated, and do `git rebase main`.' by a COLLABORATOR of type User on 2024-06-24T19:02:57Z\n'Yes, you are correct.. I rebased it and pushed it but it seems like all the commits are still here. It'll probably be faster starting a new PR with the consolidated changes and closing this one, or I can try squashing all these commits (but I think the PR will still be in a weird state?), Any issue w/ just starting a new one?' by a CONTRIBUTOR of type User on 2024-06-24T19:12:01Z\n'I'd keep working on this PR – it should be an easy fix, and if you have multiple versions of the main branch around, the same issue will come up on any other PR you create.' by a COLLABORATOR of type User on 2024-06-24T19:22:32Z\n'Here's what I'd do:\r\n```bash\r\n$ git remote -v  # I'm assuming this is what your git remotes look like\r\norigin\tgit@github.com:pkgoogle/jax.git (fetch)\r\norigin\tgit@github.com:pkgoogle/jax.git (push)\r\nupstream\tgit@github.com:google/jax.git (fetch)\r\nupstream\tgit@github.com:google/jax.git (push)\r\n$ git checkout main\r\n$ git pull upstream main\r\n$ git checkout better_bitwise_right_shift_doc\r\n$ git rebase main\r\n$ git log  # inspect the commits: you should see your commits on top of the `main` tag\r\n$ git push origin +better_bitwise_right_shift_doc\r\n```\r\nMake sure to run each command individually and ensure it runs before moving to the next, because things change if you have uncommitted files, or if there are merge conflicts, etc.' by a COLLABORATOR of type User on 2024-06-24T19:24:48Z\n'nice... seemed to work will squash if tests pass and everything else is good/accepted.' by a CONTRIBUTOR of type User on 2024-06-24T20:28:53Z\n'Looks good, please squash the commits' by a COLLABORATOR of type User on 2024-06-24T20:55:49Z\n'Hey sorry - I don't get alerts about updated pushes unless you comment, so I never saw this. Would you like to re-open and merge this change?' by a COLLABORATOR of type User on 2024-09-03T18:39:29Z\n'No worries, I was cleaning up my notification pollution but I'll reopen this and the other one until they are closed.' by a CONTRIBUTOR of type User on 2024-09-03T21:15:59Z\n\nPR has review comments:\n'Maybe we should `return right_shift(x1, x2)` for simplicity?' by a COLLABORATOR of type User on 2024-06-14T21:13:24Z\n'agreed... is there any side-effects/consequence of having a technically different name passed to promote_args_numeric?' by a CONTRIBUTOR of type User on 2024-06-14T23:09:27Z\n'It just means that if someone passes invalid values the error message will refer to the aliased function' by a COLLABORATOR of type User on 2024-06-14T23:25:55Z\n'It seems the removed line: `@implements(getattr(np, \"bitwise_right_shift\", np.right_shift), module='numpy')` causes the CI test to look for np.bitwise_right_shift instead, which doesn't exist:\r\n\r\n```\r\n_______________ JaxNumpyOperatorTests.testShiftOpAgainstNumpy0 ________________\r\n[gw5] linux -- Python 3.9.19 /opt/hostedtoolcache/Python/3.9.19/x64/bin/python\r\ntests/lax_numpy_operators_test.py:657: in testShiftOpAgainstNumpy\r\n    np_op = getattr(np, op.__name__)\r\n/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/numpy/__init__.py:333: in __getattr__\r\n    raise AttributeError(\"module {!r} has no attribute \"\r\nE   AttributeError: module 'numpy' has no attribute 'bitwise_right_shift'\r\n=========================== short test summary info ============================\r\nFAILED tests/lax_numpy_operators_test.py::JaxNumpyOperatorTests::testShiftOpAgainstNumpy0 - AttributeError: module 'numpy' has no attribute 'bitwise_right_shift'\r\n========== 1 failed, 10655 passed, 17048 skipped in 399.33s (0:06:39) ==========\r\nError: Process completed with exit code 1.\r\n```\r\n\r\nI tried testing it locally:\r\n```sh\r\npython tests/lax_numpy_operators_test.py\r\n```\r\n\r\nbut it's not showing locally:\r\n```\r\nRan 1612 tests in 30.132s\r\n\r\nOK (skipped=2)\r\n```\r\n\r\nIt seems like we need to skip this test for aliases not defined in numpy. Would you happen to know how I can run this locally? or how we can cause the CI/CD to skip this test for this case?' by a CONTRIBUTOR of type User on 2024-06-19T18:32:47Z\n'With pytest:\r\n\r\n```sh\r\npytest tests/lax_numpy_operators_test.py\r\n================================================================================== test session starts ===================================================================================\r\nplatform darwin -- Python 3.9.19, pytest-8.2.2, pluggy-1.5.0\r\nrootdir: /Users/xxxxxxx/my_jax\r\nconfigfile: pyproject.toml\r\ncollected 1612 items                                                                                                                                                                     \r\n\r\ntests/lax_numpy_operators_test.py ................................................................................................................................................ [  8%]\r\n.................................................................................................................................................................................. [ 19%]\r\n.................................................................................................................................................................................. [ 31%]\r\n.................................................................................................................................................................................. [ 42%]\r\n.................................................................................................................................................................................. [ 53%]\r\n.................................................................................................................................................................................. [ 64%]\r\n.................................................................................................................................................................................. [ 75%]\r\n.................................................................................................................................................................................. [ 86%]\r\n.................................................................................................................................................................................. [ 97%]\r\n..............................ss............                                                                                                                                       [100%]\r\n\r\n============================================================================ 1610 passed, 2 skipped in 37.00s ============================================================================\r\n```' by a CONTRIBUTOR of type User on 2024-06-19T18:36:05Z\n'We shouldn't skip the test entirely (we need test coverage!) Instead, maybe we can add a list of aliases in case the numpy function doesn't exist.' by a COLLABORATOR of type User on 2024-06-22T04:09:51Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 6,
        "reason_for_closure": "Inconsistent behavior :- PR tried to match numpy but jax generally extends numpy's functionality."
    },
    {
        "summary": "Pull Request '21801' titled 'Add a flag to disable the checking of debug_info in ``jax.core.Jaxpr``' was authored by a User, who is associated as a NONE. \nIt was created at 2024-06-11T09:58:54Z, and was closed at 2024-06-11T16:48:31Z by a User.\nIt has a body of 'Adding a flag to disable Jaxpr debug_info checking.\r\n\r\nIn the new release of ``jax==0.4.29``, the ``make_jaxpr()`` function is crashed in [``brainstate``](https://github.com/brainpy/brainstate). This is because the arguments to JIT-compiled are dynamically added in ``brainstate``, leading to the length of ``debug_info.arg_names`` (which are given by the functional inputs) is not equal to ``len(invars)``. \r\n\r\nTherefore, I added a flag to disable the debug_info checking when customizing the ``make_jaxpr()`` function. This will provide flexibility for the development of downstream packages. Thanks!\r\n'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/21801/checks?check_run_id=26068828585) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2024-06-11T09:58:57Z\n'Can you share a reproducer we can run, please?' by a COLLABORATOR of type User on 2024-06-11T13:41:50Z\n'Here is an example code to reproduce the error:\r\n\r\n\r\n```python\r\nimport numpy as np\r\n\r\nimport brainstate as bst\r\n\r\nlif = bst.nn.LIF(1)\r\nlif.init_state()\r\n\r\n\r\ndef run(i):\r\n  with bst.environ.context(i=i, t=bst.environ.get_dt()):\r\n    lif.update(20.)\r\n  return lif.V.value\r\n\r\n\r\nindices = np.arange(10000)\r\nvs = bst.transform.for_loop(run, indices)\r\n\r\n```\r\n\r\nThanks. ' by a NONE of type User on 2024-06-11T13:46:16Z\n'make_jaxpr was indeed changed in 0.4.29 but I don't expect to see any mismatches. The behavior of make_jaxpr should be identical. Hence my request for a JAX only reproducer. That might help narrow down if it's a jax bug or a bug in your library.' by a COLLABORATOR of type User on 2024-06-11T16:43:28Z\n'I am going to close this PR. Please open an issue with a minimal repro and the stack trace of the error :)' by a COLLABORATOR of type User on 2024-06-11T16:48:31Z\n\nPR has review comments:\n'I don't think this is a good idea. Please file an issue with a JAX only reproducer and we can look into fixing it.' by a COLLABORATOR of type User on 2024-06-11T13:52:39Z\n'I will try to extract a repro from the above snippet. ' by a COLLABORATOR of type User on 2024-06-11T16:40:34Z\n'So I verified that the reproducer indeed fails with an `AssertionError`, but it's not immediately obvious what the callable being traced is.' by a COLLABORATOR of type User on 2024-06-11T17:54:29Z\n'Can you paste the stack trace please? ' by a COLLABORATOR of type User on 2024-06-11T17:55:31Z\n'```\r\nTraceback (most recent call last):\r\n  File \"/tmp/repro.py\", line 16, in <module>\r\n    vs = bst.transform.for_loop(run, indices)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/brainstate/transform/_control.py\", line 578, in for_loop\r\n    _, ys = scan(_forloop_to_scan_fun(f),\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/brainstate/transform/_control.py\", line 515, in scan\r\n    stateful_fun = StatefulFunction(f).make_jaxpr(init, xs_tree.unflatten(x_avals))\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[...]/brainstate/transform/_make_jaxpr.py\", line 414, in make_jaxpr\r\n    raise e\r\n  File \"[...]/brainstate/transform/_make_jaxpr.py\", line 397, in make_jaxpr\r\n    jaxpr, (out_shapes, state_shapes) = jax.make_jaxpr(\r\n                                        ^^^^^^^^^^^^^^^\r\nAssertionError\r\n```' by a COLLABORATOR of type User on 2024-06-11T21:47:47Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 5,
        "reason_for_closure": "Inconsistent behavior :- PR tried to enforce NumPy behavior where JAX provided a superset."
    },
    {
        "summary": "Pull Request '21371' titled '[NVIDIA] Add new SDPA API to jax.nn' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-05-22T20:34:19Z, and was closed at 2024-07-08T13:16:10Z by a Bot.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Attention plays a crucial role in modern transformer-based models. While there exist various variants, they generally follow the same workflow. Examples include the typical multi-head attention (MHA), global query attention (GQA), and multi-query attention (MQA). Additionally, new implementations like the Flash Attention algorithm aim to enhance the utilization of accelerator devices. For instance, NVIDIA cuDNN supports Flash Attention and, through its API, can result in a 1.3x end-to-end speedup for training large language models based on GPT alone.\r\n\r\nThis PR proposes introducing a new API in the `jax.nn` module to handle attention. It will first try to use the cudnn flash attention execution path when the config is compatible. Otherwise it falls back to a jax implementation. \r\n\r\ncc. @nluehr @Cjkkkk @cliffwoolley \r\n'\nPR has comments:\n'@hawkinsp Can you help find reviewers?' by a CONTRIBUTOR of type User on 2024-05-22T20:35:55Z\n'Pushed a new commit to remove the use of `is_training` for the cudnn flash attention. This is a followup of this merged [PR](https://github.com/google/jax/pull/21430).' by a CONTRIBUTOR of type User on 2024-05-31T18:46:03Z\n'@sharadmv Any updates?' by a CONTRIBUTOR of type User on 2024-06-04T16:56:32Z\n'The API should have an `implementation` option, taking values like `\"xla\"`, `\"cudnn\"`, and `None` (the default, which selects the best algorithm). This list will grow with alternative kernel implementations (Pallas, etc). It is important to be able to select the implementation type:\r\n- `\"cudnn\"` will fail immediately if there is some unsupported shape, which prevents silent reversions to slow code paths.\r\n- Generating serialized models to do inference with on a different device type (eg train on GPU and test on TPU). \r\n\r\nRegarding the names: does cuDNN expose both FlashAttention and non-FlashAttention? Perhaps this should be `\"cudnn_flash\"`? Note that XLA also has different implementations: we could support the low-memory chunked implementation given here (https://arxiv.org/abs/2112.05682) that inspired FlashAttention, and which is closer numerically to FlashAttention than standard attention and has the same memory complexity (maybe `\"xla_chunked\"`? `\"xla_low_memory\"`?). \r\n\r\nAre there any configuration options a user might want to pass to the cuDNN implementation? If so, it could be a string or a cuDNN config dataclass. Eg. in the low-memory XLA case, the chunk size is something a user might want to configure.' by a CONTRIBUTOR of type User on 2024-06-07T14:41:42Z\n'> The API should have an `implementation` option, taking values like `\"xla\"`, `\"cudnn\"`, and `None` (the default, which selects the best algorithm). This list will grow with alternative kernel implementations (Pallas, etc). It is important to be able to select the implementation type:\r\n> \r\n> * `\"cudnn\"` will fail immediately if there is some unsupported shape, which prevents silent reversions to slow code paths.\r\n> * Generating serialized models to do inference with on a different device type (eg train on GPU and test on TPU).\r\n> \r\n> Regarding the names: does cuDNN expose both FlashAttention and non-FlashAttention? Perhaps this should be `\"cudnn_flash\"`? Note that XLA also has different implementations: we could support the low-memory chunked implementation given here (https://arxiv.org/abs/2112.05682) that inspired FlashAttention, and which is closer numerically to FlashAttention than standard attention and has the same memory complexity (maybe `\"xla_chunked\"`? `\"xla_low_memory\"`?).\r\n> \r\n> Are there any configuration options a user might want to pass to the cuDNN implementation? If so, it could be a string or a cuDNN config dataclass. Eg. in the low-memory XLA case, the chunk size is something a user might want to configure.\r\n\r\nSorry, I think I missed this comment. Do you mean sth like:\r\n```python\r\ndef sdpa(..., implementation=None):\r\n  if implementation == 'cudnn':\r\n    cudnn_sdpa() # users expect to fail on error\r\n  elif implementation == 'pallas':\r\n    pallas_sdpa() # this is for the future.\r\n  elif implementation is None:\r\n    # current path of try-except. and will always fall back to `_dot_product_attention_xla`.\r\n```\r\nRe cudnn flash attentions:\r\n(1)  cuDNN used to expose both flash and non-flash attention kernel, but we choose not to use the non-flash anymore. So, the cudnn attention means cudnn flash attention now. And I am ok with the `cudnn`.\r\n(2) We don't need to pass config to cudnn calls and we are trying to hide it from users.' by a CONTRIBUTOR of type User on 2024-06-12T17:43:47Z\n'> Sorry, I think I missed this comment. Do you mean sth like:\r\n\r\nThat looks correct. We have two options here:\r\n1. Have multiple SDPA functions, one per backend/implementation.\r\n2. Have a single API with the `implementation` option. \r\n\r\nThere are pros and cons of each, and some tricky questions. For example:\r\n- How closely do numerics need to match in the super-function to be considered 'the same'? As found in this review, cuDNN with bf16 inputs does not cast the first matmul to BF16 before doing softmax, whilst XLA does. If we choose the cuDNN convention, the XLA implementation will be incredibly memory-inefficient. This might be a significant difference in certain applications (eg. training with one but doing inference with the other on a different device-type). With future Pallas kernels, we can match the numerics. But this might be harder for third-party libraries like cuDNN. We might also do autotuning and choose the best kernel with the `None` option, which becomes problematic with these numerical differences. This is an argument to have separate functions for third-party kernels that JAX has no control over and are largely opaque (hard to see what numerical choices are being made), and only have a super-function for implementations under JAX-control. \r\n- Another argument for separate functions is that the API can be restricted to only the supported features, rather than the most general function imaginable. The current design is makes it hard for users to see what is supported, and limits documentation opportunities. In addition, there are cuDNN specific options (like the philox dropout) unsupported by any other backend, further complicating the API.' by a CONTRIBUTOR of type User on 2024-06-13T14:58:11Z\n'I think the name should be `dot_product_attention` rather than `scaled_dot_product_attention`. Its also more consistent with Flax naming (https://flax.readthedocs.io/en/v0.8.0/api_reference/flax.linen/_autosummary/flax.linen.dot_product_attention.html).' by a CONTRIBUTOR of type User on 2024-06-14T12:37:08Z\n'As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features. ' by a CONTRIBUTOR of type User on 2024-06-24T17:24:13Z\n'Just pushed some new commits for the simplified sdpa. @sbodenstein PTAL.\r\n\r\nAlso talked to @Cjkkkk and he will try to implement the combination of bias and mask in the cudnn dot_product_attention API (as described [here](https://github.com/google/jax/pull/21371/files#diff-e60c5f12cc8bafdc92c096293155b9905b56a9490935e6cf86802f53bbee171fR912) in (1)). When that is in, our logic of preparing bias will be much simpler.' by a CONTRIBUTOR of type User on 2024-06-25T00:04:41Z\n'Pushed a few more changes. PTAL. @sbodenstein ' by a CONTRIBUTOR of type User on 2024-06-28T21:16:20Z\n'Please squash the commits. This will be mergeable as soon as Chris clarifies his comments.' by a COLLABORATOR of type User on 2024-07-05T08:41:52Z\n'> Please squash the commits. This will be mergeable as soon as Chris clarifies his comments.\r\n\r\nSure. Rebased. PTAL. @superbobry ' by a CONTRIBUTOR of type User on 2024-07-05T17:17:20Z\n'@superbobry: I'm happy with the state of it now. Think we can merge.' by a CONTRIBUTOR of type User on 2024-07-05T17:29:11Z\n'Pushed new commits to resolved some failed python lint tests. Btw, can we have the access to add `kokoro:force-run` label to trigger the tests?' by a CONTRIBUTOR of type User on 2024-07-05T19:45:35Z\n'Please squash the commits and we can merge.' by a COLLABORATOR of type User on 2024-07-07T10:38:02Z\n'Done. PTAL. @superbobry ' by a CONTRIBUTOR of type User on 2024-07-07T16:05:22Z\n'I still saw this lint error: ```jax/_src/nn/functions.py:924: error: Argument 4 to \"dot_product_attention\" has incompatible type \"Array | ndarray[Any, Any] | bool_ | number[Any] | int | float | complex | None\"; expected \"Array | None\"  [arg-type]``` But I am a bit confused. I think it refers to the `mask` which I have already converted to Array by `jnp.asarray(mask)` at the beginning in the function. Do you have any advice on this? @superbobry @sbodenstein ' by a CONTRIBUTOR of type User on 2024-07-07T23:02:38Z\n'No worries, I'll resolve this internally.' by a COLLABORATOR of type User on 2024-07-08T09:39:57Z\n'> As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features.\r\n\r\nThanks for adding FA! Is there a timeline to add `dropout` support in the SDPA API? I understand it is on hold due to differences in PRNG implementation. Would it be OK if we expose `dropout_rate` to the API while warning the user on reproducibility if `cudnn` is selected?\r\n\r\nhttps://github.com/google/jax/blob/417fcd574b9f33410ea8eb78ffdea825ad343eee/jax/_src/cudnn/fused_attention_stablehlo.py#L954-L956' by a NONE of type User on 2024-08-16T06:02:55Z\n'> > As discussed offline: lets land the simplest version first, without dropout or other complications. Then progressively add features.\r\n> \r\n> Thanks for adding FA! Is there a timeline to add `dropout` support in the SDPA API? I understand it is on hold due to differences in PRNG implementation. Would it be OK if we expose `dropout_rate` to the API while warning the user on reproducibility if `cudnn` is selected?\r\n> \r\n> https://github.com/google/jax/blob/417fcd574b9f33410ea8eb78ffdea825ad343eee/jax/_src/cudnn/fused_attention_stablehlo.py#L954-L956\r\n\r\nYes, this is on our radar to be implemented. Can we know what types of model you are working on that needs the dropout?' by a CONTRIBUTOR of type User on 2024-08-27T16:33:08Z\n'> Yes, this is on our radar to be implemented. Can we know what types of model you are working on that needs the dropout?\r\n\r\nAttention dropout would help for almost all low-data training regimes. [Detection Transformers](https://arxiv.org/pdf/2005.12872) are one well-known example. \r\n\r\nTorch supports FA dropout (possibly non-deterministic) in their functional [API](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html).\r\n' by a NONE of type User on 2024-08-28T04:44:40Z\n\nPR has review comments:\n'In the transformer engine docs (https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/api/jax.html#transformer_engine.jax.flax.DotProductAttention) for the JAX cuDNN fused attention API, it states \"float32_logits (bool, default = False) – Whether to compute attention logits in float32 for the unfused attention backend. For fused attention backend, the accumulation is always float32 without the perf overhead.\" It would be good to match numerics as closely as possible. What exactly is cuDNN doing here? Is it accumulating in FP32 (say for bf16 inputs) and then keeping it in FP32 to compute softmax? Or does it cast the accumulation to BF16 like this does?' by a CONTRIBUTOR of type User on 2024-06-07T14:48:39Z\n'Do we have an application for the mask function beyond causal masking? This is such a common use-case that I think it should have its own option (`is_causal`. PyTorch has this as well https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). Its ugly to have to pass `mask_fn=jax.nn.SdpaCausalMask()`.' by a CONTRIBUTOR of type User on 2024-06-07T14:54:25Z\n' This name is non-ideal, better to avoid this confusing acronym (`ScaledDotProductAttentionCausalMask` is clearer).' by a CONTRIBUTOR of type User on 2024-06-07T14:54:57Z\n'Quary -> Query.' by a CONTRIBUTOR of type User on 2024-06-07T14:55:58Z\n'Users of this function will very likely want to use an optimized FlashAttention kernel. Yet its extremely opaque to the user exactly what they must do to actually use a fast kernel (what shapes? what API surface of this function are they allowed to use?)' by a CONTRIBUTOR of type User on 2024-06-07T15:05:14Z\n'What is the motivation for this generality? Fast kernels will never be used when you have such custom functions, and this function provides little benefit over writing this yourself in JAX.' by a CONTRIBUTOR of type User on 2024-06-07T15:07:02Z\n'This is confusing. Why not just have a boolean mask?' by a CONTRIBUTOR of type User on 2024-06-07T15:09:33Z\n'This obviously doesn't compose well with the JAX RNG system as it doesn't take a JAX random seed. Could it?' by a CONTRIBUTOR of type User on 2024-06-07T15:28:12Z\n'I would separate out the xla implementation into `_dot_product_attention_xla` or something and call it in the user-facing function.' by a CONTRIBUTOR of type User on 2024-06-07T15:37:40Z\n'Import `dot_product_attention` as `cudnn_dot_product_attention`.' by a CONTRIBUTOR of type User on 2024-06-07T15:46:07Z\n'We need tests for more dtypes. For example for FP32: is cudnn flashattention using TF32 by default to match the JAX implementation? (not sure whether this is even supported)' by a CONTRIBUTOR of type User on 2024-06-07T16:04:20Z\n'Done.' by a CONTRIBUTOR of type User on 2024-06-11T18:34:33Z\n'Done.' by a CONTRIBUTOR of type User on 2024-06-11T18:34:41Z\n'The jax random seed is also an int, like [here](https://github.com/google/jax/blob/5cf52b8215f285e63ecb66c865d72a69b5c0a5f1/jax/_src/random.py#L197-L215). My understanding is that cudnn uses a different prng algo of \"philox\" but jax uses \"threefry\". So, if users choose to use cudnn flash attn + dropout, I would like them to explicitly use this `xxxPhiloxDropout` to be aware of the possible numeric difference. I don't think jax supports philox. But maybe I missed your point. Can you clarify on  how to \"compose well with the JAX RNG system\"?' by a CONTRIBUTOR of type User on 2024-06-11T18:38:51Z\n'Yes, there are more masking types as defined [here](https://github.com/google/jax/blob/ce4a56a1370d32ad36cb068ef5908357895cf187/jax/_src/cudnn/fused_attention_stablehlo.py#L44). Although we can use a separate is_causal for the common case, but does they mean we need to provide knob options to the API for any new mask types?' by a CONTRIBUTOR of type User on 2024-06-11T18:41:20Z\n'Done.' by a CONTRIBUTOR of type User on 2024-06-11T18:41:27Z\n'In attention, we typically use additive masks to make sure the undesired values have a very small value (negative inf) so that the following softmax generate 0s for them. For example, in praxis like [here](https://github.com/google/praxis/blob/bef3beea055c55146811565f5d50c809998298d7/praxis/layers/attentions.py#L2964). Also, just noticed that pytorch API supports both bool mask (multiplicative mask) and float mask (additive mask). So, do you want to to support both?' by a CONTRIBUTOR of type User on 2024-06-11T18:46:43Z\n'The motivation is also from the praxis impl like [here](https://github.com/google/praxis/blob/main/praxis/layers/attentions.py#L3015). Also, I think the design rationale is to make the public API support surface as general as possible. And yes, the fast attn support surface is just a subset of the API at this moment, but we keep trying to expand that gradually.' by a CONTRIBUTOR of type User on 2024-06-11T18:50:16Z\n'Yes, that is a valid concern. The reason we use this try-except block is the fast attn support surface is too complex to show that here and if you check [here](https://github.com/google/jax/blob/main/jax/_src/cudnn/fused_attention_stablehlo.py) and search \"raise\", the restrictions are scattered here and there on the shapes/sharding/cuda/cudnn versions/etc. We would like these restrictions stay on that file because they are specific to cudnn and they might keep changing. But to your question, we try to provide users a readable error string on why flash attn is not used at runtime.' by a CONTRIBUTOR of type User on 2024-06-11T18:58:12Z\n'Yes, we accumulate in fp32 and do the softmax directly on it. We don't do a cast_to BF16 and then cast_back_to FP32 for softmax in flash attention.' by a CONTRIBUTOR of type User on 2024-06-11T20:06:38Z\n'Do you mean put all the following code into a new function `_dot_product_attention_xla`?' by a CONTRIBUTOR of type User on 2024-06-11T20:07:53Z\n'Having a boolean mask allows for optimizations (less data to move around), and allows this function to handle the complexity zeroing out correctly for all dtypes. So if we have an additive fbias + boolean mask, that seems like all cases. So lets do that.' by a CONTRIBUTOR of type User on 2024-06-13T14:15:14Z\n'I think we should be as conservative as possible with features for this JAX function. Easy to add this if needed, but hard to take away. And as mentioned, it provides almost not benefit to users right now (its very easy to implement an XLA attention with custom softmax if a user wants this). So lets remove everything not needed for hooking up cuDNN attention.' by a CONTRIBUTOR of type User on 2024-06-13T14:17:04Z\n'Yes.' by a CONTRIBUTOR of type User on 2024-06-13T14:17:59Z\n'OK so you are using quite different numerics for BF16 in your XLA and cuDNN implementations. The XLA-equivalent-here is to use `preferred_element_type=jnp.float32` in `jnp.einsum`. This has the negative consequence of using much more memory. But having inconsistent numerics might mean that if you train with cuDNN, inference will no longer work due to the downcasted numerics. Is there an option for controlling this in cuDNN, so we can do the XLA-default downcast behaviour by default and users can switch using `preferred_element_type=jnp.float32`?' by a CONTRIBUTOR of type User on 2024-06-13T14:22:38Z\n'I meant: JAX has an API for splitting PRNG keys (you don't work with raw integer seeds directly except seeding the very first `jnp.PRNGKey`). All JAX random functions take either an int or a `jnp.PRNG` object), so this should also I think.' by a CONTRIBUTOR of type User on 2024-06-13T14:26:27Z\n'Currently, you are not throwing an error when flashattention is not used. I think its easy to miss error messages that are printed/logged somewhere. This is related to the idea of having an implementation option, where users can explicitly ask for the cudnn implementation and fail if its not supported (with good errors).' by a CONTRIBUTOR of type User on 2024-06-13T14:40:43Z\n'Masking is one way to implement causal attention, but good implementations avoid implementing it in this way. As it is conceptually distinct from masking, this should be reflected in the API. ' by a CONTRIBUTOR of type User on 2024-06-14T12:58:06Z\n'This further shows the limitations of the causal attention as mask API: this excludes the user using fast causal attention + having a custom array mask (eg to mask per-example padding).' by a CONTRIBUTOR of type User on 2024-06-14T13:04:15Z\n'For the other masking types: I notice they are not supported in your current API yet. I think we should discuss these in a separate PR. But we should get rid of masking_fn for now. ' by a CONTRIBUTOR of type User on 2024-06-14T13:07:26Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 21,
        "num_review_comments": 30,
        "reason_for_closure": "Here are some concise reasons for closure based on the provided examples:\n\n* **Inconsistency/Discrepancy:** PR introduces behavior inconsistent with related APIs/libraries.\n* **Resolved/Redundant:** Issue addressed by other means/upstream changes; PR no longer necessary.\n* **Erroneous/Misguided:** PR's approach is flawed, introduces bugs, or misinterprets the problem.\n* **Miscommunication/Misunderstanding:** Author and reviewers failed to align on required changes.\n* **Lack of Value/Benefit:** PR doesn't offer sufficient benefit or solves a non-issue.\n* **Exposes Internals:** PR risks exposing undesirable internal APIs.\n* **Incomplete/Insufficient:** PR lacks essential features or testing for desired functionality.\n* **Abandoned/Withdrawn:** Author explicitly abandoned or withdrew the PR.\n* **Out of Scope:** PR addresses something outside the project's current goals or scope.\n\n\nIt's important to tailor the reason to the specific situation, but these general categories cover many common closure scenarios."
    },
    {
        "summary": "Pull Request '21363' titled 'Check in a minimal JAX custom call example using XLA FFI' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-05-22T16:06:15Z, and was closed at 2024-06-04T18:13:55Z by a User.\nPR has comments:\n'Going to keep pushing to this PR for now, but will open a new one when it's ready.' by a CONTRIBUTOR of type User on 2024-06-03T20:40:24Z\n\nPR has review comments:\n'I think just the `ffi.h` include is sufficient here?' by a COLLABORATOR of type User on 2024-05-24T17:34:32Z\n'Not crucial, but some of this boilerplate could be abstracted with the [`XLA_FFI_DEFINE_HANDLER` macro](https://github.com/openxla/xla/blob/6d17fd8fbe27b06fc4828dc0c38ecdd8f9ba8573/xla/ffi/api/api.h#L1602). This is the syntax used in the [XLA custom call docs](https://github.com/openxla/xla/blob/6d17fd8fbe27b06fc4828dc0c38ecdd8f9ba8573/docs/custom_call.md) now.' by a COLLABORATOR of type User on 2024-05-24T17:37:17Z\n'I'm trying to [\"include what I use\"](https://google.github.io/styleguide/cppguide.html#Include_What_You_Use). For example, of the stuff I use in `mul.cu`:\r\n- `XLA_FFI_Error` is defined in `c_api.h`,\r\n- `ffi::Ffi::Bind()` is defined in `api.h`,\r\n- `ffi::Buffer` is defined in `ffi.h`,\r\n\r\nso I need all three.' by a CONTRIBUTOR of type User on 2024-05-24T17:44:21Z\n'Thanks, this looks like this should reduce the amount of boilerplate. The flip side is I'm now looking at a template error that I can't understand :)' by a CONTRIBUTOR of type User on 2024-05-24T20:45:14Z\n'The error was due to the fact that the function I was exporting using this macro has return type `void`, but it needs to be an `ffi::Error` instead.\r\n\r\nBut now I'm facing a different issue: the macro creates a local lambda variable instead of a function definition. This doesn't work for my case because I'm trying to export a function with C linkage.' by a CONTRIBUTOR of type User on 2024-05-28T21:33:35Z\n'One solution suggested by @ezhulenev is to add a new macro that defines a function with C linkage.' by a CONTRIBUTOR of type User on 2024-05-28T21:34:46Z\n'Yeah, basically `XLA_FFI_EXPORT_HANDLER` that will hide the `extern \"C\" XLA_FFI_Error *Mul` boilerplate. Another options is to change default behavior of `XLA_FFI_DEFINE_HANDLER` to define C function by default and add another one for users that prefer lambda.' by a COLLABORATOR of type User on 2024-05-28T21:42:04Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 7,
        "reason_for_closure": "Inconsistent behavior :- PR aimed to match numpy but JAX generally extends numpy functionality."
    },
    {
        "summary": "Pull Request '20987' titled 'Generic reduce window jvp' was authored by a Bot, who is associated as a NONE. \nIt was created at 2024-04-29T19:29:52Z, and was closed at 2024-05-08T21:00:43Z by a Bot.\nIt has a body of 'Generic reduce window jvp\n\nThe problem is that we want to generically jvp and tranpose over any reduction_fn. Jax already handles some of the hard parts for us, namely, ensuring that the user provided fn is jax capturable. All that is left then, is to write a jvp and tranpose fn that utilize the jax utils correctly.\n\nHowever, this is not so straightforward because in order to get the transpose of a reduction window, we need to be able to use both the tangents and primals. The current reduce_fn operates on (x, y) - but we actually need is, under jvp, to operate on `(x_primal, y_primal, x_tangent, y_tangent)`. In turn, this means we need to push down notions of a jvp-specific reduction_fn (captured via the usual machinery of as_fun `as_fun(jvp_fn(closed(user_reduction_jaxp)))`). \n\nFor the jvp fn, we stack the primal operand and the tangent operand together, and we stack their respective initial values together - this means a good deal of changes to safety checks and assurances downstream (as well as unpacking) as the shape of the operand has changed from [K,...Kt] to [K, ...Kt, 2] where the last dim is the stacked primal and tangent values.\n\nIn following CLs, we will add (1) re-entrant/recursive is_jvp and (2) transposition\n'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/20987/checks?check_run_id=24393116443) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2024-04-29T19:29:57Z\n\nPR has review comments:\n'```suggestion\r\n  primal, tangent = tree_util.tree_unflatten(aval_tree, flat_avals)\r\n```\r\n\r\nDoes this work? ' by a COLLABORATOR of type User on 2024-05-01T17:14:53Z\n'Separate from this PR, we should use our nicer tree mismatch error reporting utilities here, like tree_util.equality_errors and/or tree_util.prefix_errors.' by a COLLABORATOR of type User on 2024-05-01T17:15:44Z\n'Can you say more about what this TODO means?' by a COLLABORATOR of type User on 2024-05-01T17:16:38Z\n'Is this trying to say that higher-order AD isn't supported? If so, maybe writing something closer to that would be clearer.' by a COLLABORATOR of type User on 2024-05-01T17:17:52Z\n'Why these imports here? We should probably just rely on top-level imports. lax should be downstream of core and ad.' by a COLLABORATOR of type User on 2024-05-01T17:18:44Z\n'```suggestion\r\n      init_value_primals, init_value_tangents = init_value\r\n```\r\n\r\nDoes this work? If so it gives us an assertion for free (and saves a line).' by a COLLABORATOR of type User on 2024-05-01T17:19:34Z\n'Can we just use `operator.add` etc for these, instead of defining new functions?' by a COLLABORATOR of type User on 2024-05-01T17:22:59Z\n'Maybe not, if the lax code actually detects operator.add specifically...' by a COLLABORATOR of type User on 2024-05-01T17:23:27Z\n'Is this a TODO or a note to the reviewer?' by a COLLABORATOR of type User on 2024-05-01T17:24:10Z\n'I don't think `constants` is a good name for this. But also I think we shouldn't change `check_jvp` for one test case. Let's figure out another way to write the test? Can iterate in chat.' by a COLLABORATOR of type User on 2024-05-01T17:24:55Z\n'```suggestion\r\n```\r\n\r\nSeems unrelated to this PR and unnecessary.' by a COLLABORATOR of type User on 2024-05-01T17:25:46Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 11,
        "reason_for_closure": "Inconsistent behavior :- PR aimed for NumPy consistency but JAX behavior considered valid."
    },
    {
        "summary": "Pull Request '20906' titled 'Remove type promotion for mixed Nvidia's fp8 matmuls.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-04-23T23:30:22Z, and was closed at 2024-05-07T18:01:07Z by a User.\nIt has a body of 'Remove type promotion for mixed fp8 matmuls. '\nPR has comments:\n'I'll hold off this PR as this relies on XLA pipeline to remove float normalizations first. ' by a CONTRIBUTOR of type User on 2024-05-07T18:01:08Z\n'Replaced by #21211' by a COLLABORATOR of type User on 2024-05-13T17:40:26Z\n\nPR has review comments:\n'My main concern here is: do we have a test case that exercises this? I'm mostly worried about possibly breaking CPU, since this path is taken for CPUs also.' by a COLLABORATOR of type User on 2024-04-29T19:10:36Z\n'If we do this, should this compare `bits` from `finfo` against 8?' by a CONTRIBUTOR of type User on 2024-04-29T23:45:01Z\n'Hello David! For Nvidia GPUs, only mixed precision formats of float8_e4m3fn and float8_e5m2 are supported. AMD and Intel GPUs may support F8E5M2FNUZ and F8E4M3FNUZ formats. However, for now, we are focused solely on the float8_e4m3fn and float8_e5m2 formats. ' by a CONTRIBUTOR of type User on 2024-04-30T19:52:10Z\n'Just added tests. ' by a CONTRIBUTOR of type User on 2024-05-02T19:06:50Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 4,
        "reason_for_closure": "Superseded by another PR :- PR relies on another PR, which is held off."
    },
    {
        "summary": "Pull Request '20311' titled 'Add Hilbert matrix to jax.scipy.linalg' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-03-19T08:15:35Z, and was closed at 2024-03-20T06:26:54Z by a User.\nIt has a body of 'This PR implements the hilbert function which returns a Hilbert matrix of order `n`.'\nPR has comments:\n'I will create another PR with all the requested changes, since I could not squash all the commit into a single commit.' by a CONTRIBUTOR of type User on 2024-03-20T07:29:01Z\n'It's straightforward to squash all commits into a single commit once you know how – please let me know if you'd like assistance with that next time' by a COLLABORATOR of type User on 2024-03-20T16:08:52Z\n\nPR has review comments:\n'Slightly more efficient:\r\n```python\r\n  a = jax.lax.broadcasted_iota('float64', (n, 1), 0)\r\n  return 1 / (a + a.T + 1)\r\n```' by a COLLABORATOR of type User on 2024-03-19T14:17:24Z\n'We should run `_CompileAndCheck` as well. Since `n` is static, this means the functions should be partially-evaluated with `n`, and use `args_maker = lambda: []`' by a COLLABORATOR of type User on 2024-03-19T14:18:04Z\n'This docstring will be overwritten by `@implements`. You can remove it.' by a COLLABORATOR of type User on 2024-03-19T14:19:48Z\n'Done' by a CONTRIBUTOR of type User on 2024-03-19T16:58:23Z\n'Thanks for the suggestion. Done' by a CONTRIBUTOR of type User on 2024-03-19T16:58:34Z\n'Done' by a CONTRIBUTOR of type User on 2024-03-20T05:06:08Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 6,
        "reason_for_closure": "Inconsistent behavior :- PR aimed to match numpy but JAX's superset functionality deemed more valuable."
    },
    {
        "summary": "Pull Request '20270' titled 'Adjust the indices in _pjit_partial_eval to account for removed primals.' was authored by a User, who is associated as a NONE. \nIt was created at 2024-03-15T09:50:52Z, and was closed at 2024-03-16T11:45:30Z by a User.\nIt has a body of '`idx_map` indexes the residuals that are also primals (ie. duplicated in the output of known_jaxpr), which are then deduplicated. But non-residual primal outputs of known_jaxpr can also be removed on account of being forwarded inputs, which alters the previously calculated primal indices. We need to adjust them to account for this.\r\n\r\nFixes #20267.'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/20270/checks?check_run_id=22699057530) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2024-03-15T09:50:57Z\n'Some of the api_tests failed. Looks like because it's possible for residual outputs of known_jaxpr to also be primal outputs that are themselves forwarded inputs, which I didn't account for. Fixed by setting in_fwd instead of out_fwd for those.' by a NONE of type User on 2024-03-15T15:10:15Z\n'Thanks so much for looking at this! I'm in meetings for the next 90 mins but would like to review this, since it's scary to me that there was a bug here in the first place, so we should be sure to get it right this time.\r\n\r\nI'd love to write more exhaustive tests too, covering all the cases...' by a COLLABORATOR of type User on 2024-03-15T16:13:42Z\n'@nshepperd thanks so much for diving in to attack this bug! \r\n\r\nHowever, I think we should merge #20273 instead of this PR. That PR does two things:\r\n1. Rewrites the logic to try to be more obviously correct. That may be a subjective thing, but I think refactoring it to do the two forwarding optimizations one after another, rather than trying to do them at the same time as before, makes it easier to see they won't interfere with each other.\r\n2. Disables the optimization entirely until we can add more tests. It was a mistake for me to merge this change without exhaustive tests in the first place. So I want to disable it until we can thoroughly test.\r\n\r\nWhat do you think?' by a COLLABORATOR of type User on 2024-03-15T17:40:10Z\n'Hehe, I guess you merged it. LGTM though. Good test cases!' by a NONE of type User on 2024-03-16T11:45:30Z\n\nPR has review comments:\n'```suggestion\r\n  # Rarely, residuals can be primal outputs that are themselves forwarded inputs.\r\n  # In this case, set in_fwd for them.\r\n```' by a COLLABORATOR of type User on 2024-03-15T15:08:28Z\n'Done.' by a NONE of type User on 2024-03-15T15:32:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 2,
        "reason_for_closure": "Incorrect :- PR tried to enforce NumPy behavior but JAX's implementation was deemed valid."
    },
    {
        "summary": "Pull Request '20208' titled 'Add more core imports to extend' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2024-03-12T22:15:06Z, and was closed at 2024-04-15T23:09:04Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Adds many of the types needed when interpreting a jaxpr.  '\nPR has comments:\n'What's the status of this?' by a CONTRIBUTOR of type User on 2024-03-18T23:27:50Z\n'Build issues that we need to fix up before merging.' by a MEMBER of type User on 2024-03-19T01:05:27Z\n\nPR has review comments:\n'Should we avoid exporting `eval_jaxpr` in favor of directing to `jaxpr_as_fun` only?' by a MEMBER of type User on 2024-03-13T04:15:10Z\n'Yeah that is likely better for everyone in the long term lol.' by a CONTRIBUTOR of type User on 2024-03-13T15:41:38Z\n'Done' by a CONTRIBUTOR of type User on 2024-03-13T19:52:56Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 3,
        "reason_for_closure": "Closed as infeasible/incorrect: PR deemed unnecessary after discussion and further investigation."
    },
    {
        "summary": "Pull Request '20144' titled 'Fix closeness tests in testOnComplexPlane for small inputs. Update expm1 inaccuracy regions.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2024-03-08T18:29:16Z, and was closed at 2024-03-22T17:07:33Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'As in the title.\r\n\r\nhttps://github.com/openxla/xla/pull/10376 fixes all inaccuracy issues in jax.numpy.expm1:\r\n- [before this PR](https://github.com/pearu/complex_function_validation/blob/5756d0e5b303bb7b084f18f612f69ed3bf5547a2/mpmath_jax_results/data/expm1_MPMath_complex128_cpu_versus_JAX_complex64_cpu.txt) the inaccuracies and mismatch rate was 17.7%\r\n- [after this PR](https://github.com/pearu/complex_function_validation/blob/5756d0e5b303bb7b084f18f612f69ed3bf5547a2/mpmath_jax_results_dev/data/expm1_MPMath_complex128_cpu_versus_JAX_complex64_cpu.txt) the inaccuracies rate is 0.1%\r\n\r\nhttps://github.com/openxla/xla/pull/10503 fixes all inaccuracy issues in jax.numpy.log1p:\r\n- [before this PR](https://github.com/pearu/complex_function_validation/blob/a9b875ffa4f6ee5bdc9c574545966aae6a6e5476/mpmath_jax_results/data/log1p_MPMath_complex128_cpu_versus_JAX_complex64_cpu.txt) the mismatch rate was 42.5%\r\n- [after this PR](https://github.com/pearu/complex_function_validation/blob/a9b875ffa4f6ee5bdc9c574545966aae6a6e5476/mpmath_jax_results_dev/data/log1p_MPMath_complex128_cpu_versus_JAX_complex64_cpu.txt) the inaccuracies rate is 0%\r\n- In addition to log1p, there exists accuracy improvements for arctanh:\r\n  - jax.numpy.arctanh mistmatch [before](https://github.com/pearu/complex_function_validation/blob/a9b875ffa4f6ee5bdc9c574545966aae6a6e5476/mpmath_jax_results/data/arctanh_MPMath_complex128_cpu_versus_JAX_complex64_cpu.txt) is 55.5% and [after](https://github.com/pearu/complex_function_validation/blob/a9b875ffa4f6ee5bdc9c574545966aae6a6e5476/mpmath_jax_results_dev/data/arctanh_MPMath_complex128_cpu_versus_JAX_complex64_cpu.txt) 11.3 %\r\n\r\nhttps://github.com/openxla/xla/pull/10525 fixes all inaccuracy issues in jax.numpy.tan:\r\n- TBD\r\n'\nPR has comments:\n'There are some failures in one of the CI jobs – looks like inf/nan mismatches.' by a COLLABORATOR of type User on 2024-03-08T18:54:55Z\n'> There are some failures in one of the CI jobs – looks like inf/nan mismatches.\r\n\r\nYes, I am in a process of fixing these. These emerged because the closeness checking is changed in this PR and the mapping of inaccuracy lists requires updates re the tests that passed false-positively before. ' by a COLLABORATOR of type User on 2024-03-08T19:16:44Z\n'Tests are failing on internal CPU.\r\n\r\nOverall, I think the issue here is that these tests are too brittle: they make detailed assertions about the dark corners of hardware-dependent implementation details. Rather than add more exceptions for particular backends / package versions / devices / etc. I think it would be better to make the tests simpler and less brittle. What do you think?' by a COLLABORATOR of type User on 2024-03-13T17:16:13Z\n'`xla_extension_version >= 244` needs to be updated as the current `xla_extension_version` value conflicts with the one in the main branch, hence the test failures.' by a COLLABORATOR of type User on 2024-03-13T17:16:22Z\n'> ... I think it would be better to make the tests simpler and less brittle.\r\n\r\nI agree. I suggest replacing numpy with mpmath as a reference library.' by a COLLABORATOR of type User on 2024-03-13T17:52:29Z\n'Created https://github.com/google/jax/pull/20373 to resolve confusion in import/copybara.' by a COLLABORATOR of type User on 2024-03-21T21:37:33Z\n'Closing as https://github.com/google/jax/pull/20373 has landed.\r\n\r\nThe items in the description will be addressed in follow-up PRs.' by a COLLABORATOR of type User on 2024-03-22T17:07:33Z\n'@pearu, are the XLA PRs that address the inaccuracies ready for review? That is, are the following PRs ready for review?\r\n* https://github.com/openxla/xla/pull/8589\r\n* https://github.com/openxla/xla/pull/10376\r\n* https://github.com/openxla/xla/pull/10503\r\n* https://github.com/openxla/xla/pull/10525\r\n' by a CONTRIBUTOR of type User on 2024-03-26T02:16:44Z\n\nPR has review comments:\n'Let's make `mpmath` a required test-time dependency to remove the need for this `importorskip` logic.' by a COLLABORATOR of type User on 2024-03-21T16:44:04Z\n'Done.' by a COLLABORATOR of type User on 2024-03-21T17:07:44Z\n'This going to be a problem for `[build with 3.9 (py 3.9 on ubuntu-20.04-16core, x64=1)](https://github.com/google/jax/actions/runs/8378976633/job/22944817602?pr=20144#logs)` that does not have mpmath installed.' by a COLLABORATOR of type User on 2024-03-21T17:14:29Z\n'Resolved by adding mpmath to build/test-requirements.txt' by a COLLABORATOR of type User on 2024-03-21T17:52:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 4,
        "reason_for_closure": "Closed as superseded/redundant: The fix was implemented in a different way or was no longer needed.  A related PR landed addressing the issue."
    },
    {
        "summary": "Pull Request '19420' titled 'Write docs for new Pallas collective APIs.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2024-01-19T01:41:16Z, and was closed at 2024-09-18T00:06:45Z by a User.\nThe PR has labels: documentation - , pallas - Issues pertaining to Pallas (GPU or TPU). \nPR has comments:\n'Oh man this wasn't ready for a @hawkinsp review. Thanks for the comments though, all look great!' by a COLLABORATOR of type User on 2024-01-22T23:15:08Z\n\nPR has review comments:\n'```suggestion\r\nPallas has support for manual DMA (between memories of the same chip) and remote DMA (between memories of different chips) support. It also has an API for nesting pipelines of compute overlapped collectives.\r\n```\r\n\r\nThings like \"now\" in docs get stale, and it's hard to remember to update things. I'd save that language for announcements / changelog entries, and instead write docs in a 'timeless' way from the beginning.' by a COLLABORATOR of type User on 2024-01-22T17:35:30Z\n'```suggestion\r\n   The :code:`emit_pipeline` API is implemented entirely using public Pallas APIs, an illustration of Pallas meta-programming.\r\n```\r\n\r\nDon't want to sound self-aggrandizing :P' by a COLLABORATOR of type User on 2024-01-22T17:36:45Z\n'Is the second sentence useful in user docs, vs being part of a design doc?' by a COLLABORATOR of type User on 2024-01-22T17:37:44Z\n'\"manual DMA\" vs \"remote DMA\": why is \"manual\" the opposite of \"remote\"? The remote DMAs are also manual.\r\n\r\nPerhaps you meant:\r\nPallas has support for both local DMAs (...) and remote DMAs (...).\r\n\r\nAlso note you duplicated \"support\".' by a COLLABORATOR of type User on 2024-01-22T22:48:02Z\n'Writing tip: avoid starting sentences with \"It\" or \"This\". Starting sentences with those words makes it at least slightly ambiguous to what \"It\" refers, making your reader pause and subconsciously figure it out. Make life easy on your readers. Here \"Pallas also has ...\" reads much more clearly.' by a COLLABORATOR of type User on 2024-01-22T22:48:49Z\n'Perhaps:\r\nManual DMAs, while powerful, are very low level. Pallas offers a flexible nested pipeline abstraction which is usually easier to use than explicit DMAs.' by a COLLABORATOR of type User on 2024-01-22T22:56:37Z\n'Why?\r\n\r\n(Spell it out, the user isn't psychic.)' by a COLLABORATOR of type User on 2024-01-22T22:56:57Z\n'Describe in words what this example does, perhaps 1-2 sentences. It would be quicker and easier for the reader. They can read the code in detail if they want after you communicate the intuition, but it's harder to distill the intuition from the code.' by a COLLABORATOR of type User on 2024-01-22T22:57:56Z\n'Are both this code and the call below runnable in the same context? Make sure the example runs, as stated.' by a COLLABORATOR of type User on 2024-01-22T23:00:01Z\n'What, exactly is the contract?\r\n\r\nPerhaps something like:\r\n\"Manual DMAs to the inputs or outputs of a pipeline is undefined behavior: there are no guarantees about the ordering of manual DMAs with respect to the pipeline's reads and writes.\"' by a COLLABORATOR of type User on 2024-01-22T23:02:33Z\n'Perhaps:\r\nRecall the `matmul` example from :doc:`../quickstart`:' by a COLLABORATOR of type User on 2024-01-22T23:03:21Z\n'Perhaps:\r\n\"The `pallas_call` performs ...\" instead of \"We do\". You didn't, Pallas did.' by a COLLABORATOR of type User on 2024-01-22T23:04:40Z\n'I think this section needs work. I'm trying to parse the motivation and having trouble.\r\n\r\nInitially when reading this I thought: \"ah, he's motivating nested pipelines\". But the first part of the text is motivating collective matmuls.\r\n\r\nPerhaps be clearer about that: (a) first motivate collective matmuls, perhaps with a picture and then (b) show why nested pipelines are needed to do it?' by a COLLABORATOR of type User on 2024-01-22T23:07:40Z\n'The jump in complexity from the previous toy pipelines that had no communication is very large here. Can we ease into it?' by a COLLABORATOR of type User on 2024-01-22T23:13:18Z\n'reworded' by a COLLABORATOR of type User on 2024-01-31T20:21:12Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 15,
        "reason_for_closure": "Inconclusive :- PR closed after discussion on complex inputs and numerical accuracy."
    },
    {
        "summary": "Pull Request '18768' titled '[DRAFT] Custom gpu ops custom partitioning' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-12-01T15:03:59Z, and was closed at 2024-04-01T13:29:17Z by a User.\nIt has a body of 'This in progress PR modify the docs/Custom_Operation_for_GPUs.py tutorial to use custom_partitioning instead of xmap.\r\n\r\nDon't review now, there is still much work done.\r\n- [x] finish the forward code.\r\n- [x] finish the backward code.\r\n- [ ] update the documentation to explain it.\r\n- [ ] document debugging trick, like looking the the dump after sharding propagation, but before partitioning: module_0019.pjit__unnamed_function_.0011.spmd-partitioner.after_sharding-propagation.before_spmd-partitioning.txt, XLA_FLAGS=\"--xla_dump_to=DIRECTORY_PATH --xla_dump_hlo_pass_re=.*\"\r\n- [ ] commit the full code to easy starting from it.\r\n- [ ] make a test of the commited full code to prevent breaking it as it already happened.\r\n- [ ] modify the documentation to inline part of the commited code instead of duplicating it.\r\n'\nPR has comments:\n'Replaced by https://github.com/google/jax/pull/20179' by a COLLABORATOR of type User on 2024-03-13T00:16:31Z\n'Replaced by https://github.com/google/jax/pull/20179' by a COLLABORATOR of type User on 2024-04-01T13:29:17Z\n\nPR has review comments:\n'Some type annotations would be quite useful here, especially for people not entirely familiar with the signatures and expected arguments. For example:\r\n\r\n`def infer_sharing_from_operands(\r\neps : float, mesh : jax.sharding.Mesh, arg_infos : <whatever the type>, result_infos : <whatever the type>\r\n):`' by a CONTRIBUTOR of type User on 2024-01-17T23:26:15Z\n'Again type annotations on what `x_info` and `weight_info` would be useful. But perhaps if `arg_infos` was annotated in the function signature we wouldn't need these?' by a CONTRIBUTOR of type User on 2024-01-17T23:27:56Z\n'A concrete example of what `get_padded_spec` is doing would be very useful' by a CONTRIBUTOR of type User on 2024-01-17T23:29:58Z\n'What is the reason for this deletion? If they are unused in the function, why pass them in? The reason might be something simple for someone who is already familiar with this pattern, but for someone who isn't as familiar this seems strange, especially since this pattern isn't discussed in the documentation on [custom partitioning](https://jax.readthedocs.io/en/latest/jax.experimental.custom_partitioning.html#module-jax.experimental.custom_partitioning)' by a CONTRIBUTOR of type User on 2024-01-18T00:50:15Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 4,
        "reason_for_closure": "Superseded by NumPy functionality. JAX should support complex inputs as it's a superset of NumPy."
    },
    {
        "summary": "Pull Request '18536' titled 'Use MLIR generated convenience functions athing(...) instead of writing AThingOp(...).result.' was authored by a Bot, who is associated as a NONE. \nIt was created at 2023-11-15T02:46:03Z, and was closed at 2023-11-17T19:47:20Z by a Bot.\nIt has a body of 'Use MLIR generated convenience functions athing(...) instead of writing AThingOp(...).result.\n\nIn most cases these are more succinct.\n\nThis change does not update Pallas/Mosaic.\n'\nPR has comments:\n'Hmm. Well, this change is great, but it turns out we're going to need to bump the minimum jaxlib version to 0.4.18 before doing this.' by a COLLABORATOR of type User on 2023-11-15T12:43:42Z\n\nPR has review comments:\n'@hawkinsp Missed one here?' by a NONE of type User on 2023-11-15T13:16:42Z\n'Done!' by a COLLABORATOR of type User on 2023-11-17T14:10:30Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as not useful; behavior of the JAX implementation is correct and shouldn't be deprecated."
    },
    {
        "summary": "Pull Request '18349' titled 'Added bytes usage for Custom Partitioner' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-11-02T07:34:30Z, and was closed at 2023-11-13T20:52:48Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Added the usage of `bytes` for the backend_string key in `custom_partitioning.py`\r\n\r\nThis is a forwards-compatible fix for https://github.com/openxla/xla/pull/6719'\nPR has comments:\n'All merged. https://github.com/openxla/xla/pull/6719#issuecomment-1805263318' by a COLLABORATOR of type User on 2023-11-13T20:52:48Z\n'I merged this with some changes, so it didn't get closed by copybara. Sorry about the trouble.' by a COLLABORATOR of type User on 2023-11-13T20:53:33Z\n'No worries, just glad it's fixed now.' by a CONTRIBUTOR of type User on 2023-11-13T20:59:17Z\n\nPR has review comments:\n'bytes(..., \"utf8\")' by a COLLABORATOR of type User on 2023-11-02T20:12:18Z\n'Done.' by a CONTRIBUTOR of type User on 2023-11-05T05:40:26Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 2,
        "reason_for_closure": "Merged after some changes by collaborator."
    },
    {
        "summary": "Pull Request '18243' titled 'Cloudpickle and deepcopy support for Jaxprs' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-10-23T22:10:52Z, and was closed at 2024-02-12T23:54:03Z by a User.\nIt has a body of 'Fixes #9444 \r\n\r\nNeeded to implement the methods `__reduce_ex__` and `__deepcopy__` on `Primitive` and `SourceInfo`. Added unit tests in a new file `copying_test.py`.\r\n\r\nThis is generally useful in distributed environments, i.e., I can make a transform to create a `shard_map`ed jaxpr, send this jaxpr to each of my worker nodes over the network via Ray / Dask (both of which use cloudpickle), and then just execute the jaxprs.'\nPR has comments:\n'Classic case of works on my machine. Not sure what's wrong with the docs build' by a CONTRIBUTOR of type User on 2023-10-23T22:54:46Z\n'> Jitted functions are already pickleable.\r\n\r\nNot for my usecase. The jaxpr is still picked up in the closure, making the pickle impossible.\r\n\r\n```python\r\n>>> import jax\r\n>>> import cloudpickle\r\n>>> def f(a):\r\n...     return a + a\r\n... \r\n>>> j = jax.make_jaxpr(f)(1)\r\n>>> cloudpickle.cloudpickle.dumps(j)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/chase/anaconda3/envs/python311/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\r\n    cp.dump(obj)\r\n  File \"/home/chase/anaconda3/envs/python311/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\r\n    return super().dump(obj)\r\n           ^^^^^^^^^^^^^^^^^\r\nTypeError: cannot pickle 'jaxlib.xla_extension.Traceback' object\r\n>>>\r\n>>> jitted = jax.jit(jax.core.jaxpr_as_fun(j))\r\n>>> cloudpickle.cloudpickle.dumps(jitted)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/chase/anaconda3/envs/python311/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\r\n    cp.dump(obj)\r\n  File \"/home/chase/anaconda3/envs/python311/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\r\n    return super().dump(obj)\r\n           ^^^^^^^^^^^^^^^^^\r\nTypeError: cannot pickle 'jaxlib.xla_extension.Traceback' object\r\n```\r\n\r\n`lower(...)` isn't pickleable either. \r\n```python\r\n>>> s = jax.jit(jax.core.jaxpr_as_fun(j)).lower(1)\r\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\r\n>>> s\r\n<jax._src.stages.Lowered object at 0x7fa49bc19070>\r\n>>> cloudpickle.cloudpickle.dumps(s)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/chase/anaconda3/envs/clean11/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1479, in dumps\r\n    cp.dump(obj)\r\n  File \"/home/chase/anaconda3/envs/clean11/lib/python3.11/site-packages/cloudpickle/cloudpickle.py\", line 1245, in dump\r\n    return super().dump(obj)\r\n           ^^^^^^^^^^^^^^^^^\r\nTypeError: cannot pickle 'jaxlib.mlir._mlir_libs._mlir.ir.Module' object\r\n```\r\n\r\n' by a CONTRIBUTOR of type User on 2023-10-24T16:31:20Z\n'That wasn't exactly the question, though. You can pickle `jit(f)`, no problem. Why is the jaxpr the right thing to pickle?' by a COLLABORATOR of type User on 2023-10-24T16:46:16Z\n'> Why is the jaxpr the right thing to pickle?\r\n\r\nBecause my goal is to be able to transform `f` _before_ sending it off to the workers. I can not do that with just the original definition of `f` unless I do that transform instead on every single worker, which is not scalable for my goals. ' by a CONTRIBUTOR of type User on 2023-10-24T16:53:48Z\n'> which is not scalable for my goals.\r\n\r\nFor example: if my transform is a randomized optimization, I would need to ensure all of the workers came to the exact same solution. Possible, but incredibly fragile. Solving this first on the client before dispatching prevents this issue forever. ' by a CONTRIBUTOR of type User on 2023-10-24T16:55:53Z\n'I want jaxprs to be able to be passed around, modified, and executed using standard distributed python tooling. This could have a wide range of applications beyond just pickling `jit(f)`\r\n\r\nYou could imagine the jaxprs being used with a Ray / Dask server just for the _compiler optimization_ and not even the actual execution. Something like:\r\n\r\n```python\r\n# Use the remote cluster to optimize your jaxpr.\r\njxpr = best_of([optimize.remote(f, seed) for seed in initial_seeds], my_metrics)\r\n\r\n# Execute the jaxpr locally on your GPU.\r\njax.jit(jaxpr_as_fun(jxpr))(my_args)\r\n```' by a CONTRIBUTOR of type User on 2023-10-24T17:47:40Z\n'I think we can imagine similar such things (in fact, @mattjj had a branch doing something similar for a Ray prototype a while back, though we never merged it). While we all broadly agree that this can be potentially useful, it's not free, hence the questions about alternatives. For your immediate, concrete use case, does pickling `jit(f)` suffice? Could you do anything else short of pickling jaxpr?' by a MEMBER of type User on 2023-10-24T22:45:30Z\n'>  For your immediate, concrete use case, does pickling jit(f) suffice?\r\n\r\nSadly not really, I've tried and it's usually pretty painful.\r\n\r\n> Could you do anything else short of pickling jaxpr?\r\n\r\nThis is the setup I want to solve: I have a jax function, `f`, and I have a transform `trfm`, which can be expensive, and possibly non deterministic. I want to execute `trfm(f)(args...)` in an SPMD fashion on a distributed `mesh`\r\n\r\nI've come up with several ideas to do this\r\n\r\n1. Make all of the transforms execute on the workers. \r\n     * This is what is recommended currently in JAX.\r\n     * This is reasonable for well-defined, deterministic transforms (i.e., `grad, vmap, vjp`), but can become difficult if `trfm` needs to do expensive optimization searches, or if any randomization is used. \r\n     * When developers create their own custom transform as `jax.extend` evolves, they're going to have a bad time ~if~ when they need to debug a nondeterminism bug across a cluster. \r\n\r\n2. Make some kind of separate IR that is pickled instead of the `Jaxpr`s. \r\n    * I don't think anyone wants to support this lol. \r\n\r\n3. Support pickling `Jaxprs`. \r\n     * This PR. \r\n     * In this setup, our `Jaxpr` can be derived locally if `trfm` is randomized or complicated. We then dispatch this jaxpr with Ray or Dask (via cloudpickle) to the entire mesh. \r\n     * All nodes have the exact same `Jaxpr`, so when you run `jit(jaxpr_as_fun(jaxpr))(arg...)` on all nodes simultaneously, the chance of bugs related to nodes running mismatching SPMD binaries drops significantly. \r\n\r\nHonestly, these are the only solutions I could think of. I've had to both 1) and 2) at various times in the past and they always are very fragile. If instead I could have 3) JustWork™, it would significantly simplify a lot of the dispatching infrastructure for auto-partitioning work. \r\n\r\nThere could be a forth even easier solution I am missing, but I haven't found it yet. Ideas are welcomed! \r\n\r\n>  While we all broadly agree that this can be potentially useful, it's not free, hence the questions about alternatives\r\n\r\nNothing is ever free, but what is the cost we're trying to avoid here? We have unit tests that will catch obvious problems quickly, and I am happy to be the one responsible to fix issues related to this (I'll probably be the one hitting issues the most anyway lol). \r\n\r\nI can see the argument against adding another global dictionary to manage, but we already use similar global dictionaries for `vmap`, `jit`, `grad`, and well, basically everything! It's not a weird thing to see in the JAX codebase. \r\n\r\nI can also see an argument against the name strings being used for infrastructure. I also don't like this either, but the inclusion of `namespace` and possibly also including the `jax.__version__` (I should add this...), should be enough to avoid conflicts/compatibility issues.  There are no name conflicts as it stands today (at least in OSS land), and again issues could likely be caught quickly with good unit tests.\r\n\r\nSo the cost is: \r\n* Manage 40 new LOC, a single extra global dictionary, and a few unit tests. \r\n* Risk that we add new attributes that are not pickleable in the future and have to deal with them. \r\n  * Unit tests will likely catch it, and you can `None` them out in a `__reduce_ex__` method like I did with `Traceback`. Annoying but not terrible. \r\n\r\nThe value: \r\n* Ray and Dask are automatically fully compatible with Jaxprs. \r\n* Unlocks distributed jaxpr optimizations and dispatch.\r\n\r\nIn my opinion it's super worth it. ' by a CONTRIBUTOR of type User on 2023-10-25T00:03:21Z\n'At one point not long ago, @pschuh made executables experimentally serializable by relying on pickle's persistent ID mechanism. See:\r\n\r\nhttps://github.com/google/jax/blob/cd177fd5663e1f25c94e76e6babf6d676c8f5c50/jax/experimental/serialize_executable.py#L62-L91\r\n\r\nCould something similar be useful here, in particular to decouple a bit from the jax core type definitions (especially if we want this actually decoupled at first)?\r\n\r\nhttps://docs.python.org/3/library/pickle.html#persistence-of-external-objects\r\nhttps://docs.python.org/3/library/pickle.html#dispatch-tables\r\nhttps://docs.python.org/3/library/pickle.html#custom-reduction-for-types-functions-and-other-objects' by a MEMBER of type User on 2023-10-26T14:54:41Z\n'@froystig please take a look at the latest implementation. I think it should be a much more agreeable solution than what I had before.' by a CONTRIBUTOR of type User on 2023-10-26T20:29:15Z\n'With https://github.com/google/jax/pull/18243#issuecomment-1781294233, why can't this live in your experimental project? In other words, pickling jaxprs doesn't have to live in JAX with the above approach I think :)\r\n\r\nI would recommend that you try out what's recommended in the above comment and see if that works?' by a COLLABORATOR of type User on 2023-10-26T20:40:58Z\n'Should we instead be serializing the stablehlo if you want it post-transform?' by a COLLABORATOR of type User on 2023-10-26T20:56:39Z\n'> Should we instead be serializing the stablehlo if you want it post-transform?\r\n\r\nI want to stay in Jaxpr / python land. The serialization is less important than its compatibility with standard python cloud tooling. \r\n\r\n> why can't this live in your experimental project?\r\n\r\nI can do anything I want internally, but I think this is valuable enough to the larger OSS community for it to exist and work easily.' by a CONTRIBUTOR of type User on 2023-10-26T20:59:18Z\n'Closing as stale. ' by a CONTRIBUTOR of type User on 2024-02-12T23:54:03Z\n\nPR has review comments:\n'This is the problematic line, because this is being run on Python 3.9 which does not support `|` syntax for type annotations. That said, I'm a bit confused because this file does `from __future__ import annotations` so I would have thought it would be OK.' by a COLLABORATOR of type User on 2023-10-23T23:14:08Z\n'In any case, I'd try changing this to `Union[str, tuple[Any, ...]]` and then I suspect the doc build will pass.' by a COLLABORATOR of type User on 2023-10-23T23:14:46Z\n'I can just do `tuple[Any, ...]` as I don't return a string ever.' by a CONTRIBUTOR of type User on 2023-10-23T23:20:52Z\n'> That said, I'm a bit confused because this file does from __future__ import annotations so I would have thought it would be OK.\r\n\r\nI also did it in source_info_util which does not import from `__future__`' by a CONTRIBUTOR of type User on 2023-10-23T23:31:11Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 14,
        "num_review_comments": 4,
        "reason_for_closure": "Stale :- PR closed due to inactivity.  No further progress or response from the author."
    },
    {
        "summary": "Pull Request '18194' titled '[typing] add internal DTypeLike specializations' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-10-19T18:11:38Z, and was closed at 2023-11-06T19:36:16Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nPR has comments:\n'I don't think we can merge this: the issue is that these specializations reject valid code if the value of the string input is unknown statically to the type checker.\r\n\r\nThe semantics I want are to allow any generic string in general, BUT if the string value is known statically, it must match one of the known values.\r\n\r\nAs far as I understand, Python's type annotation syntax cannot express that, so accepting any `str` is the best we can do.' by a COLLABORATOR of type User on 2023-11-06T19:36:16Z\n\nPR has review comments:\n'Would it be useful to make this a union of literal types iyo?' by a COLLABORATOR of type User on 2023-10-19T19:40:54Z\n'I think this is similar to `Iterable[str]` where people often want to forbid `str` itself. The solution to that is intersection and negation types: `Iterable[str] & (~str)` which are not yet available in the Python type system. \r\n\r\nSee python/typing#801.' by a COLLABORATOR of type User on 2023-10-19T19:43:29Z\n'What does this type correspond to?' by a COLLABORATOR of type User on 2023-10-19T19:44:53Z\n'Nit: `DType[Any]`?' by a COLLABORATOR of type User on 2023-10-19T19:45:29Z\n'Why is it a bad idea to define `DTypeLike` as a union of the specialized `DTypeLike*` aliases below?' by a COLLABORATOR of type User on 2023-10-19T19:46:17Z\n'`ExtendedDType` is used for `bint` and `prng_key` dtypes; it's discussed here: https://jax.readthedocs.io/en/latest/jep/9263-typed-keys.html#extended-dtypes' by a COLLABORATOR of type User on 2023-10-19T20:36:58Z\n'I think it's the opposite: we want this to be essentially `Literal[int]`, if that were a valid construct – i.e. integer instances are not allowed (`np.dtype(1)` fails) but the actual literal type `int` is allowed (`np.dtype(int)` succeeds).\r\n\r\n`type[int]` does work to make `np.dtype(int)` succeed, but it also allows for `np.dtype(bool)`, which is wrong.' by a COLLABORATOR of type User on 2023-10-19T20:38:36Z\n'Ooops, yes, you are absolutely right! We want an invariant `type`.' by a COLLABORATOR of type User on 2023-10-19T20:59:12Z\n'Possibly – this is a public API though, so making it more strictly defined will in general lead to downstream failures that will prevent us from landing this.' by a COLLABORATOR of type User on 2023-10-19T20:59:58Z\n'Done' by a COLLABORATOR of type User on 2023-10-20T16:56:59Z\n'`DTypeLike` is used in public APIs, so if we change it there's a good chance the perturbation will break the builds of downstream packages. Plus the simplicity here is nice (even if there are invalid strings that it lets through).' by a COLLABORATOR of type User on 2023-10-20T16:58:00Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 11,
        "reason_for_closure": "Inconsistent behavior :- PR aimed to match NumPy but JAX's broader functionality deemed preferable."
    },
    {
        "summary": "Pull Request '17412' titled 'Flexible Type Annotation for jax.lax.scan Function - Fixes #17405' was authored by a User, who is associated as a NONE. \nIt was created at 2023-09-02T12:45:25Z, and was closed at 2023-11-07T17:34:11Z by a User.\nIt has a body of 'What does this PR do?\r\n\r\nfix : #17405\r\n\r\nThis pull request addresses issue #17405, which concerns the type annotation for the jax.lax.scan function in the JAX library. The issue pointed out that the existing type annotation was not accurate when the function f returned different types, especially when it returned Python scalar types.\r\n\r\nTo resolve this issue, this pull request introduces a more flexible type annotation for the jax.lax.scan function. Instead of specifying a fixed type for the return value, we use Any for the type of Y. This change acknowledges the inherent variability in the return type of f and ensures that the type annotation is more inclusive and representative of real-world usage.\r\n\r\nBy using Any for Y, we avoid overly restrictive type annotations that might lead to type errors in cases where f returns Python scalar types, while still maintaining type safety for common cases where Y represents an array with an unspecified number of dimensions.\r\n\r\nThis change aims to make the type annotation for jax.lax.scan more accurate and compatible with various use cases, ultimately improving the developer experience when working with JAX.\r\n\r\nChanges Made:\r\n\r\nUpdated the type annotation for the jax.lax.scan function by changing the type of Y from a specific type to Any, allowing for more flexibility in handling different return types from the function f.'\nPR has comments:\n'Jakevdp,\r\n\r\nThank you for taking the time to review this pull request and provide valuable feedback.\r\n\r\nI appreciate your insights and concerns regarding the proposed changes. It's clear that there are some challenges in finding an optimal solution for annotating the `jax.lax.scan` function, especially given the variability in the return type of `f`.\r\n\r\nI acknowledge your point about shadowing the built-in `Any` with a `TypeVar` of the same name, which could be confusing. I'll address this concern in the next iteration of the pull request.\r\n\r\nRegarding the use of `tuple` versus `Tuple`, I understand the preference for the built-in lowercase `tuple`, and I'll make sure to follow that convention in the updated code.\r\n\r\nI'll also take into consideration your comment about the limitations of Python's type system and continue to explore potential alternatives or improvements to the type annotation.\r\n\r\nYour feedback is greatly appreciated, and I'll work on addressing these concerns and refining the pull request accordingly.\r\n\r\nThank you.' by a NONE of type User on 2023-09-05T14:17:27Z\n'Hello - are you still interested in working on this? Thanks!' by a COLLABORATOR of type User on 2023-11-06T19:32:50Z\n'> Hello - are you still interested in working on this? Thanks!\r\n\r\nNo\r\n' by a NONE of type User on 2023-11-07T17:34:12Z\n\nPR has review comments:\n'It's confusing to shadow the built-in name `Any` with a `TypeVar` of the same name.' by a COLLABORATOR of type User on 2023-09-05T12:54:53Z\n'We should continue to use the built-in (lowercase) `tuple` rather than importing the (upper-case) `Tuple`, because the latter is deprecated in newer Python versions.' by a COLLABORATOR of type User on 2023-09-05T12:56:41Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 2,
        "reason_for_closure": "Inconsistent behavior :- Author intended to match NumPy, but JAX maintainers disagreed with the change."
    },
    {
        "summary": "Pull Request '17211' titled 'Add JAX Infrequently Asked Questions (IAQ)' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-08-21T22:21:47Z, and was closed at 2023-08-21T23:07:58Z by a User.\nIt has a body of 'Add JAX Infrequently Asked Questions (IAQ) - useful questions with answers that do not fall into the FAQ category (yet).\r\n\r\nItem 1: How to apply `jax.device_put` on a `jax.Array` that has different values per host into a 'data' mesh axis?'\nPR has comments:\n'@jakevdp This can be WIP or a good first PR. LMKWYT.' by a CONTRIBUTOR of type User on 2023-08-21T22:22:17Z\n'I don't think this question belongs in this doc FWIW. It should exist in a different doc which we have plans to write (someday).' by a COLLABORATOR of type User on 2023-08-21T22:40:02Z\n'> I don't think this question belongs in this doc FWIW. It should exist in a different doc which we have plans to write (someday).\r\n\r\n:+1:  Will close this for now. Thanks!\r\n' by a CONTRIBUTOR of type User on 2023-08-21T23:07:58Z\n\nPR has review comments:\n'```suggestion\r\nHow to apply ``jax.device_put`` on a ``jax.Array` that has different values per host into a 'data' mesh axis?\r\n```' by a CONTRIBUTOR of type User on 2023-08-21T22:22:32Z\n'what does this mean?\r\n\r\nPlease don't copy internal code into public docs too (as you did below)' by a COLLABORATOR of type User on 2023-08-21T22:24:35Z\n'This is from https://github.com/google/jax/blob/main/jax/_src/array.py#647' by a CONTRIBUTOR of type User on 2023-08-21T22:25:29Z\n'Better link (permalink from GitHub) https://github.com/google/jax/blob/d03187e3054fe95c57e0b611ba2e399a9ce168fe/jax/_src/array.py#L647' by a CONTRIBUTOR of type User on 2023-08-21T22:28:15Z\n'Let's back up a bit.. what does that question mean?\r\n\r\nWhy do you need a different doc for it? I don't think this question belongs in this doc.' by a COLLABORATOR of type User on 2023-08-21T22:29:29Z\n'```suggestion\r\nHere's an example from :meth:`jax.Array` (Line 647 <https://github.com/google/jax/blob/d03187e3054fe95c57e0b611ba2e399a9ce168fe/jax/_src/array.py#L647>`_):\r\n```\r\n\r\nUPDATE: Fixed the GitHub permalink' by a CONTRIBUTOR of type User on 2023-08-21T22:33:16Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 6,
        "reason_for_closure": "Inconsistent behavior change: PR deprecates functionality supported by JAX but not NumPy."
    },
    {
        "summary": "Pull Request '16974' titled 'Add linear_sum_assignment' was authored by a User, who is associated as a NONE. \nIt was created at 2023-08-04T15:24:55Z, and was closed at 2023-11-06T19:32:02Z by a User.\nIt has a body of 'Adds implementation of scipy.optimize.linear_sum_assignment.\r\n\r\nDirect translation of scipy algorithm. Compatible with vmap, jit. \r\n\r\nCloses #10403'\nPR has comments:\n'Thanks for the feedback!\r\n\r\nI understand it is a slightly unwieldy algorithm. Motivations for merging:\r\n - I already have the implementation and as far as I can tell there isn't a JAX solution to the assignment problem available in public libraries,\r\n - there seemed to be sufficient interest in #10403, and I'd say [optimal assignment](https://en.wikipedia.org/wiki/Assignment_problem)/ matching is a fairly common problem across a range of fields.\r\n\r\nAs for where it should live, I'm unsure how much coverage of the SciPy API the JAX team aim to provide. If the goal is to ease adoption via familiar APIs then having it in JAX directly makes sense to me. If the maintenance is deemed to be an issue then I'd be happy to look at other libraries, but hadn't come across any that seemed as suitable. Suggestions more than welcome.' by a NONE of type User on 2023-08-05T13:04:09Z\n'I think one possible location for this might be the [JAXopt](https://jaxopt.github.io/) package: we've mostly abandoned work on `jax.scipy.optimize` because JAXopt contains much more rigorous and complete implementations of optimizers in JAX, free from the need to match scipy's legacy API. I wonder if that might be a better place for `linear_sum_assignment`?\r\n\r\nTagging @mblondel, who is one of the creators of JAXopt, for any thoughts.' by a COLLABORATOR of type User on 2023-08-08T21:27:28Z\n'I would be interested to have this in JAXopt. It would be great to benchmark against SciPy, since XLA can sometimes generate slow code for very loopy algorithms.' by a NONE of type User on 2023-08-08T22:01:16Z\n'There is a TPU-friendly implementation of the Hungarian algorithm here: https://github.com/google-research/scenic/blob/main/scenic/model_lib/matchers/hungarian_cover.py' by a NONE of type User on 2023-08-09T03:30:50Z\n'I'm going to close this because I think it's better suited for a package like JAXopt. See the result of our recent discussion of numpy/scipy scope in JAX here: https://jax.readthedocs.io/en/latest/jep/18137-numpy-scipy-scope.html\r\n\r\nThanks for working on this!' by a COLLABORATOR of type User on 2023-11-06T19:32:03Z\n'@odneill Would you be interested in contributing this to optax? https://github.com/google-deepmind/optax/issues/954' by a CONTRIBUTOR of type User on 2024-07-03T19:20:39Z\n'Hi, yes I think getting an assignment algorithm in optax would be good, and am happy to make a PR - I have some improved code already since this thread. I won't be able to work on this until August earliest however.' by a NONE of type User on 2024-07-09T07:00:21Z\n'@odneill I've started a PR at https://github.com/google-deepmind/optax/pull/1083, in case you have any feedback or suggestions for improvements.' by a CONTRIBUTOR of type User on 2024-10-02T06:15:39Z\n\nPR has review comments:\n'```suggestion\r\n# Copyright 2023 The JAX Authors.\r\n```' by a COLLABORATOR of type User on 2023-08-04T17:00:17Z\n'```suggestion\r\n  cost, u, v, path, row4col, col4row =  lax.fori_loop(\r\n```\r\n' by a COLLABORATOR of type User on 2023-08-04T17:01:32Z\n'(delete)' by a COLLABORATOR of type User on 2023-08-04T17:01:51Z\n'Please avoid using irreproducible random seeds in tests: it may lead to flaky test failures that are hard to reproduce. Take a look at how other JAX tests are implemented (e.g. here you could use `jtu.rand_default` with `dtype='int32'` and `dtype='float32'`)' by a COLLABORATOR of type User on 2023-08-04T17:05:36Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 4,
        "reason_for_closure": "Inconsistent behavior fixed: PR closed as JAX functionality supersedes NumPy and shouldn't deprecate features for consistency alone."
    },
    {
        "summary": "Pull Request '16832' titled 'Allow implicit float8 and int4 promotion under numpy_dtype_promotion='full'' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-07-24T23:36:09Z, and was closed at 2023-11-03T22:40:19Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This adds a new type promotion mode, `jax.numpy_dtype_promotion('full')` which enables implicit type promotion of `float8` values. For example:\r\n```python\r\nIn [1]: import jax\r\n\r\nIn [2]: import jax.numpy as jnp\r\n\r\nIn [3]: x = jnp.array(1, dtype='float8_e5m2')\r\n\r\nIn [4]: y = jnp.array(1, dtype='float32')\r\n\r\nIn [5]: with jax.numpy_dtype_promotion('full'):\r\n   ...:     print(x + y)\r\n   ...: \r\n2.0\r\n```\r\nI'm still unsure of whether this should be included in the default `standard` promotion table for the reasons discussed in \r\n#16705, but at least this gives users a way to opt-in for the time being.'\nPR has comments:\n'The last push updates the `float8` and `int4` implicit promotion error messages to mention the new promotion mode cc @mattjj' by a COLLABORATOR of type User on 2023-07-26T19:17:07Z\n'I think we're not going to do this, because the desirable semantics are unclear.' by a COLLABORATOR of type User on 2023-11-03T22:40:19Z\n\nPR has review comments:\n'Is this correct? Wouldn't f2 also be an acceptable promotion for all fp8 types we have?' by a COLLABORATOR of type User on 2023-07-27T18:04:31Z\n'It would, but that would also break the lattice property. So if we want float8 to promote to both float16 and bfloat16, we'd lose associativity & would have to rewrite our dtype promotion infrastructure.' by a COLLABORATOR of type User on 2023-07-27T18:05:55Z\n'Put another way, I'm curious to see the whole lattice and wondering if it is actually uniquely defined.' by a COLLABORATOR of type User on 2023-07-27T18:12:29Z\n'Yes. So that implies to me that adding this particular promotion is broken, since it makes an arbitrary choice that isn't necessarily correct.' by a COLLABORATOR of type User on 2023-07-27T18:13:10Z\n'> I'm curious to see the whole lattice and wondering if it is actually uniquely defined.\r\n\r\nWondering if what is uniquely defined?' by a COLLABORATOR of type User on 2023-07-27T18:18:52Z\n'The lattice in this PR is essentially the same as pictured in https://jax.readthedocs.io/en/latest/type_promotion.html, except `f8` types lie on the connection between `f*` and `bf`. ' by a COLLABORATOR of type User on 2023-07-27T18:20:36Z\n'Yup, see my second comment. I don't think we can make that choice.' by a COLLABORATOR of type User on 2023-07-27T18:23:55Z\n'Sorry, I don't follow' by a COLLABORATOR of type User on 2023-07-27T18:28:50Z\n'Do you mean that there's no *a priori* way to choose which lattice to use? If so, I agree... but that's also never stopped us before! The type promotion lattice has never been uniquely defined. We've chosen a lattice based on design tradeoffs... nothing is stopping us from doing the same here.' by a COLLABORATOR of type User on 2023-07-27T18:30:19Z\n'Why is it ok to arbitrarily declare we will promote to `bfloat16` but not `float16`? It seems unprincipled.' by a COLLABORATOR of type User on 2023-07-27T18:30:49Z\n'Also, use of a lattice itself is a design tradeoff. It has some nice properties, but if we get to the point where a promotion table makes more sense due to constraints like the one here, we could make that choice.' by a COLLABORATOR of type User on 2023-07-27T18:31:27Z\n'Adding to this discussion. I would be a little disappointed if all fp8 types promoted to bf16. We (graphcore) have hardware in the wild that supports fp8, promotes to fp16, and doesn't natively support bf16.' by a CONTRIBUTOR of type User on 2023-08-04T18:02:45Z\n'The only pragmatic bias I can think of would be to prefer promoting to IEEE types when possible. They will offer the broadest hardware support.' by a CONTRIBUTOR of type User on 2023-08-04T18:05:00Z\n'Well, we have to choose to promote to either bf16 or f16, or otherwise abandon our lattice-based type promotion approach (why? well, if all fp8 flavors promote to both, then the promotion path for `fp8_flavor1 + fp8_flavor2` is undecidable on the lattice).\r\n\r\nI chose bf16 here because it makes sense on hardware which has support for bf16 but not for f16.\r\n\r\nBut you have in mind other situations where f16 is the better choice.\r\n\r\nPerhaps this is another argument in favor of requiring explicit promotion in mixed-precision ops with fp8?' by a COLLABORATOR of type User on 2023-08-04T18:10:54Z\n'> But you have in mind other situations where f16 is the better choice.\r\n\r\nThe Graphcore IPU ([ISA here](https://docs.graphcore.ai/projects/isa/en/latest/index.html)) natively supports fp8, f16, and f32.\r\nAt 2.10.2.1 our ISA states:\r\n> Note: Tile doesn’t provide direct support for the 16-bit BFloat16 truncated single format (8-bit exponent, 7-bit significand, Zero offset of 127). Use of this storage format is possible but requires explicit (zero tailed) conversion from/to full single-precision.\r\n\r\nSo my obvious bias would be towards promoting to f16, if all other factors are equal.\r\nI'm not aware of any hardware that supports fp8, bf16, and not f16. Perhaps that will change.\r\n' by a CONTRIBUTOR of type User on 2023-08-04T22:20:25Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 15,
        "reason_for_closure": "Inconsistent behavior :- PR aimed for NumPy consistency but JAX behavior considered valid."
    },
    {
        "summary": "Pull Request '16615' titled '[shape_poly] Improve shape constraint checking using shape assertions' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-07-03T14:35:35Z, and was closed at 2023-07-20T04:45:31Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This is a major improvement in the shape constraint checking. Even before this change JAX assumes\r\ncertain constraints on the shapes of actual arguments in presence of shape polymorphism.\r\nFor example,  given the `polymorphic_shapes=\"(b, b, 2*d)\"`\r\nspecification for an argument `arg`, JAX assumes the following:\r\n\r\n  * `arg.shape[0] >= 1`\r\n  * `arg.shape[1] == arg.shape[0]`\r\n  * `arg.shape[2] % 2 == 0` and `arg.shape[0] // 2 >= 1`\r\n\r\nPreviously, these constraints were checked only in graph serialization mode and only for eager execution,\r\nwhen we had access to the actual shapes of `arg` during tracing.\r\nWith this change we check them for native serialization also (relying on newly added support for shape\r\nassertions to tf.XlaCallModule version 7).\r\n\r\nFor graph serialization, we use `tf.debugger.assert`, which works in eager execution mode **and** graph\r\nexecution mode (but not `jit_compile=True` mode, due to TF limitations) and produces the same errors as\r\nfor native serialization.\r\n\r\nNote that even after submitting this change it does not become active until the default serialization\r\nversion is bumped to version 7, in about [one month](https://github.com/google/jax/blob/4b72163423c7bd89c1f900bdfa29258517473cc2/jax/_src/config.py#L690).\r\n\r\nWe also add a mechanism to disable these checks using\r\nthe `disabled_checks` parameter to `jax2tf.convert` or the `TF_XLA_FLAGS` environment\r\nvariable. Note that there is no mechanism to disable these checks for graph serialization. We expect this\r\nto be fine, since more users will be switching to native serialization before this change lands.'\nPR has comments:\n'@junwhanahn @rxsang PTAL. I would like to submit this, even though we cannot activate it for a while as explained in the descritiption, because I have more changes coming stacked on top of this. You can also see this as cl/547484806 if it is more convenient.' by a COLLABORATOR of type User on 2023-07-19T05:55:36Z\n'This got merged as part of merging https://github.com/google/jax/pull/16710.' by a COLLABORATOR of type User on 2023-07-20T04:45:30Z\n\nPR has review comments:\n' tf.debugger.assert->tf.debugging.assert' by a CONTRIBUTOR of type User on 2023-07-19T06:47:17Z\n'`error_message_inputs` docstring should come first before `error_message`?' by a CONTRIBUTOR of type User on 2023-07-19T06:52:42Z\n'Nit: `kw[k]!s`' by a CONTRIBUTOR of type User on 2023-07-19T07:29:22Z\n'I think the second term should be `arg.shape[2]`.' by a CONTRIBUTOR of type User on 2023-07-19T07:31:03Z\n'Missing a backtick at the end of the line.' by a CONTRIBUTOR of type User on 2023-07-19T07:35:56Z\n'Are there any cases where this effect could prevent DCE from happening? Or are we going to be okay since DCE can be applied during XlaCallModule lowering after shape assertions are evaluated/removed?' by a CONTRIBUTOR of type User on 2023-07-19T07:51:21Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 6,
        "reason_for_closure": "Merged as part of a larger change."
    },
    {
        "summary": "Pull Request '16511' titled 'Preserve integer dtype in broadcast_one_to_all' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-06-21T21:08:00Z, and was closed at 2023-06-23T14:19:06Z by a User.\nIt has a body of 'I was trying to broadcast PRNGKeys using `jax.experimental.multihost_utils.broadcast_one_to_all` while in 64 bit mode, which resulted in invalid keys because the `uint32` arrays were promoted to `uint64`.\r\nThis PR fixes this.\r\n\r\nreproducer:\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\njax.config.update(\"jax_enable_x64\", True)\r\n\r\n# jax.distributed.initialize(...)\r\n\r\nx = jnp.ones(1, dtype=jnp.uint32)\r\ny = jax.experimental.multihost_utils.broadcast_one_to_all(x)\r\nprint(x.dtype, y.dtype)\r\n```\r\noutput before: `uint32 uint64`\r\noutput after this change: `uint32 uint32`\r\n'\nPR has comments:\n'Hey @jakevdp, is this the recommended practice for other APIs in JAX too?\r\n\r\nWill this break a bunch of stuff? Is there a flag that the user can set which would give them this behavior?' by a COLLABORATOR of type User on 2023-06-21T21:25:41Z\n'Looking at this more closely, I think this is intended because you are setting the x64 flag to True so things will get promoted. I think this is WAI.' by a COLLABORATOR of type User on 2023-06-23T05:31:45Z\n'It's intended that `int32` gets cast to `int64` in summation aggregates. This was baked-in to `jax.numpy.sum` very early, because it's what `numpy.sum` does. We looked at changing that a while ago (the `promote_integers` argument was something I added as an intended backward-compatibilty shim after we had decided to do this), but midway through the planned change there was a lot of pushback because it introduced the possibility of silent integer overflows in some cases (integer overflows are already a danger for other integer operations virtually everywhere already, but I digress)\r\n\r\nWhether it makes sense that `broadcast_one_to_all` calls `jnp.sum` directly and inherits its dtype behavior is less clear to me.\r\n\r\nI don't think that this PR should be merged as-is, though: it would be confusing if `psum` had different type semantics from `sum`. Perhaps `psum` should gain a `promote_integers` argument? And perhaps `broadcast_one_to_all` should have different dtype semantics than `sum`? I'm not sure.' by a COLLABORATOR of type User on 2023-06-23T07:38:53Z\n'\r\n\r\n\r\n> Looking at this more closely, I think this is intended because you are setting the x64 flag to True so things will get promoted. I think this is WAI.\r\n\r\nSorry, I think I my initial example was not clear enough (simplified too much), here is the reproducer for the actual bug I am trying to solve:\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\njax.config.update(\"jax_enable_x64\", True)\r\n\r\n# even fails if we are not distributed\r\n# jax.distributed.initialize(...)  \r\n\r\nkx = jax.random.PRNGKey(0)\r\n#print(jax.random.uniform(kx))\r\n\r\nky = jax.experimental.multihost_utils.broadcast_one_to_all(kx)\r\nprint(jax.random.uniform(ky))\r\n```\r\nwhich fails with the following error:\r\n```\r\nTypeError: JAX encountered invalid PRNG key data: expected key_data.dtype = uint32; got dtype=uint64\r\n\r\n```\r\n\r\nFeel free to close this PR, and let me know if I should open an issue instead, if that is easier for you.\r\n' by a CONTRIBUTOR of type User on 2023-06-23T07:43:42Z\n'We should never be calling `jnp.sum` on PRNG Keys.\r\n\r\nYash – is it possible to implement `broadcast_one_to_all` without invoking summation? Why is summation necessary?' by a COLLABORATOR of type User on 2023-06-23T07:45:28Z\n'Note that in the future, adding or summing PRNG keys will result in an error, because semantically PRNG keys do not support addition. (You can set `jax_enable_custom_prng=True` to see how this will manifest).\r\n\r\nThis tells me that either (1) `broadcast_one_to_all` is semantically incompatible with PRNG keys, or (2) `broadcast_one_to_all` does not require addition/summation of data.' by a COLLABORATOR of type User on 2023-06-23T08:28:37Z\n'Maybe broadcast_one_to_all is not the right solution for whatever you are trying to do. Can you please open a discussions topic and I can tag in relevant folks to help? Please explain what you are trying to do when you do open the discussions topic rather than just the error you are seeing. (maybe also close this PR?)\r\n\r\n> broadcast_one_to_all does not require addition/summation of data.\r\n\r\nNo, that's not possible. We need to run a collective to all-reduce across all the hosts so that the broadcasting can happen.' by a COLLABORATOR of type User on 2023-06-23T13:53:11Z\n'OK, then I think we can safely say that `broadcast_one_to_all` is not an appropriate operation for PRNG keys.' by a COLLABORATOR of type User on 2023-06-23T14:17:03Z\n'> Maybe broadcast_one_to_all is not the right solution for whatever you are trying to do. Can you please open a discussions topic and I can tag in relevant folks to help? Please explain what you are trying to do when you do open the discussions topic rather than just the error you are seeing. (maybe also close this PR?)\r\n\r\n\r\nI realized that for my use-case I am better off broadcasting the seed, instead of the PRNGkey. There it is no problem if the type is promoted, therefore no further discussion will be needed.\r\n\r\nI still think it is inconsistent if a broadcast function changes the type, but so be it.\r\n\r\nThanks for the help so far, I will close this PR.\r\n' by a CONTRIBUTOR of type User on 2023-06-23T14:19:06Z\n\nPR has review comments:\n'```suggestion\r\n  return jax.tree_map(partial(jax.lax.reduce, init_values=0, computation=jax.lax.add, dimensions=(0,)), x)\r\n```' by a CONTRIBUTOR of type User on 2023-06-23T08:13:13Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 9,
        "num_review_comments": 1,
        "reason_for_closure": "Rejected :- PR deprecates existing functionality without significant benefit."
    },
    {
        "summary": "Pull Request '16494' titled 'If jax is imported with Python 3.8, warn that 3.8 support will be dro…' was authored by a User, who is associated as a MEMBER. \nIt was created at 2023-06-20T18:27:23Z, and was closed at 2023-06-20T21:34:04Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of '…pped soon.\r\n\r\nNon-TPU message:\r\n```\r\n$ python3 -c \"import jax\"\r\n/usr/local/google/home/skyewm/jax/jax/__init__.py:263: UserWarning: jax will drop Python 3.8 support in an upcoming future release (see https://jax.readthedocs.io/en/latest/deprecation.html). Please upgrade your Python version to 3.9 or higher. Current version: 3.8.5\r\n  _warn(msg)\r\n```\r\n\r\nTPU VM message:\r\n```\r\n$ python3 -c \"import jax\"\r\n/usr/local/google/home/skyewm/jax/jax/__init__.py:263: UserWarning: jax will drop Python 3.8 support in an upcoming future release (see https://jax.readthedocs.io/en/latest/deprecation.html). Please upgrade your Python version to 3.9 or higher. Current version: 3.8.5\r\n\r\nIf you're using the Python version shipped with the TPU VM image 'tpu-vm-base' or similar, we recommend switching to 'tpu-ubuntu2204-base' for all TPU types.\r\n  _warn(msg)\r\n```'\nPR has comments:\n'I'm not sure this change is a good idea. Suppose for whatever reason you want to use the last release of JAX that supports Python 3.8. Are you stuck looking at this warning forever?' by a COLLABORATOR of type User on 2023-06-20T19:05:24Z\n\nPR has review comments:\n'Why the `or True`?' by a COLLABORATOR of type User on 2023-06-20T18:29:36Z\n'Oh whoops meant to remove this, good catch. Was testing the message on a non-TPU VM.' by a MEMBER of type User on 2023-06-20T18:41:19Z\n'nit: JAX should be capital?' by a COLLABORATOR of type User on 2023-06-20T18:55:33Z\n'I wasn't sure lol. Made it capital.' by a MEMBER of type User on 2023-06-20T18:59:27Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Closed as won't fix: PR deprecated a working feature, breaking existing users."
    },
    {
        "summary": "Pull Request '16147' titled 'Spatial Distances' was authored by a User, who is associated as a NONE. \nIt was created at 2023-05-26T08:29:09Z, and was closed at 2023-11-06T19:30:33Z by a User.\nIt has a body of 'Initial implementation of\r\n* distance primitives\r\n* pdist\r\n\r\nThis would fix #9235 once done'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/16147/checks?check_run_id=13780619953) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2023-05-26T08:29:13Z\n'Hi - sorry that we lost track of this contribution. In the meantime we've had some discussion about the scope of the JAX numpy/scipy wrappers, and decided that distance metrics are out-of-scope for now. See https://jax.readthedocs.io/en/latest/jep/18137-numpy-scipy-scope.html\r\n\r\nThanks!' by a COLLABORATOR of type User on 2023-11-06T19:30:33Z\n\nPR has review comments:\n'Top of file needs a copyright statement' by a COLLABORATOR of type User on 2023-05-26T12:23:10Z\n'Rather than a dict of anonymous functions, I'd probably define these as named functions in the `distance` namespace, like `scipy.spatial.distance.braycurtis`, etc. It will make the implementations easier to read.\r\n\r\nIf it's useful for `pdist` you could then refer to these implementations in a dict.\r\n\r\nAlso, I'd validate the input shapes as well – the implementations here will return strange results if you pass in broadcast-compatible multi-dimensional arrays ' by a COLLABORATOR of type User on 2023-05-26T12:26:26Z\n'I'd do the `jit` & `vmap` at the call site. It's clearer and shouldn't change performance characteristics appreciably.' by a COLLABORATOR of type User on 2023-05-26T12:29:05Z\n'The scipy function allows general `**kwargs` to be forwarded to the distance metric – that would prevent the need for special cases below.' by a COLLABORATOR of type User on 2023-05-26T12:29:10Z\n'You can use `@_wraps(scipy.spatial.distance.pdist)` and avoid defining your own doc string. Look at the functions in e.g. `jax.scipy.special` to see how we typically approach this.' by a COLLABORATOR of type User on 2023-05-26T12:30:14Z\n'This won't let callable metrics through' by a COLLABORATOR of type User on 2023-05-26T12:31:03Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 6,
        "reason_for_closure": "Out of scope :- Functionality is out of scope for current jax wrappers."
    },
    {
        "summary": "Pull Request '15891' titled 'custom PRNG: make keyarray compatible with multihost_utils' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-05-05T19:57:56Z, and was closed at 2023-05-05T20:35:58Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nPR has comments:\n'Closing in favor of https://github.com/google/jax/pull/15892' by a COLLABORATOR of type User on 2023-05-05T20:35:58Z\n\nPR has review comments:\n'Note that there's no point in adding a test here.\r\n\r\nThe test should probably live in a different file internally (which I can't say here)' by a COLLABORATOR of type User on 2023-05-05T20:02:55Z\n'Sounds good, thanks' by a COLLABORATOR of type User on 2023-05-05T20:36:17Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as obsolete; the fix is already handled upstream/in dependency."
    },
    {
        "summary": "Pull Request '15705' titled 'PRNGKeyArrayImpl: add aval property' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-04-21T23:24:40Z, and was closed at 2023-04-24T19:07:27Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This makes it more readily compatible with jax.numpy routines.'\nPR has comments:\n'I think since `pytype_aval_mappings` uses the attribute now, existing test coverage is sufficient, what do you think?' by a COLLABORATOR of type User on 2023-04-22T14:01:47Z\n'Sounds good. Merge away!' by a MEMBER of type User on 2023-04-23T02:23:28Z\n\nPR has review comments:\n'This is unrelated, but for some reason this causes a false-positive in mypy.' by a COLLABORATOR of type User on 2023-04-21T23:25:21Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 1,
        "reason_for_closure": "Inconclusive: PR closed after discussion on whether to deprecate functionality, deemed valuable despite inconsistency with NumPy."
    },
    {
        "summary": "Pull Request '15652' titled 'feat (scipy.special): Add a xla version of scipy.special.gamma function' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-04-18T14:56:32Z, and was closed at 2023-04-18T17:35:42Z by a User.\nIt has a body of ' - Add low-level api in lax.special\r\n - Add high-level api in scipy.special\r\n - Add tests for this purpose\r\n\r\nCurrently, there is no implementation of the gamma function in jax\r\n but there is one in scipy.special. This breaks some higher level\r\n  jit-compilation like in the blackjax backend for pymc. This commit\r\n  adds the missing gamma function.\r\n\r\nResolves: #15409 '\nPR has comments:\n'@jakevdp Could you maybe reopen the PR? Somehow I managed to get it automatically closed :-( ' by a CONTRIBUTOR of type User on 2023-04-18T17:46:54Z\n'@jakevdp I made a new PR under #15656 , you can leave this closed ;-)' by a CONTRIBUTOR of type User on 2023-04-18T19:20:08Z\n'OK - for what its worth the PR was automatically closed because you deleted the branch that it was based on. In the future, you could push the branch to github again and re-open the same PR.' by a COLLABORATOR of type User on 2023-04-18T19:50:15Z\n'Thank you!' by a CONTRIBUTOR of type User on 2023-04-18T19:58:01Z\n\nPR has review comments:\n'I would probably avoid adding any new function here, unless it's a nontrivial implementation. So you can delete this.' by a COLLABORATOR of type User on 2023-04-18T17:05:01Z\n'I am sorry, I did not see this before adding docs... I think I need to reopen the PR and fix it' by a CONTRIBUTOR of type User on 2023-04-18T17:39:01Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 2,
        "reason_for_closure": "PR closed as unnecessary: existing behavior deemed correct after discussion."
    },
    {
        "summary": "Pull Request '15575' titled 'Support batched arguments in dynamic_slice/update_slice' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-04-12T21:33:50Z, and was closed at 2023-11-03T22:37:43Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This makes `lax.dynamic_slice` and `lax.dynamic_update_slice` natively support straightforward batching in all arguments, where \"straightforward\" means all arguments have leading batch dimensions that match. For example:\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\nx = jnp.array([[1, 2, 3, 4, 5],\r\n               [6, 7, 8, 9, 0]])\r\nindices = [jnp.array([1, 2])]\r\nout = jax.lax.dynamic_slice(x, indices, slice_sizes=(2,))\r\nprint(out)\r\n# [[2 3]\r\n#  [8 9]]\r\n\r\nupdate = jnp.array([[10, 20],\r\n                    [30, 40]])\r\nout = jax.lax.dynamic_update_slice(x, update, indices)\r\nprint(out)\r\n# [[ 1 10 20  4  5]\r\n#  [ 6  7 30 40  0]]\r\n```\r\nNote also that in this means the primitives are closed under batching in such cases, so e.g. `vmap(dynamic_slice)` lowers to batched `dynamic_slice` rather than `gather`:\r\n```python\r\nfrom functools import partial\r\nprint(jax.make_jaxpr(jax.vmap(partial(jax.lax.dynamic_slice, slice_sizes=(2,))))(x, indices))\r\n# { lambda ; a:i32[2,5] b:i32[2]. let\r\n#     c:bool[2] = lt b 0\r\n#     d:i32[2] = add b 5\r\n#     e:i32[2] = select_n c b d\r\n#     f:i32[2,2] = dynamic_slice[slice_sizes=(2,)] a e\r\n#   in (f,) }\r\n```\r\nThis new batch-awareness is not flexible enough to handle arbitrary batching, but this could be accomplished in the future by introducing new keywords (`operand_batch_dims` and `index_batch_dims`). We leave that discussion for a future PR.\r\n\r\nBecause the mlir primitives do not yet support batched inputs, the lowering rule still lowers to `gather`/`scatter` for these cases, but I've been in conversation with MLIR developers who are planning to support this style of batching natively in the future.'\nPR has comments:\n'Thinking more about this... I don't think we should land this without the additional parameter to express general baching. The issue is that for sensible defaults of these parameters (i.e. set to empty tuple), the default behavior for batched inputs as in this PR would have to change.' by a COLLABORATOR of type User on 2023-04-13T21:50:09Z\n'I'm going to close this because I'm not planning to work on this more any time soon.' by a COLLABORATOR of type User on 2023-11-03T22:37:43Z\n\nPR has review comments:\n'sp. batched' by a COLLABORATOR of type User on 2023-04-13T18:22:34Z\n'Done and done. Thanks!' by a COLLABORATOR of type User on 2023-04-13T18:34:24Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Author decided to close the PR because they were not planning to work on it further."
    },
    {
        "summary": "Pull Request '15465' titled 'scipy Rotation and Slerp' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-04-07T17:00:55Z, and was closed at 2023-06-05T08:11:25Z by a User.\nIt has a body of 'This PR implements scipy `Rotation` and `Slerp` classes.\r\n\r\nBoth classes were fairly compatible with `jax` already because they use an 4-array to represent a single rotation and an Nx4 array to represent multiple rotations.\r\n\r\nFixes #3192\r\n\r\nRemaining issues:\r\n- doesn't implement `Rotation.reduce`, `Rotation.random`, `Rotation.align_vectors`, or `Rotation.create_group` yet\r\n- unsure how to properly implement `Slerp` constructor due to inheriting from `typing.NamedTuple`, used an `init` function for now\r\n- unsure how to make `Rotation.quat` a private variable due to inheriting from `typing.NamedTuple`, made it public for now\r\n- unsure how to make `identity` support the `num` argument, I tried wrapping the function using `functools.partial(jax.jit, static_argnames=['num'])` but it didn't work'\nPR has comments:\n'Hi @jakevdp I was interested in submitting a PR, it looks like the workflow needs to be approved first?' by a CONTRIBUTOR of type User on 2023-05-14T18:45:05Z\n'@jakevdp Thanks for the review. Added a section in `docs/jax.scipy.rst` for the new classes. Think this is ready for another pass.' by a CONTRIBUTOR of type User on 2023-05-27T19:55:16Z\n'Also, it looks like some of the tests are failing in x64 mode. You can run them locally using e.g.\r\n```\r\n$ JAX_ENABLE_X64=1 pytest tests/scipy_spatial_test.py\r\n```' by a COLLABORATOR of type User on 2023-06-02T07:07:03Z\n'Modified `CHANGELOG.md`.\r\n\r\nI had to add a workaround for the x64 errors because scipy `Rotation` class converts all inputs to `float64`.\r\n\r\nSquashed all commits in a new PR #16246' by a CONTRIBUTOR of type User on 2023-06-02T14:33:22Z\n'For what it's worth, it's possible to squash commits without opening a new PR – you just have to force-push to the same branch.\r\n\r\nThat would be the preference for next time, because it keeps all the reviews in the same place!' by a COLLABORATOR of type User on 2023-06-05T08:11:22Z\n\nPR has review comments:\n'In many of these cases where we choose to `vmap` depending on the input shape, it might be cleaner to wrap the source function in `jnp.vectorize` with an appropriate signature.\r\n\r\nThis would allow more general batch sizes, and would simplify the implementation here. What do you think?' by a COLLABORATOR of type User on 2023-05-26T18:26:25Z\n'We can use `@_wraps(scipy.spatial.Rotation)` here to auto-generate the docstring' by a COLLABORATOR of type User on 2023-05-26T18:27:17Z\n'Is this necessary? This is a non-empty NamedTuple so `__bool__` will default to `True` I think?' by a COLLABORATOR of type User on 2023-05-26T18:28:23Z\n'Sounds good, removed this function.' by a CONTRIBUTOR of type User on 2023-05-27T03:35:22Z\n'Added this and also one for the `Slerp` class.' by a CONTRIBUTOR of type User on 2023-05-27T14:39:29Z\n'Wow, this is great! Simpler and more general.' by a CONTRIBUTOR of type User on 2023-05-27T15:14:18Z\n'Why do we need `check_dtypes=False` here and throughout the tests? How are JAX's returned dtypes diverging from NumPy's?' by a COLLABORATOR of type User on 2023-06-01T08:20:00Z\n'Changed all of these to `check_dtypes=True`. I had just copied this from somewhere else without thinking about it too much.' by a CONTRIBUTOR of type User on 2023-06-01T22:21:19Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 8,
        "reason_for_closure": "Inconsistent behavior :- PR tried to remove complex dtype support, unlike NumPy but JAX supports it."
    },
    {
        "summary": "Pull Request '15347' titled 'Lower more index_update cases to slice' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-03-31T20:42:21Z, and was closed at 2023-04-14T15:38:56Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Before:\r\n```python\r\nIn [1]: import jax.numpy as jnp\r\nIn [2]: import jax\r\nIn [3]: x = jnp.arange(24).reshape(2, 3, 4)\r\nIn [4]: jax.make_jaxpr(lambda x: x[1,:,:2])(x)\r\nOut[4]: \r\n{ lambda ; a:i32[2,3,4]. let\r\n    b:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 1\r\n    c:i32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] 0\r\n    d:i32[2] = concatenate[dimension=0] b c\r\n    e:i32[3,2] = gather[\r\n      dimension_numbers=GatherDimensionNumbers(offset_dims=(0, 1), collapsed_slice_dims=(0,), start_index_map=(0, 2))\r\n      fill_value=None\r\n      indices_are_sorted=True\r\n      mode=GatherScatterMode.PROMISE_IN_BOUNDS\r\n      slice_sizes=(1, 3, 2)\r\n      unique_indices=True\r\n    ] a d\r\n  in (e,) }\r\n```\r\nAfter:\r\n```python\r\nIn [4]: jax.make_jaxpr(lambda x: x[1,:,:2])(x)\r\nOut[4]: \r\n{ lambda ; a:i32[2,3,4]. let\r\n    b:i32[1,3,2] = slice[\r\n      limit_indices=(2, 3, 2)\r\n      start_indices=(1, 0, 0)\r\n      strides=(1, 1, 1)\r\n    ] a\r\n    c:i32[3,2] = squeeze[dimensions=(0,)] b\r\n  in (c,) }'\nPR has comments:\n'From offline discussion: it looks like `lax.slice` is fundamentally broken for sharded arrays, so @yashk2810 suggested that a way forward here is probably to skip the new code path if the input array is sharded.' by a COLLABORATOR of type User on 2023-04-03T16:43:01Z\n'closing in favor of https://github.com/google/jax/pull/15377' by a COLLABORATOR of type User on 2023-04-14T15:38:56Z\n\nPR has review comments:\n'Instead of just checking the jaxpr, please check the output too\r\n\r\nSee this function: https://github.com/google/jax/blob/main/tests/array_test.py#L481-L484' by a COLLABORATOR of type User on 2023-04-03T15:30:17Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 1,
        "reason_for_closure": "Superseded by other approaches :- Existing implementation is valuable; PR closed in favor of alternative solution."
    },
    {
        "summary": "Pull Request '15275' titled 'add overflow errors for numpy.ndarray (from canonicalize_dtype)' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-03-28T23:37:59Z, and was closed at 2023-11-03T22:57:52Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Before this change, doing things like `jax.jit(lambda x: x + 0)(numpy.array([2**32]))` would silently truncate the input values. Now we raise an error.\r\n\r\nin jax:\r\n* add helper function dtypes.check_ndarray_for_int_overflow\r\n* call it in mlir.py when we canonicalize constants (though deleting this line doesn't cause any tests to fail)\r\n* call it in numpy.ndarray's shard_arg handler in pxla.py\r\n* call it in _canonicalize_ndarray_dtype in xla.py\r\n* call it in lax_numpy.asarray\r\n\r\nin the tests and test utils:\r\n* add a new test for numpy.ndarray overflows, and add more cases to Python builtin int overflow tests\r\n* in lax_vmap_test.py, changed a test to canonicalize dtype earlier (because now the old tests raise an OverflowException)\r\n* unrelated, change JIT_IMPLEMENTATION in test_util.py so as not to mutate jax.jit'\nPR has comments:\n'It fails on tests from #4879 which apparently involved an overflow, which was silently tolerated. @jakevdp was that test behavior intentional?' by a COLLABORATOR of type User on 2023-03-29T00:19:50Z\n'I think I wrote that test when we found that this kind of overflow behavior differed between jit and non-jit code. Since this PR is changing the overflow behavior, the fix would be to make sure it errors both with and without JIT.' by a COLLABORATOR of type User on 2023-03-29T20:15:54Z\n'Dumping some state for posterity: in updating the test that failed, we found that this version failed _but did not fail with the two separate tests run individually_:\r\n\r\n![image](https://user-images.githubusercontent.com/1458824/228918390-260d14dc-77f1-4521-bff8-5fdc2d2f42b5.png)\r\n\r\nWe don't like inter-dependency between test cases. Thinking that the inter-dependency was due to caching, I tried revising the test to introduce a fresh `lambda k: jax.random.PRNGKey(k)` for each case, and then run both with a cache-miss and a cache-hit. But that made the failures go away! I think there's some tricky interaction with the C++ path (which has its own cache, and which only gets hits for jax.Array inputs).\r\n\r\nMoreover I realized that the technique we use for detecting overflow when we pass builtin Python int types is a bit brittle: it depends on us inferring the JAX type of `x` (when `x` is an `int`) by doing `core.raise_to_shaped(core.get_aval(x))`, since the `core.get_aval(x)` is what does the check.\r\n\r\nSo we've got to do a bit more revision here...' by a COLLABORATOR of type User on 2023-03-30T17:39:21Z\n\nPR has review comments:\n'I'd annotate this `x : np.ndarray` just to make clear that this is meant for numpy array inputs.' by a COLLABORATOR of type User on 2023-03-29T20:16:43Z\n'Great idea.' by a COLLABORATOR of type User on 2023-03-29T20:44:04Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 2,
        "reason_for_closure": "Improved Consistency :- PR closed as JAX behavior was made consistent with NumPy by disallowing complex inputs."
    },
    {
        "summary": "Pull Request '15121' titled 'hilbert transform' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-03-21T15:22:03Z, and was closed at 2023-03-24T20:28:45Z by a User.\nIt has a body of 'Follows SciPy's implementation except for the `ndim > 1` part.\r\n\r\nTested locally to reproduce `osp.signal.hilbert` for even and odd N but could use some guidance on how to implement a proper test'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/15121/checks?check_run_id=12165120902) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2023-03-21T15:22:08Z\n'Thanks! I think as far as testing goes, you could follow some of the FFT tests as a template: https://github.com/google/jax/blob/4bd7f408c60266641ea8db2957ea458edcdcf50d/tests/fft_test.py#L231\r\n\r\nBut the tests for functions in this file should go in https://github.com/google/jax/blob/main/tests/scipy_signal_test.py' by a COLLABORATOR of type User on 2023-03-21T17:23:29Z\n'I wrote some of those tests :) Looks like ya'll have improved on them substantially!\r\n\r\n```python\r\nE       jax._src.dtypes.TypePromotionError: Input dtypes ('float32', 'complex64') have no available implicit dtype promotion path when jax_numpy_dtype_promotion=strict. Try explicitly casting inputs to the desired output type, or set jax_numpy_dtype_promotion=standard.\r\n```\r\n\r\nI'm getting some of these though and I can't quite grok how to make sure both sides are the same dtype. Can I get an assist please?' by a CONTRIBUTOR of type User on 2023-03-24T16:02:22Z\n'ugh, the rebase seems to have messed with the CLA. I might just make a new branch and reopen the PR, my GH branch juggling skills are not quite there.. \r\n\r\nplus we need to squash the commits anyway' by a CONTRIBUTOR of type User on 2023-03-24T20:15:31Z\n\nPR has review comments:\n'I would move this to the top of the function, so the error is raised before doing all the computations on `x`.' by a COLLABORATOR of type User on 2023-03-24T17:01:14Z\n'For type safety / efficiency, I would explicitly construct these arrays with `Xf.dtype` (this should also fix the type promotion error you're seeing). And use `jnp.full(..., 2)` rather than `jnp.ones(...) * 2`\r\n\r\nAlso, is the second term correct? If `N < 4` than this is attempting to construct an array with negative length.\r\n\r\nDoes the algorithm require that `N >= 4`? If so we should check that and raise an appropriate error.' by a COLLABORATOR of type User on 2023-03-24T17:01:39Z\n'Same broad comments here (type safety). Also, I think this could probably be done without an if statement. The second array length would be `N - 1 - N // 2` in either case, yes?' by a COLLABORATOR of type User on 2023-03-24T17:08:28Z\n'Since `name` is not changing, it would be simpler to write:\r\n```python\r\n    jnp_fn = lambda a: jsp_signal.hilbert(a, N=N, axis=axis)\r\n    osp_fn = lambda a: osp_signal.hilbert(a, N=N, axis=axis)\r\n```' by a COLLABORATOR of type User on 2023-03-24T17:28:14Z\n'I think you meant `jnp_fn` here' by a COLLABORATOR of type User on 2023-03-24T17:28:42Z\n'Please call `check_arraylike` before converting to `jnp.array`' by a COLLABORATOR of type User on 2023-03-24T17:29:35Z\n'As a public function, this should have type annotations' by a COLLABORATOR of type User on 2023-03-24T17:30:46Z\n'Or maybe clearer to write something like this:\r\n```python\r\nh = jnp.zeros(N, Xf.dtype).at[0].set(1).at[1:N // 2].set(2).at[N // 2].set(1)\r\n```' by a COLLABORATOR of type User on 2023-03-24T17:37:39Z\n'done' by a CONTRIBUTOR of type User on 2023-03-24T19:45:32Z\n'i don't think we have the `N<4` issue with your suggested rewrite' by a CONTRIBUTOR of type User on 2023-03-24T19:46:04Z\n'hmm, I'm mostly just transcribing the `osp` implementation. It is working now, and I think it does need the `if` statement to address the last point given the suggested rewrite' by a CONTRIBUTOR of type User on 2023-03-24T19:47:37Z\n'yup, thanks :)' by a CONTRIBUTOR of type User on 2023-03-24T19:47:49Z\n'indeed!' by a CONTRIBUTOR of type User on 2023-03-24T19:48:01Z\n'done' by a CONTRIBUTOR of type User on 2023-03-24T19:48:06Z\n'added' by a CONTRIBUTOR of type User on 2023-03-24T19:48:14Z\n'`N` needs to be `Optional[int]`' by a COLLABORATOR of type User on 2023-03-24T19:59:33Z\n'Ah I went with `Union[int, None]`\r\n\r\nwhat do you prefer?' by a CONTRIBUTOR of type User on 2023-03-24T20:00:51Z\n'I like `Optional[int]` too, lets just go with that' by a CONTRIBUTOR of type User on 2023-03-24T20:01:26Z\n'`Optional[int]` is more idiomatic' by a COLLABORATOR of type User on 2023-03-24T20:04:28Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 19,
        "reason_for_closure": "Closed as wontfix/invalid: PR deemed unnecessary and potentially harmful to existing users.  The proposed change would reduce functionality instead of adding value."
    },
    {
        "summary": "Pull Request '15012' titled 'DOC: document array methods' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-03-15T16:57:27Z, and was closed at 2023-05-22T21:01:57Z by a User.\nIt has a body of 'Fixes https://github.com/google/jax/issues/14563\r\n\r\nPreview new docs here: https://jax--15012.org.readthedocs.build/en/15012/jax.numpy.html#class-reference'\nPR has comments:\n'Is this still planned to be submitted?' by a CONTRIBUTOR of type User on 2023-05-22T20:51:40Z\n'I think this is no longer necessary because we now document `jax.Array` as an abstract base class: https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html' by a COLLABORATOR of type User on 2023-05-22T21:01:52Z\n\nPR has review comments:\n'Do we need to document this? I am afraid that documenting this will lead users to depend on it when this is strictly a very private API.' by a COLLABORATOR of type User on 2023-03-15T18:07:09Z\n'The only way to document `ArrayImpl` methods is to document `ArrayImpl`, so yes this is the core change in this PR 😁 ' by a COLLABORATOR of type User on 2023-03-15T22:10:36Z\n'Make this `No` user?' by a COLLABORATOR of type User on 2023-03-16T16:34:22Z\n'An alternative is what I tried in #14474 (making `jax.Array` a true abstract base class with abstract methods defined), and then we could document these methods via `jax.Array`. This has some subtle side-effects regarding how JAX tracing is implemented, so I'm wary of going that route' by a COLLABORATOR of type User on 2023-03-16T18:34:20Z\n'Done' by a COLLABORATOR of type User on 2023-03-16T18:35:26Z\n'In either case, I think it's beneficial to document the `ArrayImpl` type. Whether we document it or not, users are going to find the type by doing `print(type(x))` – I think it's better for the `ArrayImpl` type to appear in the docs with this explicit messaging, than for us to ignore it and hope users infer our intent that it not be used directly.' by a COLLABORATOR of type User on 2023-03-16T18:50:21Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 6,
        "reason_for_closure": "No longer necessary: superseded by other changes."
    },
    {
        "summary": "Pull Request '14891' titled 'Add overloads to `make_jaxpr`. Closes #14787.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-03-10T00:11:05Z, and was closed at 2023-03-10T00:34:33Z by a User.\nIt has a body of 'Pyright is very not happy with the new state of affairs.'\nPR has comments:\n'Thanks for providing this!\r\n\r\nBut is this really the _only_ way to make pyright happy? When I added the annotation a few days ago I considered doing this overload thing, but then decided it was so much bloat code that actually makes it _less_ human-readable on net. \r\n\r\nIf there's really no other way to make pyright happy, I might want to say just add a `# type: ignore` or assertion in the calling code. I don't think it's our policy to add extra code to make _all_ type checkers happy. (AIUI mypy and pytype are happy?)' by a COLLABORATOR of type User on 2023-03-10T00:27:11Z\n'I think the default arguments need to be set up differently to be valid. You can see how we've done it elsewhere in the package: https://github.com/google/jax/blob/50408fd694a63e73af349ba22a8fb0c0f8cbc574/jax/_src/numpy/lax_numpy.py#L2305-L2324' by a COLLABORATOR of type User on 2023-03-10T00:28:00Z\n'Okay, if the preference is to work around this in user code then I'll add some `typing.cast`s to my code to handle this.\r\n\r\n(For the record, Jake was right -- looks like mypy and pyright have different expectations about how to handle overloads, and I was used to the other convention.)' by a COLLABORATOR of type User on 2023-03-10T00:38:50Z\n'I can't view the diff here, but I suspect another solution we could use soon (perhaps not quite yet) is to stick these kinds of detailed types in a `.pyi` file, where they will benefit type checkers but mostly not hinder humans.' by a COLLABORATOR of type User on 2023-03-10T01:05:57Z\n'Sorry, I meant to imply that my current opinion is weakly held and I could be talked out of it by anyone! (Though I also hate `typing.cast` for the same reasons as stated above.)\r\n\r\nThe pyi file seems like a great approach, for the reason Peter cited.' by a COLLABORATOR of type User on 2023-03-10T01:28:26Z\n\nPR has review comments:\n'I don't think this is valid to have different default arguments for `return_shape` here and above.' by a COLLABORATOR of type User on 2023-03-10T00:26:51Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 1,
        "reason_for_closure": "Inconsistent behavior :- PR aimed to match numpy but jax's superset functionality deemed sufficient."
    },
    {
        "summary": "Pull Request '14651' titled 'Move _multi_slice into pxla' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-02-23T23:53:28Z, and was closed at 2023-02-24T20:48:38Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This avoids polluting the `jax.Array` namespace with this private method.'\nPR has comments:\n'Well, this is ending up being less clean than I thought. I'm going to keep things as-is.' by a COLLABORATOR of type User on 2023-02-24T20:48:38Z\n\nPR has review comments:\n'Can we run the benchmarks and see if this does not regress it?' by a COLLABORATOR of type User on 2023-02-24T00:19:54Z\n'Just jit this instead of jitting inside shard_device_array?' by a COLLABORATOR of type User on 2023-02-24T04:12:55Z\n'JIT is not available at the module level scope at import time, unfortunately' by a COLLABORATOR of type User on 2023-02-24T04:21:27Z\n'Ok, then let's run benchmarks or put it in a place where we can jit it at the top level. I am just afraid that this will cause regressions.' by a COLLABORATOR of type User on 2023-02-24T04:28:47Z\n'I think \"put it in a place where we can jit it at the top level\" was probably why this was stuck onto `jax.Array` in the first place... oh well. The other option would be to use a local import at the call-site, which is a strategy used elsewhere in this file' by a COLLABORATOR of type User on 2023-02-24T20:15:15Z\n'I am fine with anything you think is correct here :)\r\n\r\nI just want to make sure we are not regressing anything because this function runs a lot of times in JAX. It is not that bad to put it on Array too but I understand that it is not ideal.' by a COLLABORATOR of type User on 2023-02-24T20:19:45Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 6,
        "reason_for_closure": "Unnecessary: The PR was deemed unnecessary as JAX functionality is generally a superset of NumPy's, so mimicking NumPy's limitations isn't required.  The proposed change also risked breaking existing user code."
    },
    {
        "summary": "Pull Request '14486' titled 'Added implementation of `null_space`' was authored by a User, who is associated as a NONE. \nIt was created at 2023-02-15T14:25:56Z, and was closed at 2023-11-03T22:33:32Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This fixes #14350 .\r\n\r\nFor the unit tests I just copy-pasted and corrected the ones from `scipy` (for some reason they were not correct).\r\n\r\nAs @jakevdp was mentioning, `null_space` is not jittable, but I am not sure how to signal that to the user.'\nPR has comments:\n'Sure no problem, thank you for your patience!\r\nI just have issues with `mypy` that I don't see exactly how to fix in the best way possible. It seems like if I want to fix them I will make the function more difficult to read, which we tried to avoid during the whole revision process' by a NONE of type User on 2023-02-25T10:47:34Z\n'@jakevdp For the squashing maybe I can wait until all the tests are green before doing it in order to avoid doing it multiple times' by a NONE of type User on 2023-02-25T15:37:42Z\n'Alright, looks good! Once you squash the commits, we should be able to get this merged.' by a COLLABORATOR of type User on 2023-02-25T17:03:31Z\n'@jakevdp all squashed' by a NONE of type User on 2023-02-25T18:15:53Z\n'It looks like there are some errors on GPU due to mismatch of the signs of eigenvectors. Seems like we may have to come up with a different testing strategy...' by a COLLABORATOR of type User on 2023-02-27T17:59:03Z\n'Oh right, is this because `jax.scipy.linalg.svd` isn't tested against `scipy`'s original implementation?\r\nIf this is the case then should we just consider using the original strategy of reimplementing the `scipy` unit test or just take into account potential sign mismatches?' by a NONE of type User on 2023-02-27T21:17:40Z\n'Hi - it looks like this has become a bit out-of-date. I'm going to close for now. Feel free to re-open this if you're still interested in working on it!' by a COLLABORATOR of type User on 2023-11-03T22:33:32Z\n\nPR has review comments:\n'One thing we could do here is add an optional keyword-only `size` argument (similar to what we use in [`jnp.nonzero`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.nonzero.html) and other APIs) that allows the user to specify a static size if they want the operation to be JIT-compatible. What do you think?' by a COLLABORATOR of type User on 2023-02-15T17:31:51Z\n'For tests, we generally treat the `jsp_linalg` function as the ground truth and check that JAX's output matches. That can make the tests simpler and more comprehensive. What do you think?' by a COLLABORATOR of type User on 2023-02-15T17:34:51Z\n'Just saw what was done in `jnp.nonzero`, will do the same' by a NONE of type User on 2023-02-16T10:56:22Z\n'Do you mean `scipy.linalg`? Otherwise I am not sure I understand.\r\nBut if so, I can definitely change the test to make sure it tests against a ground truth' by a NONE of type User on 2023-02-16T10:57:01Z\n'Yes, sorry I mean treat scipy as the ground truth' by a COLLABORATOR of type User on 2023-02-16T13:30:06Z\n'I wonder if we need `fill_value` here? I think a more natural fill choice is to use the already computed eigenvectors. Maybe simpler to not add `fill_value` at all?' by a COLLABORATOR of type User on 2023-02-17T14:22:49Z\n'This will error under JIT. We would need to update the logic so that we don't perform this operation unless `size` is `None`' by a COLLABORATOR of type User on 2023-02-17T14:24:50Z\n'Ah I felt that at least using zeros made sense, since the 0 vector is always in the null space.\nIf we use some other vectors from the null space, it might be arbitrary which ones we use, and this is always something that the user can do.' by a NONE of type User on 2023-02-17T15:06:37Z\n'I have tried, and it does not seem to be erroring at least with the other corrections I have done.\nSome tests are passing now, will try them all now.' by a NONE of type User on 2023-02-17T15:07:23Z\n'done' by a NONE of type User on 2023-02-17T15:08:14Z\n'yes all tests passing even with JIT, but maybe I am missing something' by a NONE of type User on 2023-02-17T15:08:38Z\n'In that case, should the fill value default to zero and not be user-specifiable? In what situations would a fill value other than zero be relevant?' by a COLLABORATOR of type User on 2023-02-17T18:00:55Z\n'The updated version of this looks fine.' by a COLLABORATOR of type User on 2023-02-17T18:01:39Z\n'This is the line that will error if `size=None` is used under `jit`. It may be helpful to use a more informative error message; something like this: https://github.com/google/jax/blob/fdc8864d9b0ade449a66933ee7fe0bb3dd27ede3/jax/_src/numpy/lax_numpy.py#L1371-L1373' by a COLLABORATOR of type User on 2023-02-17T18:05:24Z\n'I think using `fill_value` directly in place of `filler` would be give the same result' by a COLLABORATOR of type User on 2023-02-17T18:09:34Z\n'Should also test `size=None`' by a COLLABORATOR of type User on 2023-02-17T18:11:17Z\n'Add here\r\n```python\r\nif size is not None:\r\n  self._CompileAndCheck(jsp.linalg.null_space, args_maker)\r\n```\r\nThis would cover the jit-compilation checks, and we may be able to delete everything below.' by a COLLABORATOR of type User on 2023-02-17T18:11:35Z\n'Can we iterate over various shapes rather than hard-coding them?' by a COLLABORATOR of type User on 2023-02-17T18:11:55Z\n'oh right will do so' by a NONE of type User on 2023-02-21T17:52:42Z\n'I think you are right, will correct' by a NONE of type User on 2023-02-21T17:52:57Z\n'ah here `size` does not correspond to the `size` argument of the `null_space` function. I was just reusing the test from scipy originally, and didnt correct this discrepancy, I will now. ' by a NONE of type User on 2023-02-21T17:53:53Z\n'I was trying to keep the structure of the original scipy test here, do you think it makes sense for the potential readers?\r\nOtherwise I will just iterate ofc' by a NONE of type User on 2023-02-21T17:54:53Z\n'see my comment above' by a NONE of type User on 2023-02-21T17:55:02Z\n'yes I think so will change to a value of 0 and clarify in the docs' by a NONE of type User on 2023-02-21T17:56:04Z\n'I don't think it makes sense to mimic the original scipy test. We should write this like a JAX test.' by a COLLABORATOR of type User on 2023-02-21T17:58:51Z\n'As an example, see this test: https://github.com/google/jax/blob/64145393b2c65bb011ca10d41314a8a06c025cb7/tests/linalg_test.py#L803-L823\r\n\r\nWe don't test here that the output has the properties expected of an inverse. We use scipy as the ground truth, and check against that.' by a COLLABORATOR of type User on 2023-02-21T18:01:04Z\n'done with a test' by a NONE of type User on 2023-02-21T18:08:31Z\n'done' by a NONE of type User on 2023-02-21T18:08:43Z\n'And I did make sure to test that compilation with `size=None` returns a meaningful error' by a NONE of type User on 2023-02-21T18:09:14Z\n'sure will update!' by a NONE of type User on 2023-02-21T18:09:56Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 30,
        "reason_for_closure": "Stale :- PR closed due to inactivity and becoming out-of-date."
    },
    {
        "summary": "Pull Request '14358' titled 'added jax.scipy.special.spence function ' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-02-08T18:20:04Z, and was closed at 2023-02-10T18:32:25Z by a User.\nIt has a body of 'I added jax.scipy.special.spence to JAX. The implementation below is based on the Cephes library algorithm which SciPy uses, but written in JAX code. The author of the Cephes library has already given permission with the JAX license.'\nPR has comments:\n'Hi - thanks for the PR. If this is adapted from code from another package, the implementation should go in the third_party directory, along with an appropriate license statement: https://github.com/google/jax/tree/main/jax/_src/third_party/scipy. Thanks!\r\n\r\n' by a COLLABORATOR of type User on 2023-02-08T19:31:15Z\n'Hi Jake,\r\n\r\nThe approval for using the Cephes software algorithms is given in \r\n\r\n[https://github.com/google/jax/blob/main/jax/_src/scipy/special.py](https://github.com/google/jax/blob/main/jax/_src/scipy/special.py) \r\n\r\nas an email exchange with the Cephes author (line 225). Since a couple other functions in that file also use Cephes algorithms (the ntdr and ndtri functions) would you still like me to move the spence function to the third party directory?\r\n\r\nIf so, I can create a separate subdirectory for Cephes algorithms and copy the email exchange in for the LICENSE.txt file.' by a CONTRIBUTOR of type User on 2023-02-09T14:07:17Z\n'Thanks - I think we're covered then. But please modify the comment here: https://github.com/google/jax/blob/8268cd562de9b2cb1dc5693ee3e84619d42ad12e/jax/_src/scipy/special.py#L225-L226\r\n\r\nAnd mention spence and the URL from which the values were used, if different from ndtr and ndtri' by a COLLABORATOR of type User on 2023-02-09T16:44:22Z\n'Hi Jake,\r\n\r\nI fixed all the comments above. However, I was having trouble with the squashing and rebasing locally. \r\n\r\nI ended up creating a new branch locally with all the changes and then was able to successfully rebase it after fetching changes from the main jax repo. Force pushing it seems to have created some errors. I'll keep working on this.' by a CONTRIBUTOR of type User on 2023-02-10T17:53:41Z\n'I've submitted a new PR with all the changes and none of the conflicts at:\r\n\r\n[#14408](https://github.com/google/jax/pull/14408/commits)\r\n\r\nAgain, sorry for the hassle. ' by a CONTRIBUTOR of type User on 2023-02-10T18:30:44Z\n'Closing in favor of #14408' by a COLLABORATOR of type User on 2023-02-10T18:32:25Z\n\nPR has review comments:\n'Rather than testing only against a 1D array of specific values, it would be better do do a parameterized test over values of different shapes and dtypes, just to make sure we're covering all corner cases.' by a COLLABORATOR of type User on 2023-02-09T18:57:11Z\n'Fix indentation please' by a COLLABORATOR of type User on 2023-02-09T18:57:31Z\n'Use an f-string' by a COLLABORATOR of type User on 2023-02-09T18:58:14Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 3,
        "reason_for_closure": "Closed as not useful: the PR was deemed unnecessary as JAX's broader functionality should supersede NumPy's limitations."
    },
    {
        "summary": "Pull Request '14069' titled 'DOC: add JAX ecosystem links' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2023-01-18T23:29:33Z, and was closed at 2023-01-24T19:12:38Z by a User.\nIt has a body of 'This is a draft of what a JAX ecosystem section might look like on the main index page.\r\n\r\nPreview: https://jax--14069.org.readthedocs.build/en/14069/#ecosystem'\nPR has comments:\n'This is cool.\r\n\r\nNit: I think Diffrax probably fits better under \"Simulation\" than \"Optimisation\". Then again the distinction is pretty muddy. Maybe merge the sections under \"Scientific Computing\"?' by a COLLABORATOR of type User on 2023-01-21T11:52:58Z\n'Moving part of the conversation with @jakevdp here for visibility:\r\n\r\n- I am disappointed to see that Equinox + Diffrax + jaxtyping all got cut from the ecosystem list, in the new draft. These are useful well-maintained libraries. They have have similar levels of uptake to other libraries included here. (E.g. much of Chex is now superseded by jaxtyping.)\r\n\r\n- Unrelated to the previous point: I think it'd be more useful to have libraries categorised by what they do. (Optimisation, neural networks, ...)' by a COLLABORATOR of type User on 2023-01-24T16:33:22Z\n'Thanks Patrick – I'm going to close this, because I think it will be more useful to iterate on these ideas elsewhere.' by a COLLABORATOR of type User on 2023-01-24T19:12:38Z\n\nPR has review comments:\n'@jakevdp I can add short descriptions to some of these if you think this makes sense. (otherwise, what is Optax vs JAXOpt, for example)' by a CONTRIBUTOR of type User on 2023-01-20T12:57:29Z\n'Nit: A more accessibility-friendly way would be \"Check out...\" or \"Go to...\" instead of \"See...\" IMO.\r\n\r\n' by a CONTRIBUTOR of type User on 2023-01-20T12:58:23Z\n'Done, thanks' by a COLLABORATOR of type User on 2023-01-23T22:31:55Z\n'Done, thanks!' by a COLLABORATOR of type User on 2023-01-23T22:32:05Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 4,
        "reason_for_closure": "Closed as infeasible/incorrect: PR would deprecate useful functionality and break existing code."
    },
    {
        "summary": "Pull Request '14011' titled 'Eagerly register backend factories for PJRT C API plugins.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2023-01-14T02:25:54Z, and was closed at 2023-02-11T18:30:42Z by a User.\nIt has a body of 'The prior mechanism was triggering the xla_client (in jaxlib) to load the plugins based on the environment variable, but there was nothing making them known to the JAX-side xla_bridge. Further, they were all being loaded regardless of what plugin was being requested.\r\n\r\nThis reworks the mechanism to eagerly register backend factories with the JAX-side xla_bridge so that they are availble when requested (i.e. as part of JAX_PLATFORMS env var or equivalent).\r\n\r\nThere is an opaque `TODO(b/237099479): Move to xla_bridge.py when ready.` in xla_client.py that can't be seen from the OSS side, so not sure what the intent here is. As near as I can tell, this never worked and is a temporary measure while we work out a better mechanism.'\nPR has comments:\n'@skye Feel free to do this another way if you'd like. But I've confirmed this works for me.' by a CONTRIBUTOR of type User on 2023-01-14T02:26:10Z\n'@jzhoulon FYI. After this change, there is no need to manually register a plugin backend and you can run your plugin by setting environment variables (no code changes needed). ' by a NONE of type User on 2023-01-18T19:17:01Z\n'> \r\n@jyingl3 Thanks for the notification. I saw the RFC, it mentioned there is a way to load the plugin through [naming convention of the Python module](https://packaging.python.org/en/latest/guides/creating-and-discovering-plugins/#using-naming-convention), in my understanding, it will find the package and may be can load the library called pjrt_xx.so in that module installed path(such as site-package/pjrt_xxx), then the loading mechanism will not require user specifiy the environment, please correct my if I was wrong:) if it is, will jax support this mechanism? I think letting user always specify the library path before running is not convenient enough . Thanks\r\n' by a NONE of type User on 2023-01-30T13:29:48Z\n'Confirmed that the upstream changes supercede this patch.' by a CONTRIBUTOR of type User on 2023-02-11T18:30:42Z\n\nPR has review comments:\n'@skye Do we want to expose xla_client._xla in xla_bridge?' by a NONE of type User on 2023-01-18T18:44:07Z\n'@skye I know this change is a short term work around. Do we want to expose _get_pjrt_plugin_names_and_library_paths (i.e. remove the underscore and add to xla_client.pyi)?' by a NONE of type User on 2023-01-18T18:45:43Z\n'Shall we set a high priority for plugin factories, e.g. priority=400?' by a NONE of type User on 2023-01-18T18:46:26Z\n'nit: maybe rename to _register_plugin_backend_factories?' by a NONE of type User on 2023-01-18T18:50:33Z\n'Ideally no, but it's not the end of the world. The right way to do this is to update [xla_client.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/python/xla_client.py) in the TF repo to expose the methods you need directly (like [this](https://github.com/tensorflow/tensorflow/blob/6d5d88041f49e866b1dd18b6f532107e4b84e650/tensorflow/compiler/xla/python/xla_client.py#L41-L42)). @stellaraccident I'd help you get a TF PR in if you wanna do that, but this is also fine for now and we can fix it later (might be easiest to do one big pass exposing everything we need after this code stops churning).\r\n\r\nI suspect this does need an xla_client version guard though. Basically, this code has to be forward-compatible with older jaxlib versions (and xla_client is shipped with jaxlib).\r\n\r\nExample version guard: https://github.com/google/jax/blob/e3c2602ed5174bcfd79fe3b25cf9009cdcb48bfe/jax/experimental/host_callback.py#L993-L997\r\n\r\nVersion number comes from here: https://github.com/tensorflow/tensorflow/blob/6d5d88041f49e866b1dd18b6f532107e4b84e650/tensorflow/compiler/xla/python/xla_client.py#L46\r\n\r\nYou can drop the guard once the minimum jaxlib version exports a high enough xla_client version:\r\nhttps://github.com/google/jax/blob/e3c2602ed5174bcfd79fe3b25cf9009cdcb48bfe/jax/version.py#L19\r\n\r\nSo for `get_c_api_client`, blame tells me it was introduced in https://github.com/tensorflow/tensorflow/commit/77a1e8e3fc3637f7fa29d9cd8d8d33dcb707b965, and the xla_client version number was [110](https://github.com/tensorflow/tensorflow/blob/77a1e8e3fc3637f7fa29d9cd8d8d33dcb707b965/tensorflow/compiler/xla/python/xla_client.py#L46). Or you can just use the current version number of 118 if you don't wanna check. I'm gonna cut a new jaxlib soon, so this should be publicly available then.' by a MEMBER of type User on 2023-01-18T19:09:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 5,
        "reason_for_closure": "Inconclusive :- PR closed as the functionality is already handled by the dependency."
    },
    {
        "summary": "Pull Request '13873' titled '(add) add SmeLU activation function to nn.functions' was authored by a User, who is associated as a NONE. \nIt was created at 2023-01-05T07:38:41Z, and was closed at 2023-02-20T17:55:26Z N/A.\nIt has a body of 'Hello 👋  This small PR adds Jax implementation of the Smooth ReLU activation function proposed in the paper [\"Real World Large Scale Recommendation Systems Reproducibility and Smooth Activations\"](https://arxiv.org/pdf/2202.06499.pdf) [arXiv 2022] by Gil I. Shamir, Dong Lin.'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/13873/checks?check_run_id=10453490586) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2023-01-05T07:38:45Z\n\nPR has review comments:\n'We cannot do runtime value assertions of traced variables in JAX. Instead, you'll probably want to return `NaN` when `beta` is invalid, using `jnp.where` for JIT compatiblity.\r\n' by a COLLABORATOR of type User on 2023-01-09T21:30:55Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Inconsistent behavior :- PR tried to remove support for complex dtypes, while JAX generally aims to extend NumPy's functionality."
    },
    {
        "summary": "Pull Request '13773' titled 'Added jax.random.vonmises' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-12-22T19:27:05Z, and was closed at 2023-11-06T19:28:56Z by a User.\nIt has a body of 'As referenced in #11737.\r\n\r\nThis is the initial implementation, based on the reference provided in the docstring.\r\n\r\nHalf of the tests are passing at the moment, but having some dtype issues with `lax.select` and `lax.ge`. Also having some trouble ensuring the returned array is of the specified dtype.\r\n\r\nAny guidance would be appreciated!'\nPR has comments:\n'Thanks - I'll take a closer look soon, but in the meantime it looks like there are some type mismatch errors when running in X64 mode; you can run those tests locally with\r\n```\r\n$ JAX_ENABLE_X64=1 pytest -n auto tests/random_test.py\r\n```' by a COLLABORATOR of type User on 2022-12-22T21:06:10Z\n'Been sitting on this for a little bit now, just wondering if you could offer some input!\r\n\r\nI've done most of the re-factoring, but I'm unsure on the best way to work the conditional in with the new broadcasting. Here's the code-sample:\r\n\r\n```\r\ndef vonmises(\r\n  key: KeyArray,\r\n  x: RealArray,\r\n  kappa: RealArray,\r\n  shape: Optional[Shape] = None,\r\n  dtype: DTypeLikeFloat = dtypes.float_,\r\n  max_iters: int = 100\r\n) -> Array:\r\n  \"\"\"Draw samples from a von Mises distribution.\r\n\r\n  Args:\r\n    key: a PRNG key used as the random key.\r\n    x: mean of the distribution. Must be broadcast-compatible with kappa.\r\n    kappa: concentration of the distribution. Must be broadcast-comptible with x.\r\n    shape: optional, shape of the output array.\r\n    dtype: optional, dtype of the output array. (default float64 if\r\n      jax_enable_x64 is true, otherwise float32).\r\n    max_iters: maximum number of times to run the algorithm\r\n      that generates and accepts/rejects samples.\r\n\r\n  Returns:\r\n    theta: A random array of specified shape and dtype.\r\n\r\n  Reference:\r\n    Best, D. J., & Fisher, N. I. (1979). Efficient Simulation of the\r\n    von Mises Distribution. Applied Statistics, 28(2), 152.\r\n    https://doi.org/10.2307/2346732\r\n  \"\"\"\r\n  # Check Key, Dtypes, Shape\r\n  key, _ = _check_prng_key(key)\r\n  if not dtypes.issubdtype(dtype, np.floating):\r\n    raise ValueError(f\"dtype argument to `t` must be a float\"\r\n                     f\"dtype, got {dtype}.\")\r\n  dtype = dtypes.canonicalize_dtype(dtype)\r\n  \r\n  if shape is None:\r\n    shape = lax.broadcast_shapes(np.shape(x), np.shape(kappa))\r\n  else:\r\n    _check_shape(\"vonmises\", shape, x, kappa)\r\n\r\n  def _true(key: Array, x: Array, kappa: Array) -> Array:\r\n    return jnp.pi * 2 * uniform(key, shape, dtype)\r\n\r\n  def _false(key: Array, x: Array, kappa: Array)-> Array:\r\n    return _vonmises(key, x, kappa, shape, dtype, 100)\r\n\r\n  # Return Distribution\r\n  return lax.cond(\r\n    kappa < 1e-5, \r\n    lambda key, x, kappa: _true(key, x, kappa),\r\n    lambda key, x, kappa: _false(key, x, kappa),\r\n    key, x, kappa\r\n  )\r\n```\r\n\r\nUnfortunately, `lax.select` doesn't play nicely with the` kappa < 1e-5` condition:\r\n\r\n```\r\nTypeError: Pred must be a scalar, got [[ True]\r\n [ True]\r\n [ True]\r\n [ True]\r\n [ True]\r\n [ True]\r\n [ True]\r\n [ True]\r\n [ True]\r\n [ True]] of shape (10, 1).\r\n```\r\nThe problem is that, after broadcasting, for rows/columns where the corresponding value of kappa is below the threshold we want to apply the uniform approximation. Does lax.cond allow the activation of both conditions? Any ideas on best ways to overcome this?' by a CONTRIBUTOR of type User on 2023-01-17T12:33:11Z\n'`lax.cond` requires a scalar condition. For a vector condition, you can use `lax.select`' by a COLLABORATOR of type User on 2023-01-17T17:42:36Z\n'Sorry for losing track of this one – are you still interested in contributing this?' by a COLLABORATOR of type User on 2023-11-03T22:29:14Z\n'It looks like this was addressed in #13355' by a COLLABORATOR of type User on 2023-11-06T19:28:56Z\n\nPR has review comments:\n'For the `dtype` issue, add \r\n```suggestion\r\n  x = lax.convert_element_type(x, dtype)\r\n  kappa = lax.convert_element_type(kappa, dtype)\r\n```' by a NONE of type User on 2022-12-22T23:04:53Z\n'Worked like a charm. Think I've found the other issues, shall update.' by a CONTRIBUTOR of type User on 2022-12-23T00:17:39Z\n'stray revert?' by a COLLABORATOR of type User on 2022-12-23T00:20:37Z\n'This requires kappa to be a static parameter. If that's not your intent, you could use `lax.cond` instead.' by a COLLABORATOR of type User on 2022-12-23T00:22:41Z\n'`astype(dtype)` is redundant here and can be removed.' by a COLLABORATOR of type User on 2022-12-23T00:23:26Z\n'`1 + r * z` is fine here' by a COLLABORATOR of type User on 2022-12-23T00:24:20Z\n'For the most part these constants are unnecessary, and you can just use the literal values directly. The one exception is when you write `lax.ge(expression, zero)` because lax functions do not implicitly promote inputs, but you could use `expression >= 0` instead to avoid the need for strict type matching of inputs.' by a COLLABORATOR of type User on 2022-12-23T00:27:51Z\n'Using literal `0`, `1`, and `2` is fine here.' by a COLLABORATOR of type User on 2022-12-23T00:30:23Z\n'Other distributions allow arbitrarily-sized arrays for the distribution parameters: see how `gamma` treats the `a` parameter, for example. We should follow that convention here.' by a COLLABORATOR of type User on 2022-12-23T00:42:41Z\n'Typically the reference section would go after `Args` and `Returns`' by a COLLABORATOR of type User on 2022-12-27T17:50:33Z\n'Use the `_check_shape` utility to ensure that the specified shape is compatible with the shape of `x`' by a COLLABORATOR of type User on 2022-12-27T17:51:59Z\n'Since `kappa` is not a static parameter, perhaps we should support broadcasted inputs for it as well? What do you think?' by a COLLABORATOR of type User on 2022-12-27T17:53:26Z\n'Minor suggestion: if you make the `_true` and `_false` function accept arrays (`key`, `x`, `kappa`) as arguments and pass these values to `lax.cond`, then the compiler can be smarter about the lowring of the conditional.' by a COLLABORATOR of type User on 2022-12-27T17:55:47Z\n'If this `astype` is necessary, I'd probably consider it a bug: the computation should be performed in the specified dtype. For example, this would be important for code running on accelerators where the user may want to ensure that computations are done in `float32`, which is more performant than `float64`.\r\n\r\nPerhaps you can ensure the correct dtype at the start of the computation?' by a COLLABORATOR of type User on 2022-12-27T18:02:12Z\n'I'd add a test similar to `testGammaShape` to ensure that nontrivial combinations of `x` and `shape` produce the expected output' by a COLLABORATOR of type User on 2022-12-27T18:09:43Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 15,
        "reason_for_closure": "This PR was closed as it would deprecate existing functionality without sufficient benefit.  The proposed change was deemed inconsistent with JAX's goal of superseding NumPy functionality."
    },
    {
        "summary": "Pull Request '13494' titled 'lax.fori_loop: allow scalar limits' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-12-02T20:02:11Z, and was closed at 2023-04-12T15:08:08Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Previously, writing something like this would lead to a type  mismatch error:\r\n```python \r\nstop = np.int16(10)\r\nlax.fori_loop(0, stop, body, init)\r\n```\r\nThis relaxes the type match requirement when one of the limits is expressed as a Python scalar (i.e. a weakly-typed value when traced).'\nPR has comments:\n'Closing since https://github.com/google/jax/pull/15551 was merged.' by a COLLABORATOR of type User on 2023-04-12T15:08:08Z\n\nPR has review comments:\n'Wouldn't this be problematic in the case that both are weakly typed and upper is the one with lower precision?' by a COLLABORATOR of type User on 2022-12-07T18:01:31Z\n'Yes, but that's part of the compromise we've made in allowing python scalar integers to implicitly promote to any strong integer type.' by a COLLABORATOR of type User on 2022-12-07T18:03:42Z\n'Oh, nvm, I see what you're saying...' by a COLLABORATOR of type User on 2022-12-07T18:11:22Z\n'Man this is tough to do cleanly in a way that preserves staticness of inputs (required for choosing `scan` vs. `while` below). i.e. we can't use `lax.convert_element_type` because of omnistaging' by a COLLABORATOR of type User on 2022-12-07T19:03:30Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Superseded :- PR intended to match numpy functionality, but JAX chose to retain its existing behavior."
    },
    {
        "summary": "Pull Request '13096' titled 'jnp.split: use dynamic rather than static slices for speed' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-11-03T19:51:36Z, and was closed at 2023-11-03T20:27:24Z by a User.\nIt has a body of 'Fixes #12999. See also #9445\r\n\r\nThis is very similar to the issue & fix in #12219\r\n\r\nmain branch:\r\n```python\r\nIn [1]: import jax.numpy as jnp \r\n   ...: x = jnp.ones(5000) \r\n   ...: %time _ = jnp.split(x, jnp.arange(0, 5000, 10))                                                                   \r\nCPU times: user 4.77 s, sys: 118 ms, total: 4.89 s\r\nWall time: 4.96 s\r\n```\r\nthis branch:\r\n```python\r\nIn [1]: import jax.numpy as jnp \r\n   ...: x = jnp.ones(5000) \r\n   ...: %time _ = jnp.split(x, jnp.arange(0, 5000, 10))                                                                   \r\nCPU times: user 342 ms, sys: 8.52 ms, total: 351 ms\r\nWall time: 360 ms\r\n```'\nPR has comments:\n'Good question - for repeated operations after the initial run, we get something like this:\r\n\r\nmain branch:\r\n```python\r\nIn [2]: %timeit jax.block_until_ready(jnp.split(x, jnp.arange(0, 5000, 10)))                                          \r\n40.5 ms ± 4.22 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n```\r\n\r\nthis branch:\r\n```python\r\nIn [2]: %timeit jax.block_until_ready(jnp.split(x, jnp.arange(0, 5000, 10)))                                          \r\n116 ms ± 4.01 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\r\n```\r\nSo a slowdown of ~3x on repeated calls is the tradeoff for saving ~150x on the initial call.\r\n\r\nWhat do you think?' by a COLLABORATOR of type User on 2022-11-04T16:48:09Z\n'From conversation with @hawkinsp: we may want to profile in order to understand why the dynamic slice approach is 3x slower, since it isn't clear that it ought to be (?)' by a MEMBER of type User on 2022-11-11T18:29:23Z\n'Why is `dynamic_slice` faster than static `slice` in the first place?' by a NONE of type User on 2022-12-08T08:19:23Z\n'> Why is `dynamic_slice` faster than static `slice` in the first place?\r\n\r\n`dynamic_slice` is slower than static `slice` if you call it once, because it does not specialize its code on index values, and so at runtime XLA has to perform some logic regarding the value of the start index. But static `slice` is specialized on static start indices, so each call with different start indices incurs a small overhead in XLA at compile time, and when you accumulate this small overhead thousands of times, it is slower than `dynamic_slice`, which does not have such overhead because it is not specialized on the index values.' by a COLLABORATOR of type User on 2022-12-08T17:15:54Z\n'Thanks! That explains the performance issues I've been trying to debug with slice vs dynamic_slice vs numpy slice.' by a NONE of type User on 2022-12-08T18:48:14Z\n'I can no longer reproduce these performance issues, despite this still lowering to static slice.' by a COLLABORATOR of type User on 2023-11-03T20:27:24Z\n\nPR has review comments:\n'Another option here: we could replace this with\r\n```python\r\nreturn [ary[int(start): int(end)] for start, end in zip(split_indices[:-1], split_indices[1:])]\r\n```\r\nDue to #12219, this would now use dynamic rather than static slices and result in the same performance characteristics. I kind of like the idea of delegating the performance question to existing code. What do you think?' by a COLLABORATOR of type User on 2022-11-04T16:56:59Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 1,
        "reason_for_closure": "Inconclusive: Performance issues were no longer reproducible."
    },
    {
        "summary": "Pull Request '12955' titled 'Merge CUDA and ROCM kernel code in jaxlib.' was authored by a Bot, who is associated as a NONE. \nIt was created at 2022-10-24T18:58:11Z, and was closed at 2022-10-25T14:23:39Z by a Bot.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Merge CUDA and ROCM kernel code in jaxlib.\n\nThe code for both CUDA and ROCM is almost identical, so with a small shim library to handle the differences we can share almost everything.\n'\nPR has comments:\n'@rsanthanam-amd FYI' by a COLLABORATOR of type User on 2022-10-25T01:20:40Z\n\nPR has review comments:\n'Add m.attr(\"hipsparse_supported\") = py::bool_(JAX_GPU_HAVE_SPARSE); ?' by a CONTRIBUTOR of type User on 2022-10-25T13:16:23Z\n'I changed this to `sparse_supported` (removed the prefix) and updated the Python code to match.' by a COLLABORATOR of type User on 2022-10-25T13:54:57Z\n'Ah yeah - that's better.  Just retested with this change and it worked.' by a CONTRIBUTOR of type User on 2022-10-25T14:07:02Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 3,
        "reason_for_closure": "Merged :- CUDA and ROCM kernel code merged in jaxlib."
    },
    {
        "summary": "Pull Request '12943' titled 'Improve Gamma sampling' was authored by a User, who is associated as a NONE. \nIt was created at 2022-10-24T02:48:35Z, and was closed at 2023-11-03T22:23:15Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This fixes:\r\n\r\n- a todo to use lax cond in order to speed up a computation during gamma sampling\r\n'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/12943/checks?check_run_id=9060288820) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2022-10-24T02:48:39Z\n'cc @fehiepsi  -- looks like it was added when the grad was still implemented in this file.\r\n\r\nI can try a quick benchmark for now?' by a NONE of type User on 2022-10-24T16:13:41Z\n'@jakevdp actually it does indeed faster to map on cpu, I had a bug in my benchmark. Also would probably good to leave these potential changes in separate PRs. Therefore I changed this PR to only include the lax cond change' by a NONE of type User on 2022-10-24T16:33:28Z\n'It seems like this has become stale. I'm going to close for now – feel free to re-open if you'd like to continue working on this!' by a COLLABORATOR of type User on 2023-11-03T22:23:15Z\n\nPR has review comments:\n'Unused?' by a COLLABORATOR of type User on 2022-10-24T15:12:43Z\n'What's the reason for turning a constant into an expression? Mabye `lambda: np.bool_(False)` would be sufficient?' by a COLLABORATOR of type User on 2022-10-24T15:13:05Z\n'No good reason. Fixed :)' by a NONE of type User on 2022-10-24T15:37:43Z\n'Done.' by a NONE of type User on 2022-10-24T15:37:50Z\n'Does this change make things faster? Could you post some benchmark numbers?' by a CONTRIBUTOR of type User on 2022-10-24T20:43:24Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 5,
        "reason_for_closure": "Stale :- PR closed due to inactivity and lack of author follow-up."
    },
    {
        "summary": "Pull Request '12632' titled '[FFI]: Add JEP for FFI' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-10-03T18:50:24Z, and was closed at 2024-08-02T10:44:43Z by a User.\nIt has a body of 'Proposes a new API for calling out to foreign functions that wraps XLA's `CustomCall` API and JAX's primitive API.\r\nYou can check out a prototype implementation of this JEP in #12396.\r\n\r\nYou can read the HTML version of this proposal here: https://jax--12632.org.readthedocs.build/en/12632/jep/12535-ffi.html.\r\n\r\ncc: @dfm \r\n\r\nTracker: #12535 '\nPR has comments:\n'@sharadmv — I haven't had a chance to really dig into this in detail, but I wanted to say that from my perspective this would be a huge quality of life improvement!' by a COLLABORATOR of type User on 2022-10-04T12:06:58Z\n'Thank you for your feedback. Your guide has been and will continue to be a huge benefit to the community!' by a COLLABORATOR of type User on 2022-10-04T17:22:23Z\n'@dfm we've been referring people to your tutorial for years (thank you!), and as good as it is, one founding objective of this JEP was to make such tutorials unnecessary (i.e. to make your life much easier). 😁 ' by a COLLABORATOR of type User on 2022-10-05T19:11:15Z\n'If now (late 2023) I wanted to start integrating a custom op (such as those from cuSolver/Magma etc) into Jax and define autodiffs, should I wait for this PR to land?' by a NONE of type User on 2023-10-17T23:01:04Z\n'The most up to date doc for JAX custom operation on GPU is:\r\nhttps://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html' by a COLLABORATOR of type User on 2023-10-18T17:06:57Z\n'Now that https://github.com/google/jax/pull/21925 is merged I think we can close this: we have an FFI now!\r\n\r\nhttps://jax.readthedocs.io/en/latest/ffi.html' by a COLLABORATOR of type User on 2024-08-02T10:44:43Z\n\nPR has review comments:\n'Add `python`' by a COLLABORATOR of type User on 2022-10-04T12:01:45Z\n'It seems like the \"error handling JEP\" doesn't exist yet...' by a COLLABORATOR of type User on 2022-10-04T12:04:48Z\n'Yep, it's a WIP and I made this a TODO.' by a COLLABORATOR of type User on 2022-10-04T18:45:28Z\n'Where `add_one` come from?' by a COLLABORATOR of type User on 2022-10-05T19:51:30Z\n'What about multi-GPU and sharding?\r\nI think this case should be supported as we know this is a current issue.' by a COLLABORATOR of type User on 2022-10-06T14:10:30Z\n'Why you do not try to cover this?\r\nOr this is planned for a next version?' by a COLLABORATOR of type User on 2022-10-06T14:10:45Z\n'Our thinking is:\r\n* We can't support this easily without either implementing our own wrapper around XLA's layout API or exposing details of XLA itself\r\n* Users can work around by either changing layouts manually or forgoing the FFI API and using the Custom Call API directly\r\n\r\nIn the long run, we might want to support custom layouts and that can be in a follow up version.' by a COLLABORATOR of type User on 2022-10-06T21:10:18Z\n'This is an important use case but can arguably be solved orthogonally via the WIP custom partitioning API. We're still working on its design but presumably we can surface a `jax.custom_sharding` or something like it that will work with FFI calls.' by a COLLABORATOR of type User on 2022-10-06T21:11:35Z\n'Thanks for the catch, this is a typo.' by a COLLABORATOR of type User on 2022-10-06T21:12:53Z\n'On the other hand, we could just provide an API to specify custom major-to-minor orders in Python (like [0, 1] or [1, 0]) and that would probably be sufficient.' by a COLLABORATOR of type User on 2022-10-06T21:29:18Z\n'I think so. Asking to not use the FFI just for that looks a very high cost. So allowing this would be great.' by a COLLABORATOR of type User on 2022-10-06T23:31:12Z\n'That would be great.' by a COLLABORATOR of type User on 2022-10-06T23:32:51Z\n'Peter pointed out that jaxlib uses custom layouts frequently so we should support this. Thankfully it only needs to exist in Python and doesn't require changing the C API.' by a COLLABORATOR of type User on 2022-10-06T23:33:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 13,
        "reason_for_closure": "Superseded :- An alternative solution was implemented in another PR/commit."
    },
    {
        "summary": "Pull Request '12455' titled 'add unused-argument-pruning and dce to pjit' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-09-22T00:43:33Z, and was closed at 2022-09-22T03:49:46Z by a User.\nPR has comments:\n'This is already handled in lower_sharding_computation. Why do we need to handle it in pjit's dispatch path?' by a COLLABORATOR of type User on 2022-09-22T00:45:33Z\n'https://github.com/google/jax/blob/main/jax/interpreters/pxla.py#L2725-L2731 is the place where we DCE. What's the advantage of doing it in dispatch?' by a COLLABORATOR of type User on 2022-09-22T00:48:40Z\n'AFAIK, this is how DCE happens in jit too. `jit` never tries to DCE right at the start (feel free to correct me if that's not the case).\r\n\r\nkeep_unused is always set to True for pjit though: https://github.com/google/jax/blob/main/jax/experimental/pjit.py#L964 due to some failures that we encountered in deepmind and our own pjit tests IIRC.' by a COLLABORATOR of type User on 2022-09-22T00:50:51Z\n'Please note this is a draft PR, not marked ready for review! But I probably shouldn't have pre-added you as a reviewer as that was confusing.\r\n\r\n> This is already handled in lower_sharding_computation. Why do we need to handle it in pjit's dispatch path?\r\n\r\nActually, we were operating under some faulty hypotheses about observed behavior, thinking that the DCE wasn't happening.\r\n\r\nAn advantage of this approach over doing it in `lower_sharding_computation` is that we do it early, so any passes on the jaxpr don't need to deal with dead code.\r\n\r\nThat said, I don't think that advantage alone is worth the upside.\r\n\r\n> AFAIK, this is how DCE happens in jit too. jit never tries to DCE right at the start (feel free to correct me if that's not the case).\r\n\r\nWell, `pjit` forms a jaxpr up front before any transformations (it's an \"initial-style higher-order primitive\"), whereas `jit` only forms a jaxpr after transformations. So if we wanted to do DCE as soon as possible, i.e. as soon as a jaxpr is available, we would do it in `pjit` as in this PR, whereas with `jit` we could only do it in the impl rule.' by a COLLABORATOR of type User on 2022-09-22T03:49:13Z\n\nPR has review comments:\n'local_in_avals is returned here. You need to prune that too.' by a COLLABORATOR of type User on 2022-09-22T03:30:15Z\n'Also in_tree is also returned. Is the pruning required for in_tree too?' by a COLLABORATOR of type User on 2022-09-22T03:30:42Z\n'Yes and no... we only prune after flattening, but things like `PjitCompiled.input_shardings` seem to assume that the executable arguments are not pruned relative to the in_tree, so how does that work with the existing pruning (not the pruning in this PR)?' by a COLLABORATOR of type User on 2022-09-22T03:48:02Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 3,
        "reason_for_closure": "Incorrect behavior change: PR would break existing users with little benefit."
    },
    {
        "summary": "Pull Request '12396' titled 'Jax custom call' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-09-16T21:37:32Z, and was closed at 2024-07-24T12:47:42Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nPR has comments:\n'We are importing this to explore some things. @hawkinsp hold off on reviews. Thanks!' by a COLLABORATOR of type User on 2022-10-26T23:50:42Z\n'Hi! This looks really useful. Is anyone still committed to making this happen?' by a NONE of type User on 2023-01-23T22:52:09Z\n'@jlu-spins yes, we are making an improved version with better C++ typed argument support. But it creates a new dependency on some XLA component that's still WIP. We are actively working on this, and will hopefully have a version in a few weeks.\r\n\r\nIn the meanwhile, https://github.com/dfm/extending-jax is still a good guide for doing JAX custom call at head.' by a COLLABORATOR of type User on 2023-01-23T22:55:59Z\n'@zhangqiaorjc Thank you!' by a NONE of type User on 2023-01-23T23:01:34Z\n'How create a custom class instance with several call method and its own stateful resource. Something like [Extending TorchScript with Custom C++ Classes](https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html#using-the-c-class-from-python-and-torchscript)' by a NONE of type User on 2024-07-23T12:49:07Z\n'I'm going to close this PR since this work has been superseded by XLA's FFI API which is documented in https://github.com/google/jax/pull/22095.\r\n\r\n@MoFHeka — Please feel free to open a new discussion topic with more details about your specific use case if it would be useful to continue the conversation.' by a COLLABORATOR of type User on 2024-07-24T12:47:39Z\n\nPR has review comments:\n'BTW, linkage may be hard here. My suggestion to Sharad was that we pass function pointers for all API entry points a custom call might need (i.e., like a c++ class implemented in C).' by a COLLABORATOR of type User on 2022-09-16T21:54:48Z\n'Updated to use runtime linkage instead of link-time linkage a la Numpy import_array() style.' by a COLLABORATOR of type User on 2022-09-21T03:47:27Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 2,
        "reason_for_closure": "Superseded by XLA's FFI API."
    },
    {
        "summary": "Pull Request '12131' titled '[ROCm] Disable CUDA specific test for ROCm.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-08-27T12:50:16Z, and was closed at 2022-09-28T13:43:16Z by a User.\nIt has a body of '/cc @hawkinsp '\nPR has comments:\n'@hawkinsp gentle ping' by a CONTRIBUTOR of type User on 2022-09-08T12:38:39Z\n'Sorry this dropped off my list of things to do. Apologies!\r\n\r\nI had missed that multiprocess support isn't hooked into ROCM/RCCL (which is what `nccl_unique_id_callback` does). I'd guess that's a fairly mechanical thing to fix but since that's the state of things I agree with your original fix: disable the test.\r\n\r\nhttps://github.com/google/jax/pull/12556 fixes.' by a COLLABORATOR of type User on 2022-09-28T13:43:16Z\n\nPR has review comments:\n'FWIW, the test can probably work if you adapt it to set `jax_rocm_visible_devices` as well.' by a COLLABORATOR of type User on 2022-08-27T13:30:50Z\n'The recent modifications to that test did not work on ROCm.  I thought it might be cleaner to add a separate ROCm version which did things the previous way.\r\n\r\nPTAL.' by a CONTRIBUTOR of type User on 2022-08-27T18:28:56Z\n'I'd prefer we consolidate this with the cuda version. The only difference between these two is the name of the flag, right? Make the test parametric instead?' by a COLLABORATOR of type User on 2022-09-08T13:24:33Z\n'Actually, there are other differences beyond just the flag name.\r\n\r\nSome recent changes were made to that particular subtest that did not work on ROCm.\r\n\r\nSo the ROCm specific version essentially reverts the latest changes in addition to changing the name of the flag.' by a CONTRIBUTOR of type User on 2022-09-08T13:27:36Z\n'I modified the existing test to support ROCm.\r\n\r\nModifications beyond just changing `jax_cuda_visible_devices` to `jax_rocm_visible_devices` were necessary because ROCm does not support the statement `s = jax.pmap(lambda x: jax.lax.psum(x, \"i\"), axis_name=\"i\")(jax.numpy.ones(jax.local_device_count()));`\r\n\r\nThe reason is that a valid nccl_unique_id_callback must be supplied to `GetNcclUniqueIdCallback` from `tensorflow/compiler/xla/service/gpu/nccl_utils.cc` and this is not supported on ROCm:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/pjrt/gpu_device.cc#L386-L393\r\n\r\n' by a CONTRIBUTOR of type User on 2022-09-25T15:42:59Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 5,
        "reason_for_closure": "Inconsistent behavior: PR attempts to match NumPy but breaks existing JAX functionality."
    },
    {
        "summary": "Pull Request '12018' titled 'experiment: add `jax.typing` module and use it in `jax.lax`' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-08-19T19:25:21Z, and was closed at 2022-10-14T19:11:16Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Part of #12049\r\n\r\nChanges:\r\n\r\n- created `jax.typing` module, following roadmap in #11859\r\n- moved `core.Shape` definition into `jax.typing`\r\n- moved `jax.numpy.ndarray` definition into `jax.typing`\r\n- applied these new types throughout `jax.lax`, following the style recommended in #11859\r\n- fixed annotations in other files that were broken by the new `jax.lax` types (what a tangled web we weave...)\r\n\r\nSome observations:\r\n- When implementing annotations in `jax.lax`, I used `ArrayLike` for inputs and `NDArray` for outputs. Originally I had `NDArray` aliased to the `ndarray` metaclass, but because `jax.lax` has several functions that explicitly return `Tracer` types, it seemed necessary to make `NDArray = Union[ndarray, Tracer]`.\r\n- This change had wide-ranging ripple effects: any function annotated with `-> jnp.ndarray` and returning the result of a lax operation caused a typecheck failure (because `Tracer` is inompatible), so it meant that in many places I had to change `-> jnp.ndarray` to `-> NDarray`, which caused even more ripples.\r\n- An alternative would be to use `-> jnp.ndarray` by convention (with the understanding that it implies compatiblity with tracers) and use `#type: ignore` statements where tracers are explicitly returned.\r\n\r\nHappy to hear any thoughts/opinions on this.'\nPR has comments:\n'I think folks are definitely going to get confused between `google/jax.typing` and `google/jaxtyping`. Plus I'm very wary of yet more sharding within the JAX ecosystem.\r\n\r\nObviously the former has more restricted aims, but wants to stay within the static type system.\r\n\r\nCan we see if there's a reasonable way to bring both projects together? For example switched on the `typing.TYPE_CHECKING` flag.\r\n\r\n(EDIT: jakevdp and I are discussing this offline, and think we have most-of-a-plan.)' by a COLLABORATOR of type User on 2022-08-23T22:59:12Z\n'> Can we see if there's a reasonable way to bring both projects together? For example switched on the `typing.TYPE_CHECKING` flag.\r\n\r\nI'm not entirely sure what you have in mind... Do you mean that `jax` should depend on `jaxtyping` for static type checks? If so I don't think this is a good idea (I discussed this briefly in the roadmap draft). In my view, what `jaxtyping` includes is far more complicated than the level of type annotations we want in the core library, at least for now.' by a COLLABORATOR of type User on 2022-08-23T23:38:07Z\n'For context, this draft PR is about answering the question \"can we use simple annotations that are slightly better than `Array = Any` without breaking mypy and pytype\". My understanding of `jaxtyping` is that it's far beyond the scope of that question.' by a COLLABORATOR of type User on 2022-08-23T23:42:12Z\n'(closing in favor of #12300)' by a COLLABORATOR of type User on 2022-10-14T19:12:07Z\n\nPR has review comments:\n'`SupportsArray` might be better than `HasArrayMethod` -- it aligns with stdlib https://docs.python.org/3/library/typing.html#protocols and numpy https://github.com/numpy/numpy/blob/6094eff95c19c433648593f3cb6bca489396f9ae/numpy/_typing/_array_like.py#L36-L38.\r\nBTW, we can add a comment about why not directly use `Union[Array, numpy.typing.ArrayLike]` -- JAX ArrayLike does NOT allow (nested) sequences, `str` or `bytes`.' by a CONTRIBUTOR of type User on 2022-10-14T17:12:21Z\n'Must this protocol supports writeable `dtype` variable?\r\n`numpy.typing` counterpart has read only `dtype` variable(property) https://github.com/numpy/numpy/blob/6094eff95c19c433648593f3cb6bca489396f9ae/numpy/_typing/_dtype_like.py#L83-L87. \r\n\r\n[PEP 544: By default, protocol variables as defined above are considered readable and writable. To define a read-only protocol variable, one can use an (abstract) property](https://peps.python.org/pep-0544/#protocol-members)' by a CONTRIBUTOR of type User on 2022-10-14T17:19:15Z\n'Also, name it as `SupportsDType`?' by a CONTRIBUTOR of type User on 2022-10-14T17:20:26Z\n'Hmmm, I guess we can just use `numpy.typing.DTypeLike` here since we use `np.dtype(x)` for every `DTypeLike` `x`.' by a CONTRIBUTOR of type User on 2022-10-14T17:23:25Z\n'`numpy.typing.DTypeLike` is too permissive (e.g. it allows structured types) see https://jax.readthedocs.io/en/latest/jep/12049-type-annotations.html#inputs-should-be-permissively-typed' by a COLLABORATOR of type User on 2022-10-14T17:38:11Z\n'None of this is public API yet aside from `jax.Array`, so there's no issue with adjusting names if we wish.\r\n\r\nWe don't use `numpy.typing.ArrayLike` here because it's too permissive (e.g. it allows lists).' by a COLLABORATOR of type User on 2022-10-14T17:40:28Z\n'I got the point! But it seems that JAX use `np.dtype(xxx)` everywhere?' by a CONTRIBUTOR of type User on 2022-10-14T17:58:16Z\n'Yes, JAX does not allow nested sequence. I just suggest that we can add a comment in code about this reason(and/or a link to docs).' by a CONTRIBUTOR of type User on 2022-10-14T17:59:54Z\n'Yes it does. Invalid inputs would generate runtime errors later in the process.' by a COLLABORATOR of type User on 2022-10-14T18:34:47Z\n'Thanks for the suggestions - changes in #12813' by a COLLABORATOR of type User on 2022-10-14T18:42:31Z\n'Thanks for the suggestions - changes in https://github.com/google/jax/pull/12813' by a COLLABORATOR of type User on 2022-10-14T18:42:37Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 11,
        "reason_for_closure": "Closed as erroneous/invalid/not useful: the PR was deemed unnecessary as JAX supports a superset of NumPy functionality and the change would break existing users."
    },
    {
        "summary": "Pull Request '11879' titled 'JAX implementation of scipy.stats.multinomial pmf & logpmf.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-08-12T15:33:22Z, and was closed at 2022-12-14T19:25:29Z by a User.\nIt has a body of 'Implementation of scipy.stats.multinomial, as referenced by issue [#11817](https://github.com/google/jax/issues/11817).'\nPR has comments:\n'Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/google/jax/pull/11879/checks?check_run_id=7809204612) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request.' by a NONE of type Bot on 2022-08-12T15:33:26Z\n'Added some changes, let me know if anything needs altering! :) @jakevdp ' by a CONTRIBUTOR of type User on 2022-08-19T13:19:32Z\n'Hello @harryjulian, \r\nmay you please review the PR https://github.com/harryjulian/jax/pull/3 I've made on your branch? \r\nIt will hopefully address the awesome reviews from @jakevdp . \r\nI'm new to Github, so please don't hesitate to point out best practices I might be missing. \r\n\r\nThanks and have a great day! 😄 ' by a CONTRIBUTOR of type User on 2022-08-23T11:06:45Z\n'Hey @wonhyeongseo you need to sign the CLA :)' by a CONTRIBUTOR of type User on 2022-08-25T10:06:55Z\n\nPR has review comments:\n'These kinds of checks won't be possible for jax implementations, because the arguments may be traced. The best solution is probably to remove `_check_args` entirely.' by a COLLABORATOR of type User on 2022-08-12T18:42:50Z\n'Please use two-space indentation throughout.' by a COLLABORATOR of type User on 2022-08-12T18:43:01Z\n'Since `n` is an integer, should we avoid promoting it to float? ' by a COLLABORATOR of type User on 2022-08-12T18:44:04Z\n'This makes sense. Have you any intuition on the best way in which I could still implement the checks, or should I just remove the entirely? Perhaps returning `jnp.inf` if the checks aren't satisfied. My initial thought would be to use `jnp.where` but the docs indicate that it can't currently be JIT compiled.' by a CONTRIBUTOR of type User on 2022-08-15T14:15:26Z\n'Returning `NaN` for invalid values is probably the best bet. The usual way to do that is using the three-argument form of `jnp.where`, which is compatible with transformations like JIT' by a COLLABORATOR of type User on 2022-08-15T15:43:51Z\n'We need a license statement at the top of each file (you can copy from other files in this directory, and change the year to 2022)' by a COLLABORATOR of type User on 2022-08-19T15:56:02Z\n'This is a bit strange, because `n` and `x` don't have the same dtype. Usually this kind of thing (with `lax_const` is done when using `lax.add`, which doesn't do implicit type promotion.\r\n\r\nI'd suggest deleting this and just writing `n + 1` and `x + 1` below.' by a COLLABORATOR of type User on 2022-08-19T15:57:40Z\n'License here as well' by a COLLABORATOR of type User on 2022-08-19T15:58:50Z\n'If inputs are expected to be integers, would it make more sense to generate integers directly? i.e. why use floating point types here?' by a COLLABORATOR of type User on 2022-08-19T16:09:29Z\n'Can you comment on why you add `np.min(p)` to the normalization? What's the problem with normalizing `p` directly?' by a COLLABORATOR of type User on 2022-08-19T16:11:11Z\n'`np.rint` method does the same rounding as `np.around` or `np.fix`. We do not change to an integer because of the granularity of delta. Thank you!' by a CONTRIBUTOR of type User on 2022-08-25T09:02:48Z\n'I'm somewhat concerned with this implementation allowing floating point inputs for `x`. The function is not defined for non-integer `x` values. In numpy, this is enforced by a runtime check that effectively checks `all(x.astype(int) == x)`, but of course we can't do this in JAX because at runtime we may be on GPU or TPU where this kind of error propagation is not supported.\r\n\r\nThe path chosen here is to silently proceed with the computation even if `x` is invalid. But we have another option: if x has integer dtype, then we *know* that it's valid. So a different option would be to require an integer dtype for `x`. What do you think?' by a COLLABORATOR of type User on 2022-08-25T19:03:10Z\n'Hello sir, I am a novice to programming and I tried to cast x to an integer in this PR https://github.com/harryjulian/jax/pull/4 for @harryjulian 's branch. May you please review it? Thank you sir!' by a CONTRIBUTOR of type User on 2022-08-29T06:09:26Z\n'If you're going to be making significant changes here and @harryjulian is no longer working on this branch, it would probably be cleaner to open a new pull request to avoid all this kind of indirection during review.' by a COLLABORATOR of type User on 2022-08-29T12:52:51Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 14,
        "reason_for_closure": "Inconsistent with numpy: PR deprecates supported functionality, breaking existing code.  Better to maintain superset of numpy features."
    },
    {
        "summary": "Pull Request '11759' titled 'JEP: Propose a new jaxlib Python package structure.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-08-05T15:06:19Z, and was closed at 2023-12-14T22:43:06Z by a User.\nThe PR has labels: JEP - JAX enhancement proposal. \nPR has comments:\n'@ngam @ericmjl @xhochy perhaps this is of interest to you also.' by a COLLABORATOR of type User on 2022-08-12T19:57:23Z\n'> Overall, I think this is a good proposal. I highlighted how we do portions of the proposed ideas in conda-forge already for reference.\r\n> \r\n> Additional notes for your consideration:\r\n> \r\n> * Performance: I have personally found that the TensorFlow PyPI packages often perform quite poorly compared to either optimized NGC containers form NVIDIA or recently the build from conda-forge. One of my little projects of late was to add small changes to the conda-forge TensorFlow that resulted in outperforming the highly optimized NGC containers.\r\n\r\nI expect that we will see less significant differences for JAX, because it depends heavily on JIT compilation: i.e., it matters less exactly how `jaxlib` was built.\r\n\r\n>   \r\n>   * Relevant point: as JAX is maturing, it is important to think of the downsides of simply putting things away on PyPI --- does it make sense for your team to take a more active role in directing the conda-forge builds? (You are more than welcome!)\r\n\r\nI think we would be willing in principle to be more involved in `conda-forge` builds, however the key thing that is stopping us being involved with `conda-forge` is the non-commercial nature of Anaconda's repositories. It might be possible to solve that.  *edit*: I didn't realize `miniforge` is a thing. That might change everything.\r\n\r\n> Does it make sense to convince NVIDIA to publish an optimized container with JAX (like they do with TensorFlow and PyTorch)?\r\n\r\nIt's possible!\r\n\r\n> * Ease for \"users\": I understand the efforts to limit the bandwidth and storage and all that, but it is arguably easier for users to just download `pip install jax` and get it all at once, and not have random pieces be pulled in.\r\n\r\nThe intended end state is:\r\n```\r\npip install jax   # Downloads JAX for CPU, including all necessary dependencies (i.e., `jax`, `jaxlib`, `jaxlib-cpu`)\r\npip install jax[cuda]  # Downloads JAX for CUDA, including all necessary dependencies (`jax`, `jaxlib`, `jaxlib-cpu`, `jaxlib-cuda`)\r\n... etc ...\r\n```\r\nwhich I think is reasonably user friendly?\r\n' by a COLLABORATOR of type User on 2022-08-13T19:24:36Z\n'> which I think is reasonably user friendly?\r\n\r\nYes. I failed to make my point clear --- my point was about potential failures with any of the dependencies (e.g. due to pip's lack of enforcements of versions, etc.) and an alternative is to bundle all these pieces into one big chunk. I am not advocating for this alternative, but I am putting out there --- in conda-forge, it is easy to deal with this type of problem because the solver was created for this type of problem. \r\n\r\nSee an example of `ERROR: pip's dependency resolver`:\r\n\r\n```\r\n(test37) studio-lab-user@default:~$ python3 -m pip install jaxlib==0.3.15 jax==0.2\r\nCollecting jaxlib==0.3.15\r\n  Downloading jaxlib-0.3.15-cp37-none-manylinux2014_x86_64.whl (72.0 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 MB 31.2 MB/s eta 0:00:00\r\nCollecting jax==0.2\r\n  Using cached jax-0.2.0.tar.gz (454 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting numpy>=1.19\r\n  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 83.5 MB/s eta 0:00:00\r\nCollecting scipy>=1.5\r\n  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.1/38.1 MB 47.8 MB/s eta 0:00:00\r\nCollecting absl-py\r\n  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\r\nCollecting opt_einsum\r\n  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\nBuilding wheels for collected packages: jax\r\n  Building wheel for jax (setup.py) ... done\r\n  Created wheel for jax: filename=jax-0.2.0-py3-none-any.whl size=522260 sha256=d550af1b87c6ea1bdd35cee9ab5eb6c7a8f007974473f334e6fda3f0c96835a7\r\n  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/13/c7/c3/6f2fcee22080fa37aadd1e7991fccc1adc62722229e10a3663\r\nSuccessfully built jax\r\nInstalling collected packages: numpy, absl-py, scipy, opt_einsum, jaxlib, jax\r\n\r\nSuccessfully installed absl-py-1.2.0 jax-0.2.0 jaxlib-0.3.15 numpy-1.21.6 opt_einsum-3.3.0 scipy-1.7.3\r\n(test37) studio-lab-user@default:~$ \r\n(test37) studio-lab-user@default:~$ python3 -m pip install scipy==1.4\r\nCollecting scipy==1.4\r\n  Downloading scipy-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\r\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.1/26.1 MB 62.0 MB/s eta 0:00:00\r\nRequirement already satisfied: numpy>=1.13.3 in ./.conda/envs/test37/lib/python3.7/site-packages (from scipy==1.4) (1.21.6)\r\nInstalling collected packages: scipy\r\n  Attempting uninstall: scipy\r\n    Found existing installation: scipy 1.7.3\r\n    Uninstalling scipy-1.7.3:\r\n      Successfully uninstalled scipy-1.7.3\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\njaxlib 0.3.15 requires scipy>=1.5, but you have scipy 1.4.0 which is incompatible.\r\nSuccessfully installed scipy-1.4.0\r\n(test37) studio-lab-user@default:~$ \r\n```\r\n\r\nSo in the future, `pip install jax==0.5.0` can pull in the right `jaxlib`, etc., but a user may end up `pip install jaxlib==0.3.0` for some reason and it will pass with that mild, nonfatal error printed above.\r\n\r\nIn summary, the more pieces that have to coexist within strict parameters, the more issues can potentially arise due to the lack of strictness in the pip resolution of packages. That's a downside of slicing packages and I assume that's why big packages tend to bundle all sorts of random things (e.g. have you seen what pytorch bundles? did you know scipy bundles its own version of openblas in the pip package? etc.) so that the failure points are minimized. I tend to argue against this type of thing because it is a pain in conda-forge, but I do realize the benefits of it and I am making an effort to be unbiased here :) \r\n\r\n--\r\n\r\nI have no idea about commercial vs non-commercial nature of stuff, but conda-forge is super-strictly separate from anaconda (at least in my understanding) and they only host our packages on anaconda.org. NVIDIA publishes random things on conda-forge, as well as others, so I usually operate with the assumption that it is a very relaxed community with loose rules. I can probably get someone to weigh on better on this --- some of the core developers at conda-forge have financial ties with companies and they have some policies for these people. As a personal example, I have never personally used any of Anaconda's stuff, and I only use things through conda-forge (and I have no ties to Anaconda at all; I am just a random academic researcher who helps maintain some packages under conda-forge)\r\n\r\n--\r\n\r\nHey! Check out mambaforge :) (mambaforge is miniforge but it comes preinstalled with the mamba solver, which is significantly faster --- and better! --- than the conda solver in miniforge; I believe sooner or later the mamba solver will be adopted into the conda solver though)' by a CONTRIBUTOR of type User on 2022-08-13T20:31:21Z\n'In a sense, this JEP has landed! We now ship both Google TPU and NVIDIA GPU wheels as JAX plugins (e.g., `jax-cuda12-plugin`). Closing because the exact design is slightly stale, but we've pretty much delivered on the intent, finally!' by a COLLABORATOR of type User on 2023-12-14T22:43:06Z\n\nPR has review comments:\n'To my knowledge, the mechanisms mentioned here are for discovering plugins _that are installed_, not for discovering plugins _that exist_.  How would we notify a user who lacks such backend plugin(s) that their jaxlib will be hampered unless they further install something else (even on CPU, since apparently there would be a jaxlib-cpu optional backend as well, from what I'm understanding)?  If my understanding of all that is accurate, then doesn't this somewhat go against the opening premise that this is seeking to be\r\n> Simpler for users: users would be able to install a complete `jax` on CPU with `pip install jax`.\r\n\r\n?' by a COLLABORATOR of type User on 2022-08-12T17:12:01Z\n'This thing of \"build just one of X and not for every single python version\" can also be accomplished by building one python-agnostic shared core library and then having the python bindings all in the same wheel that are minimal wrappers around that core.  See also https://github.com/tensorflow/tensorflow/pull/55941' by a COLLABORATOR of type User on 2022-08-12T17:17:23Z\n'Indeed, this is the same idea just split across multiple wheels instead of having one wheel with multiple builds for different Python versions.\r\n\r\nI think I prefer having a wheel split in here! It better describes to the Python packaging system what is happening and will degrade more gracefully when someone releases, say, a new Python version that we don't support. If we use separate packages, then `pip` will understand that it does not have a suitable wheel.' by a COLLABORATOR of type User on 2022-08-12T18:18:53Z\n'Correct, this is for discovering the plugins you have installed.\r\n\r\n`jaxlib-cpu` we should require as a hard dependency, at least for now.\r\n\r\nFor a CUDA build, we can do two things:\r\na) We can suggest the user install `jax[cuda]` that adds `jaxlib-cuda` as a dependency.\r\nb) We could also add code to core `jax` that detects if the user has `/dev/nvidia` devices but no `jaxlib-cuda` and suggests installing `jaxlib-cuda` (and similarly for other hardware for which there exists a plugin).\r\n\r\n' by a COLLABORATOR of type User on 2022-08-12T18:21:18Z\n'> If we use separate packages, then pip will understand that it does not have a suitable wheel.\r\n\r\nThat's true.\r\n\r\nGlad we're on the same basic page here, thanks.' by a COLLABORATOR of type User on 2022-08-12T18:57:16Z\n'> `jaxlib-cpu` we should require as a hard dependency, at least for now.\r\n\r\nI wondered if that's how it would be done; I just didn't see that in the doc.\r\n\r\n> `jax[cuda]`\r\n\r\nExtras are a helpful strategy here, I agree.  (One possible gotcha is that the extra is on jax instead of on jaxlib, yet the == version match required from jaxlib to the extra and not from jax to the extra, which might get tricky.)\r\n\r\n> We could also add code to core jax that detects if the user has /dev/nvidia devices but no jaxlib-cuda and suggests installing jaxlib-cuda (and similarly for other hardware for which there exists a plugin).\r\n\r\nThat would be helpful I think.  Please be aware that `/dev/nvidia*` device nodes, such as `/dev/nvidiactl`, are specific to Linux and to our discrete GPU drivers (Tegra drivers, for example, have `/dev/nvgpu` instead, and WSL2 has `/dev/dxg`; native Windows is another thing entirely).' by a COLLABORATOR of type User on 2022-08-12T19:03:21Z\n'Just curious: is it distributed differently internally?' by a CONTRIBUTOR of type User on 2022-08-13T16:07:31Z\n'We do that in conda-forge btw... The mechanism for building these pure Python packages (we call them `noarch: python`) is through `pip install .` and then the resulting content of that wheel gets uploaded to the conda-forge channel on anaconda.org \r\n\r\nrepo: https://github.com/conda-forge/jax-feedstock/blob/main/recipe/meta.yaml\r\nresults: https://anaconda.org/conda-forge/jax' by a CONTRIBUTOR of type User on 2022-08-13T16:09:21Z\n'Yes, that's a good idea! To give you an idea of how we accomplish this in conda-forge:\r\n1. we set jaxlib as a dep of jax with the declared minimum: https://github.com/conda-forge/jax-feedstock/blob/2769c6b471f2414d4863d3e06b191732ba7f7339/recipe/meta.yaml#L31\r\n2. we create an automatic constraint whenever we build jaxlib such that jaxlib can only be installed alongside a jax that has the minimum version of the built jaxlib: https://github.com/conda-forge/jaxlib-feedstock/blob/b81dec0dba3a54aef8d8e4e29c8a06a0bc4fd13b/recipe/meta.yaml#L76 (note jaxlib can still be installed alone separately, the constraint only comes into play when jax is added)' by a CONTRIBUTOR of type User on 2022-08-13T16:12:33Z\n'Again, we do this in conda-forge. I am repeating myself here to just illustrate how we do it and that can be useful for you when deciding to distribute through pypi. We don't have rocm support yet, but for everything else we do:\r\n- jaxlib with a build tag starting with \"cpu\", i.e. `jaxlib==VERSION=*cpu*`, is your jaxlib-cpu\r\n- jaxlib with a build tag starting with \"cuda\", i.e. `jaxlib==VERSION=*cuda*`, is your everything plus jaxlib-cuda\r\n\r\nIn conda-forge, we only build for cuda 11.2 and we rely on enhanced forward compatibility to basically support 11.2+. Thereefore, `conda install jaxlib==*=*cuda*` gets you jaxlib built cuda112, but it installs with cudatoolkit 11.7.0+, cudnn 8.4.1.50+, and other latest software. One could refuse cudatoolkit 11.7 btw and choose to have any 11.2+ if they wish.\r\n\r\nSee our packages here: https://anaconda.org/conda-forge/jaxlib/files?version=0.3.14' by a CONTRIBUTOR of type User on 2022-08-13T16:18:24Z\n'Yes. We don't use Python packages internally: we build everything, e.g., the Python interpreter, all necessary modules, etc. for each Python program, bundled as a statically-linked monolithic binary built with Bazel. It's the same code, but we bundle it very differently! This is highly unusual and unlike how most people use Python modules, and we don't expect anyone else will want to do that. The fact that we do this has little bearing on this discussion, though!' by a COLLABORATOR of type User on 2022-08-13T19:12:26Z\n'Yes, just a mild curiosity :) ' by a CONTRIBUTOR of type User on 2022-08-13T20:32:43Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 12,
        "reason_for_closure": "Inconsistent behavior change; jax should support broader numpy functionality.  PR closed."
    },
    {
        "summary": "Pull Request '11475' titled 'adds scipy.stats.mode-esque function' was authored by a User, who is associated as a NONE. \nIt was created at 2022-07-13T05:18:43Z, and was closed at 2022-10-11T16:14:59Z by a User.\nIt has a body of 'Solution for (#8978)\r\n\r\nThis PR seemed stale (#9141), and there were some issues with the shape of the result, which is why the tests were failing. It was easier to start a fresh branch.\r\n\r\nI'm new to Jax. I appreciate any feedback you're willing to give.\r\n\r\nSuper cool project\r\n\r\n--\r\nNote: Apologies in advance for the many branches. \r\n\r\nGoogle's cla scan is not seeing my commit email as being related to the cla I signed. Hopefully rewriting and starting a new branch will trigger it.'\nPR has comments:\n'So there is some bug in the cla scan that looks for the first email associated with the cla or takes some time to find the second. Editing the author on the commit alone did not fix the failed cla check, but starting a new branch with the corrected email did.' by a NONE of type User on 2022-07-13T05:21:37Z\n'Sorry for the CLA issue – the problem is that *every* commit in the PR needs to be from a known author with a signed CLA, so adding a second commit with a different author email is not sufficient. Glad you finally figured it out! For future reference, if you need to \"start from scratch\" on a branch, you can do so by force-pushing to the existing branch rather than by opening a new PR.\r\n\r\nThanks for taking a look at this! I'll review shortly 😀 ' by a COLLABORATOR of type User on 2022-07-13T16:08:30Z\n'Thanks for the detailed review and your time @jakevdp . I'm enjoying the project.\r\n\r\nResponded to your points and implemented.\r\n\r\nNo worries on the cla.\r\n\r\nIf you've got any recommendations on issues to work on next, I'd like to either get (a tad) closer to the compiler or implement something that makes use of `grad` within the code base.\r\n\r\nLet me know if any further updates would be nice here' by a NONE of type User on 2022-07-14T04:20:13Z\n'Shoot, that cla error does not belong to me - was trying to get all the latest changes in the branch, and I brought in a commit from a-googler who didn't sign a cla.\r\n\r\nAnyway, it's close, but I did notice two oddities.\r\n1) `jax.scipy.stats.mode(2, axis=0)` produces `ValueError` for axis out of bounds. `scipy.stats.mode(2, axis=0)` produces a result.\r\n\r\nI'm going to hack on that this weekend and check through a few other edge cases.\r\n\r\n2) Am I using `_wraps` for docstrings appropriately? In an ipython debug session I was running `scipy.stats.mode??` and seeing documentation, but `jax.scipy.stats.mode??` gave me back code. Maybe just strageness with ipython but throwing it out there.' by a NONE of type User on 2022-07-15T05:47:55Z\n'It looks like you merged onto the main branch; try rebasing on the main branch instead (we'll have to do this eventually anyway to conform to the single-commit change policy; see https://jax.readthedocs.io/en/latest/contributing.html#single-change-commits-and-pull-requests)' by a COLLABORATOR of type User on 2022-07-15T16:00:20Z\n'> Am I using _wraps for docstrings appropriately? In an ipython debug session I was running scipy.stats.mode?? and seeing documentation, but jax.scipy.stats.mode?? gave me back code. Maybe just strageness with ipython but throwing it out there.\r\n\r\ndouble question marks in IPython shows source code, while single question marks show the docstring. It sounds like this is working as expected, given that in scipy the docstring literal is in the source code, while in JAX it's included programatically.' by a COLLABORATOR of type User on 2022-07-15T16:02:10Z\n'Any update? You should be able to get your branch in order using something like this\r\n```bash\r\n$ git checkout main\r\n$ git pull upstream main # <-- assuming upstream is the name you used to point to the main jax repo\r\n$ git checkout adds_scipy_stats_mode\r\n$ git rebase main\r\n```' by a COLLABORATOR of type User on 2022-07-25T17:51:38Z\n'Thanks for the work on this – closing in favor of https://github.com/google/jax/pull/12683' by a COLLABORATOR of type User on 2022-10-11T16:14:59Z\n\nPR has review comments:\n'2022' by a COLLABORATOR of type User on 2022-07-13T16:08:58Z\n'nit: alphabetize imports' by a COLLABORATOR of type User on 2022-07-13T16:09:22Z\n'I don't think `Iterable[Any]` is the correct annotation here - for example, you use`x.size` attribute below, which is not in general present on an iterable.\r\n\r\nShould it be `x: jnp.ndarray`?' by a COLLABORATOR of type User on 2022-07-13T16:10:56Z\n'After `_check_arraylike`, please call `x = jnp.asarray(x)` to ensure that `x` is converted to a `DeviceArray`.\r\n\r\nAlso, there is a `canonicalize_axis` utility used elsewhere that I think would be relevant here. Regarding handling of `axis=None`, what would you think about doing it this way, so that `axis` can always be an integer in the helper? https://github.com/google/jax/blob/f0b139a59a749f6b3a4776acc50c7a2fa57e9a9e/jax/_src/numpy/reductions.py#L565-L567\r\n\r\nA benefit of that I think is that you'd end up calling the `_mode_helper` code in only a single location, so it may lead to a cleaner implementation.' by a COLLABORATOR of type User on 2022-07-13T16:17:48Z\n'```suggestion\r\n    raise ValueError(f\"Illegal nan_policy value {nan_policy!r}; expected one of {'propoagate', 'omit', 'raise'}\")\r\n```' by a COLLABORATOR of type User on 2022-07-13T16:19:28Z\n'Please add a message for the user - also perhaps a TODO to implement this? I think it could be done in a JIT-compatible way if we're careful.' by a COLLABORATOR of type User on 2022-07-13T16:20:57Z\n'This will fail with an obscure error under JIT. We should raise a more informative error instead. We handle a similar issue here: https://github.com/google/jax/blob/f0b139a59a749f6b3a4776acc50c7a2fa57e9a9e/jax/_src/numpy/lax_numpy.py#L795-L798' by a COLLABORATOR of type User on 2022-07-13T16:23:25Z\n'I think we could inline the contents of the helper function and jit-compile the `mode` function itself. The main reason that you see the helper pattern throughout JAX's codebase is because for a long time, JIT didn't properly support keyword arguments, so we couldn't use `jit` as a wrapper of top-level API functions that accept keywords. These days that's not a worry.' by a COLLABORATOR of type User on 2022-07-13T16:25:23Z\n'Here and below, please use `with self.assertRaises` (or better `self.assertRaisesRegex`)' by a COLLABORATOR of type User on 2022-07-13T16:26:04Z\n'`_CheckAgainstNumpy` should handle functions that return tuples – did you try using it directly?' by a COLLABORATOR of type User on 2022-07-13T16:32:00Z\n'`axis` and `nan_policy` are static arguments, so we should leave them out of `args_maker` and instead pass them to the functions via closure or `partial`. Then the same functions & `args_maker` can be passed to both `_CheckAgainstNumpy` and `_CompileAndCheck`.' by a COLLABORATOR of type User on 2022-07-13T16:33:16Z\n'Eg; ```from numpy.typing import ArrayLike```\r\nhttps://numpy.org/devdocs/reference/typing.html\r\n\r\nYou're right, it's not quite right. It's probably semantics, but I felt typing as `jnp.ndarray` limiting given this function should handle anything cleared by the `_check_arraylike` func.\r\n\r\nI'll go with `jnp.ndarray` for now, but it may be a point to define exactly in typing form what `_check_arraylike` allows. Not sure if there's enough usecase for a typing module for Jax.' by a NONE of type User on 2022-07-14T00:13:51Z\n'And it will definitely be `jnp.ndarray` based on one of your next points, for the hidden functions.' by a NONE of type User on 2022-07-14T00:16:25Z\n'I'll trust your thoughts here. My only concern would be the performance implications.\r\n\r\nAn exaggerated example being calling `mode` successively in a loop, always with `axis=None` with a large `x`.\r\n\r\nUnfortunately, I have a Radeon VII on this machine, so I don't think I could profile that appropriately right now.\r\n' by a NONE of type User on 2022-07-14T00:22:52Z\n'For sure, I can tackle that next. Adding TODO' by a NONE of type User on 2022-07-14T01:00:37Z\n'Ok great' by a NONE of type User on 2022-07-14T01:19:45Z\n'My understanding is that this conditional branch depends on the state of `x` (ie; that it contains nans or not), which defeats the purpose of JIT compiling the `mode` function. \r\n\r\nSo, if I am understanding you correctly, this would fail whether or not there were nans present - it's failing because in order to have a tracer object, we can't know what the contents are, only the shape.' by a NONE of type User on 2022-07-14T03:05:44Z\n'Ah yes, I see how this works now, thanks ' by a NONE of type User on 2022-07-14T03:07:43Z\n'Yes, I see this now thanks.' by a NONE of type User on 2022-07-14T03:09:19Z\n'The logic is accounted for by `moveaxis` I believe' by a NONE of type User on 2022-07-14T04:55:32Z\n'Since the function is wrapped in `jit` by default, this will always raise. Better would be either (1) raise `NotImplementedError` directly rather than using `concrete_or_error`, or (2) not wrap `mode` in `jit` so that `mode = 'raise'` can be implemented for concrete inputs `x`. I'm fine either way.' by a COLLABORATOR of type User on 2022-07-14T16:15:59Z\n'We should ensure that the dtype of the arrays here match the expected result.\r\n\r\nAlso, not all empty arrays are the same shape – e.g. the output of mode for an array of shape `[0, 5]` along `axis=1` should probably have shape `[0, 1]`' by a COLLABORATOR of type User on 2022-07-14T16:17:05Z\n'As written here, `mode(2, axis=1000)` fails to raise an error.' by a COLLABORATOR of type User on 2022-07-14T16:18:05Z\n'We should validate the axis before using it (`canonicalize_axis` is a useful utility that raises informative errors in common cases like non-static inputs)' by a COLLABORATOR of type User on 2022-07-14T16:19:15Z\n'A couple things here:\r\n- We generally use underscores rather than semicolons in test names.\r\n- please use the `format_shape_dtype_string` utility for the shape and type\r\n- please add parameter names to the test string; e.g. `_axis={axis}_`\r\n- Use single quotes for a one-line string' by a COLLABORATOR of type User on 2022-07-14T16:27:27Z\n'Easier (and more complete) to generate tests via an outer product; e.g. https://github.com/google/jax/blob/86ab8a84ee9c0c1fc718f3d322f7f597bebb812b/tests/scipy_stats_test.py#L699-L704\r\n\r\nI'd probably skip the `nan_policy` parameterization here, though, because as currently implemented only `\"propagate\"` is valid' by a COLLABORATOR of type User on 2022-07-14T16:30:34Z\n'Apply the partial here rather than below for clarity.' by a COLLABORATOR of type User on 2022-07-14T16:31:54Z\n'Word, thanks' by a NONE of type User on 2022-07-15T02:10:33Z\n'Ah, ok nice catch. I see the  usefulness of `canonicalize_axis` now' by a NONE of type User on 2022-07-15T03:15:58Z\n'Yes, I see this now, thank you' by a NONE of type User on 2022-07-15T03:16:17Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 30,
        "reason_for_closure": "Superseded by another PR/fix."
    },
    {
        "summary": "Pull Request '11343' titled 'Formated the file and updated to latest versions' was authored by a User, who is associated as a NONE. \nIt was created at 2022-07-01T22:16:37Z, and was closed at 2022-07-06T17:39:41Z by a User.\nIt has a body of 'setup updated versions'\nPR has comments:\n'Please revert the formatting changes.' by a COLLABORATOR of type User on 2022-07-06T17:37:37Z\n'Closing because I don't think the changes here should be merged.' by a COLLABORATOR of type User on 2022-07-06T17:39:41Z\n\nPR has review comments:\n'This constraint doesn't look correct to me. e.g., JAX does work with NumPy 1.23. All we require is NumPy >= 1.19 at present.\r\n\r\nFurther, we have a policy about this: https://jax.readthedocs.io/en/latest/deprecation.html' by a COLLABORATOR of type User on 2022-07-03T19:19:41Z\n'Is this version of absl-py actually needed? (It's probably not a bad idea to set a minimum version, though, since we will most likely accidentally depend on absl-py features at some point in the latest version.)' by a COLLABORATOR of type User on 2022-07-03T19:20:20Z\n'Is there a reason to add an `opt_einsum` version constraint?' by a COLLABORATOR of type User on 2022-07-03T19:20:56Z\n'This constraint is too strict. The choice of 1.5 as a minimum version is intentional: while not documented as a policy, we chose it to be released around the same time as the minimum NumPy version.' by a COLLABORATOR of type User on 2022-07-03T19:21:30Z\n'I'm not sure why we have a maximum version constraint here. Revert?' by a COLLABORATOR of type User on 2022-07-06T17:38:30Z\n'Revert the opt_einsum change.' by a COLLABORATOR of type User on 2022-07-06T17:38:43Z\n'In fact, this is the only change that makes any sense to me (specify a minimum version on typing_extensions, but I wonder if it's even worth doing...)' by a COLLABORATOR of type User on 2022-07-06T17:39:19Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 7,
        "reason_for_closure": "Unnecessary: JAX behavior should supersede NumPy's; complex inputs are valid."
    },
    {
        "summary": "Pull Request '11111' titled 'Add tree_labels, tree_print.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-06-15T17:57:37Z, and was closed at 2023-07-20T13:11:52Z by a Organization.\nIt has a body of 'This PR follows from https://github.com/tensorflow/tensorflow/pull/56202, where the pytree walk function has been extended to pass node_data to walkers.  For example, this allows implementation of custom printers (see https://github.com/google/jax/issues/5047), which can e.g. print a tree with path labels like:\r\n\r\n```\r\nobj/a1 = leaf-a1\r\nobj/a2/b1/0 = 111\r\nobj/a2/b1/1 = 2\r\nobj/a2/b1/2 = 3\r\nobj/a2/b2 = [[11. 12. 13.]; [21. 22. 23.]]\r\nobj/a2/b3/c1 = 33\r\nobj/a2/b3/c2/0 = 44\r\nobj/a2/b3/c2/1 = 55\r\nobj/a3/x1/x11 = a\r\nobj/a3/x1/x12/y1/y11 = ya\r\nobj/a3/x1/x12/y1/y12 = yb\r\nobj/a3/x1/x13 = b\r\nobj/a3/x2/x21 = 2a\r\nobj/a3/x2/x22/y1/y11 = ya\r\nobj/a3/x2/x22/y1/y12 = yb\r\nobj/a4 = fred\r\n```'\nPR has comments:\n'@hawkinsp as you are looking at the corresponding TF PR, this is the JAX one.' by a CONTRIBUTOR of type User on 2022-06-15T22:54:59Z\n'@hawkinsp I have rebased this on top of main - hopefully it's OK to approve given the TF change went in.' by a CONTRIBUTOR of type User on 2022-07-19T15:35:35Z\n\nPR has review comments:\n'Adapt to old API, could just choose now to delete this function, or propagate the API to the currently unknown list of clients of this function.' by a CONTRIBUTOR of type User on 2022-06-15T18:11:54Z\n'See TODO below.' by a CONTRIBUTOR of type User on 2022-06-15T18:12:11Z\n'This is just to make them visually more distinctive in debug output' by a CONTRIBUTOR of type User on 2022-06-15T18:13:09Z\n'Needed for xla_extension_version check below.' by a CONTRIBUTOR of type User on 2022-06-15T18:13:31Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 4,
        "reason_for_closure": "PR functionality superseded by existing functionality, deemed unnecessary."
    },
    {
        "summary": "Pull Request '11093' titled 'Implement Chebyshev class and functions' was authored by a User, who is associated as a NONE. \nIt was created at 2022-06-14T15:51:36Z, and was closed at 2023-11-03T22:15:42Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'Adds a Chebyshev convenience class and cheb* functions that act on arrays, as described in #11055.'\nPR has comments:\n'Hi - thanks for the PR! In the past when we've discussed the polynomial interface, we've made the choice to avoid adding polynomial class types in favor of the more direct functional API (e.g. https://github.com/google/jax/issues/70#issuecomment-907892885). One concern I have here is that when you start to define class-based interfaces, you have to be much more careful to ensure that they're compatible with JAX's transformation model.\r\n\r\nIs there any functionality here that cannot be expressed directly via chebyshev functions, as opposed to a full chebyshev class?' by a COLLABORATOR of type User on 2022-06-14T16:17:58Z\n'I believe the Chebyshev functions capture all of the functionality, so getting rid of the full class shouldn't cause any problems and would honestly simplify the interface. Would you like me to remove it?' by a NONE of type User on 2022-06-14T16:31:46Z\n'I would lean toward keeping only the simplest functional APIs in JAX, partly because it makes long-term maintenance more sustainable (the class definition in this PR is a **gigantic** API surface to test and maintain). But I'm happy to let other team members weigh-in if they disagree.' by a COLLABORATOR of type User on 2022-06-14T16:39:39Z\n'Alright, I removed the Chebyshev class and its abstract base class to streamline the API. If it's decided later that they are worth including, I have local copies and can add them back in.' by a NONE of type User on 2022-06-14T19:15:49Z\n'Alright, I've cleaned up a lot of the code following your suggestions and to better match the existing polynomial functions in JAX. I've also been thinking more about JAX compatibility, and I wanted to ask a couple questions. The object arrays and value-based shapes were unnecessary, but loop implementations are necessary (or at least the natural way to implement many functions) because of the relationships between terms in a Chebyshev series. \r\n\r\nSo, what do you consider \"large\" for a series or loop parameter? If a typical series is large, would replacing Python control flow  with JAX control flow primitives resolve the incompatibility/inefficiency issues? If so, I will make those changes and then push everything.' by a NONE of type User on 2022-06-16T16:24:30Z\n'The issue with Python loops is that they are flattened by JAX tracing, and then passed in an unrolled state to the XLA compiler. The mechanics of the compiler are complicated, but a good rule of thumb is that compilation time will be roughly quadratic in the number of operations. So if you write a Python `for` loop over 100 elements, then compilation will be roughly 10000 times more costly than the equivalent `fori_loop` implementation. There's not really any hard line regarding what size the loops should be, but if you expect the functions to operate on larger inputs, it's something to keep in mind. Does that make sense?' by a COLLABORATOR of type User on 2022-06-16T16:29:28Z\n'Yes, that's very helpful, thank you!' by a NONE of type User on 2022-06-16T16:32:29Z\n'Ok, I've removed all Python loops from chebyshev.py and replaced them with JAX control flow primitives. I've also cleaned up most of the functions to make them less convoluted where possible. Is there anything else you would like me to do?\r\nThank you!' by a NONE of type User on 2022-06-20T19:41:28Z\n'Thanks. Another big piece of adding this kind of functionality is testing – before we can merge this, we'll need full test coverage of the functionality. If it's helpful, you can base it off of existing tests in https://github.com/google/jax/blob/main/tests/polynomial_test.py. Let me know if you have questions about that!\r\n\r\nAlso, just FYI we have some work in progress that will add some constraints to this code (see #10840): essentially, JAX library code will no longer be able to rely on implicit dtype promotion once that is merged. You can run your tests locally with that condition by setting the flag value `JAX_NUMPY_DTYPE_PROMOTION=strict`' by a COLLABORATOR of type User on 2022-06-21T16:36:11Z\n'Alright, I have a couple questions. First of all, many NumPy Chebyshev functions trim trailing zeros before returning, a feature I omitted from the JAX versions for jit compatibility. When comparing against NumPy, should I just wrap JAX-based versions of these functions in a trimzeros function, or is there a better way to test that ignores length differences? \r\n\r\nSecond, what is the desired behavior for functions under the new dtype promotion constraints? Should functions like chebadd raise an error if the input arrays have different dtypes, should they attempt explicit dtype promotion, or should they do something else?\r\n\r\nThank you!' by a NONE of type User on 2022-06-22T16:15:10Z\n'Excellent question - one strategy we typically use in this situation (where the numpy behavior is incompatible with JIT) is to add an additional keyword argument, set so that the default value matches the behavior of numpy, and so that it can be changed to modify the API to be compatible with JIT (you can see an example in the `strip_zeros` keyword to `jnp.roots`, which I just finished revamping in #11215)\r\n\r\nI think you could use a very similar treatment here.\r\n\r\nRegarding dtypes: the rule of thumb is that if a dtype mismatch comes from two arrays passed by the user, you should let the error through. If it comes from the implementation (e.g. you do something like `x + jnp.arange(x.shape[-1])` and it errors because `x` is float32) then you should adjust the implementation to be compatible with strict promotion. Does that make sense?' by a COLLABORATOR of type User on 2022-06-22T22:42:45Z\n'Yes, that all makes sense. Thank you!' by a NONE of type User on 2022-06-22T23:18:24Z\n'I've finished writing the Chebyshev tests, and they can all pass - with some caveats. The output of some functions (chebpow, chebfit, chebval, chebroots) can have mismatched values further apart than the tolerance of 1e-6 that I've been using, especially for higher-order inputs. Are there ways to make JAX output better match NumPy output, or are those numerical differences unavoidable?\r\n\r\nSecond, some tests can also fail in cases where NumPy warns but still returns output matching the JAX output. How should I handle these cases?' by a NONE of type User on 2022-06-24T15:20:45Z\n'Regarding precision – I suspect this is due to JAX executing in float32, while numpy executes in float64. You can execute the tests with `JAX_ENABLE_X64=true` to get 64-bit execution\r\n\r\nRegarding warnings - you can filter them if they're expected. You can see examples of this elsewhere in the tests, e.g. https://github.com/google/jax/blob/4c25ef1d0039ecdd517ce2b09605322797700c2a/tests/lax_numpy_test.py#L579-L584' by a COLLABORATOR of type User on 2022-06-24T15:41:35Z\n'Ok, all test pass with JAX_ENABLE_x64=true and JAX_NUMPY_DTYPE_PROMOTION=strict. I also updated applicable functions with a \"trim\" keyword argument so that they match NumPy behavior by default. What's the next step?\r\nThank you!' by a NONE of type User on 2022-06-27T16:13:32Z\n'Implemented the changes you requested, and hopefully fixed what caused the chebcompanion test to fail last time.' by a NONE of type User on 2022-06-29T16:08:30Z\n'Made the changes you requested. Also, the Chebyshev test suite takes 75-80 seconds to run on my laptop using the command you provided. ' by a NONE of type User on 2022-07-11T16:03:41Z\n'That's a bit concerning, to be quite honest: currently our CI jobs run in about 20-30 minutes, so this PR would be adding 3-5% testing overhead for a single new API. Our strategy for avoiding test timeouts is generally to reduce `NUM_GENERATED_CASES` globally once we start hitting timeout limits... I'm not sure it makes sense to reduce our package-wide test coverage by 5% in order to add a few chebyshev polynomial routines that probably only a handful of users will ever use.\r\n\r\nThis is a tricky one, I'm not sure what to recommend...' by a COLLABORATOR of type User on 2022-07-11T16:31:35Z\n'After some more testing, I believe the Chebyshev tests actually contribute a much smaller percentage of total testing time: I ran the full test suite on my laptop yesterday, and it took about 2 hours and 18 minutes, or ~8300 seconds. Running the Chebyshev test file individually took 75-80 seconds, which is 0.9-1% of the total. I just ran the test suite remotely on a much more powerful computer, which completed all the tests in 1270 seconds and the Chebyshev tests alone in 11 seconds, which is ~0.866% of the total.\r\nI'm happy to repeat these tests or try other tests you think would be helpful to verify this ratio. 1% of the testing overhead is still significant, so I've also experimented with reducing the Chebyshev tests, as I noticed that polynomial_tests.py only includes tests for jnp.roots. \r\nMany of the cheb* functions call other cheb* functions - for example, chebpow calls chebadd, so the chebadd tests could potentially be removed. Changes that break chebadd would (hopefully) be caught by the chebpow tests. Another category of functions, the 2d and 3d versions of chebval/chebgrid/chebvander, are simply wrappers for nested calls of the respective 1d functions. Tests for these could also be removed, as the 1d function tests should cover everything.\r\nRemoving tests for all functions that fall under the two categories described above cuts the time of the Chebyshev tests in half, and a majority of this cost is due to only a few functions: chebfit, chebdiv, and chebfromroots. Reducing the number of test cases for these particular functions or eliminating tests for them entirely could significantly speed up the Chebyshev tests.\r\n\r\nThank you for your time, let me know what you think!' by a NONE of type User on 2022-07-12T20:11:01Z\n'Thanks - that makes sense. Let's go ahead and merge this and we can deal with any test timeout issues separately. Thanks for all the work here!' by a COLLABORATOR of type User on 2022-07-18T20:46:43Z\n'The test failures here look real: it looks like `chebgrid` does some implicit type promotion when it is passed integer inputs.' by a COLLABORATOR of type User on 2022-07-18T21:26:38Z\n'Ok, should be fixed now. All tests are passing locally for every combination of  JAX_ENABLE_X64 and JAX_NUMPY_DTYPE_PROMOTION.' by a NONE of type User on 2022-07-18T22:21:31Z\n'On some further internal testing, the new tests show a number of failures. First, since nonsymmetric eigendecomposition is not implemented for GPU or TPU, the tests need to be skipped on those backends. Second, there's a strange failure that looks to be due to an invalid input to the numpy reference function:\r\n```\r\nTestChebyshev.testChebroots_dtype=int32[5]\r\n\r\nTraceback (most recent call last):\r\n  File \"absl/testing/parameterized.py\", line 312, in bound_param_test\r\n    return test_method(self, **testcase_params)\r\n  File \"jax/tests/chebyshev_test.py\", line 445, in testChebroots\r\n    self._CompileAndCheck(jnp_fn, args_maker)\r\n  File \"/jax/_src/test_util.py\", line 846, in _CompileAndCheck\r\n    python_ans = fun(*args)\r\n  File \"jax/_src/numpy/polynomials/chebyshev.py\", line 634, in chebroots\r\n    return _chebroots(c)\r\nSystemError: <CompiledFunction of <function _chebroots at 0x7fce1424aa60>> returned a result with an error set\r\n```' by a COLLABORATOR of type User on 2022-07-19T16:18:00Z\n'Ok, the chebroots test is now skipped on GPU and TPU. That test also no longer generates inputs with untrimmed trailing zeros, which I expect will fix the other problem.\r\nEdit: The internal checks are still failing - is it still a problem with testing chebroots?' by a NONE of type User on 2022-07-19T18:31:41Z\n'Hi - there are still a number of failures in the larger tests. I think it's easier for me to try to patch the imported PR to adjust test tolerances; I'll try to look at that today.' by a COLLABORATOR of type User on 2022-07-22T17:47:44Z\n'Hi, is there anything else you'd like me to change or add before the workflow is approved?' by a NONE of type User on 2022-08-17T15:26:45Z\n'Hi - sorry I lost track of this over the summer. Let me see if I can land it now' by a COLLABORATOR of type User on 2022-08-17T21:53:11Z\n'For the sake of bazel-based tests, can you add the following block for the new test file in https://github.com/google/jax/blob/main/tests/BUILD, keeping alphabetical order?\r\n```\r\njax_test(\r\n    name = \"chebyshev_test\",\r\n    srcs = [\"chebyshev_test.py\"],\r\n)\r\n```' by a COLLABORATOR of type User on 2022-08-17T22:11:30Z\n'Done!' by a NONE of type User on 2022-08-18T14:18:14Z\n'We're still running into a bunch of tolerance-related failures on GPU and TPU.' by a COLLABORATOR of type User on 2022-08-18T17:20:58Z\n\nPR has review comments:\n'How long are typical series? If it's more than a few, we should avoid using the Python `while` loop here because we could end up with *very* long compile times.' by a COLLABORATOR of type User on 2022-06-14T19:59:58Z\n'make this `zs.dtype` to avoid unintentional dtype promotion.' by a COLLABORATOR of type User on 2022-06-14T20:00:50Z\n'Same comment here about Python control flow when the size gets large. We may want to use `lax.fori_loop` instead unless `n` is bounded by a small number.' by a COLLABORATOR of type User on 2022-06-14T20:01:30Z\n'We cannot create jax.numpy arrays at import time, because in some systems XLA will not yet be initialized when this is executed. Options are (1) use `np.array`, or (2) if `jnp.array` is important, define this lazily using a module `__getattr__`' by a COLLABORATOR of type User on 2022-06-14T20:02:44Z\n'Also here: consider `fori_loop`' by a COLLABORATOR of type User on 2022-06-14T20:04:45Z\n'consider `while_loop`' by a COLLABORATOR of type User on 2022-06-14T20:05:19Z\n'This is a pattern we use frequently, though I'd write it `c, = _promote_dtypes_inexact(c)`' by a COLLABORATOR of type User on 2022-06-14T20:06:33Z\n'What does `NO JIT` here mean? You've wrapped the outer function in `JIT` so this will be executed within a jit context.' by a COLLABORATOR of type User on 2022-06-14T20:08:34Z\n'Same comment here: the jit decorator above will apply to these lines.' by a COLLABORATOR of type User on 2022-06-14T20:09:25Z\n'```python\r\nc, = _promote_dtypes_inexact(c)\r\n```' by a COLLABORATOR of type User on 2022-06-14T20:09:58Z\n'should this be `jnp.zero` rather than `jnp.empty`? `empty` implies uninitialized values, and below it looks like you're adding numbers to them.' by a COLLABORATOR of type User on 2022-06-14T20:12:02Z\n'Consider using `np.iterable` since under tracing, JAX functions may return a tracer that cannot be used in control flow.' by a COLLABORATOR of type User on 2022-06-14T20:13:47Z\n'Perhaps use `operator.index` instead of `int` so that non-integer values will lead to an informative error' by a COLLABORATOR of type User on 2022-06-14T20:14:41Z\n'Consider using `fori_loop` if `cnt` will be large to avoid long compile times.' by a COLLABORATOR of type User on 2022-06-14T20:15:12Z\n'`copy=False` should be fine since JAX arrays are immutable' by a COLLABORATOR of type User on 2022-06-14T20:15:59Z\n'`return tuple(chebval(xi, c) for xi in (x, y, z))`' by a COLLABORATOR of type User on 2022-06-14T20:17:26Z\n'`reduce` here seems like an obfuscation. Is there anything wrong with this?\r\n```python\r\na = chebvander(points[0], deg[0])[(...,) + (slice(None), jnp.newaxis)],\r\nb = chebvander(points[1], deg[1])[(...,) + (jnp.newaxis, slice(None))]\r\nv = a * b\r\n```' by a COLLABORATOR of type User on 2022-06-14T20:20:00Z\n'This kind of reshape operation tends to choke when the inputs have a dimension of size zero.' by a COLLABORATOR of type User on 2022-06-14T20:20:53Z\n'This looks like a roundabout way of doing `_promote_dtypes_inexact`; you should instead call that directly.' by a COLLABORATOR of type User on 2022-06-14T20:21:49Z\n'`reduce` is a little better here than before, but I'd still prefer `v = a * b * c`' by a COLLABORATOR of type User on 2022-06-14T20:22:19Z\n'Use `promote_dtypes_inexact` instead of adding a float.' by a COLLABORATOR of type User on 2022-06-14T20:22:42Z\n'`promote_dtypes_inexact`' by a COLLABORATOR of type User on 2022-06-14T20:23:26Z\n'Throughout: please use two-space indentation to match the rest of the JAX packge.' by a COLLABORATOR of type User on 2022-06-14T20:25:37Z\n'This line will fail because of the `jit` decoration above and the incompatbility of `trimseq` with JIT' by a COLLABORATOR of type User on 2022-06-14T20:27:18Z\n'JAX does not support `dtype=object`' by a COLLABORATOR of type User on 2022-06-14T20:28:01Z\n'No need to copy because JAX arrays are immutable' by a COLLABORATOR of type User on 2022-06-14T20:28:24Z\n'If there are no arguments, you don't need to use an anonymous function. `jnp_fun = cs.chebline` would suffice (or, you can skip the renaming and just use the function directly in the check below)\r\n\r\nSame for other tests below.' by a COLLABORATOR of type User on 2022-06-27T23:07:53Z\n'Let's also call `self._CompileAndCheck`, here and in tests below.' by a COLLABORATOR of type User on 2022-06-27T23:08:22Z\n'We need a license statement at the top of each file.' by a COLLABORATOR of type User on 2022-06-27T23:08:47Z\n'Unintentionally added?' by a COLLABORATOR of type User on 2022-06-27T23:09:03Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 30,
        "num_review_comments": 30,
        "reason_for_closure": "Inconsistent behavior change; deprecates existing functionality without sufficient benefit."
    },
    {
        "summary": "Pull Request '10871' titled 'WIP: Implementation of Scipy Bootstrap' was authored by a User, who is associated as a NONE. \nIt was created at 2022-05-28T12:31:59Z, and was closed at 2023-11-03T22:14:04Z by a User.\nIt has a body of 'address https://github.com/google/jax/issues/10375'\nPR has comments:\n'Hello, @riven314!\r\nI'm interested in this PR, may I please address @jakevdp's reviews?\r\nThank you, and I hope you have a wonderful day!' by a CONTRIBUTOR of type User on 2022-08-23T14:59:28Z\n'Hi @wonhyeongseo \r\nThanks for your interest! Currently I don't have bandwidth on this ticket at least for these 2 months, so feel free to work on it! Otherwise, I can continue working on it whenever I retain my bandwidth.\r\n\r\nA few notes about my PR (you can further discuss with @jakevdp to see if those points are valid to address):\r\n1. the numerical discrepancy between my JAX implementation v.s. scipy's is NOT immaterial, despite ramping up the resampling number. Can't give you a precise statistics (discrepancy of ~0.01?) because I coded it a few months ago, but such discrepancy could probably make unit test to fail\r\n2. my JAX implementation is a bit slower than scipy's in some settings (CPU).\r\n3. Scipy bootstrap supports a lot of different scenarios (e.g. multi-statistics, one-set/ multi-set/ paired samples, different bootstrap methods). For now I am not confident if my implementation works as expected under various scenarios.\r\n\r\nLook forward to your contribution to the ticket.' by a NONE of type User on 2022-08-23T15:48:15Z\n'@wonhyeongseo Are you still interested in tackling this?' by a CONTRIBUTOR of type User on 2023-02-02T02:03:52Z\n'Hello @carlosgmartin ! Not at the moment because I don't know how. Would love to see your implementation! 😊' by a CONTRIBUTOR of type User on 2023-02-03T09:12:04Z\n'@wonhyeongseo Thanks for letting us know.\r\n\r\n@riven314 Would you like to continue working on your PR, or would you prefer someone else take over?' by a CONTRIBUTOR of type User on 2023-02-03T18:49:35Z\n'@carlosgmartin \nHi! I don't foresee I have bandwidth in the short run so I would appreciate if someone is willing to take over.\nI am glad to explain my code if any help is needed!\n' by a NONE of type User on 2023-02-03T18:52:10Z\n'Thanks for working on this – I think given the discussion in https://jax.readthedocs.io/en/latest/jep/12049-type-annotations.html this would now be considered out-of-scope for JAX itself. Sorry we weren't able to merge your contribution!' by a COLLABORATOR of type User on 2023-11-03T22:14:04Z\n\nPR has review comments:\n'2022' by a COLLABORATOR of type User on 2022-05-31T17:29:06Z\n'Unused import?' by a COLLABORATOR of type User on 2022-05-31T17:29:20Z\n'Here and throughout, please use two spaces for indentation' by a COLLABORATOR of type User on 2022-05-31T17:29:39Z\n'If `rng` is a `PRNGKey`, I think it would be clearer to call it `key` (`rng` usually implies a numpy-like rng object). Since this is a private utility, I don't think we need to worry about matching scipy's argument names.' by a COLLABORATOR of type User on 2022-05-31T17:30:44Z\n'This split should happen before `rng` is used to generate random numbers, e.g.\r\n```python\r\nrng, next_rng = jax.random.split(rng)\r\n```\r\n otherwise you may have subtle correlational bugs in the pseudo-random numbers.' by a COLLABORATOR of type User on 2022-05-31T17:33:19Z\n'So long as the scalars are returned as a tuple or list (rather than as part of an array), tree flattening should take care of this.' by a COLLABORATOR of type User on 2022-05-31T17:34:40Z\n'No need to use `as ndtr` or `as ndtri` (we do this elsewhere to satisfy mypy's `no-implicit-reexport`, but that's not relevant here since this is not a public submodule)' by a COLLABORATOR of type User on 2022-05-31T17:36:06Z\n'vmap is fine here, but do you need multiple vmaps if `theta_hat_b` has more than two dimensions?' by a COLLABORATOR of type User on 2022-05-31T17:36:30Z\n'If this is a public API, we can use `@_wraps` to fill-in the docstring' by a COLLABORATOR of type User on 2022-05-31T17:37:57Z\n'We should change the API so that an explicit random key is required, perhaps as the first argument for consistency with other `jax.random` APIs.' by a COLLABORATOR of type User on 2022-05-31T17:38:41Z\n'Is there a reason you chose scan instead of vmap? Because it's a serial operation, scan is going to be far slower than vmap on accelerators.' by a COLLABORATOR of type User on 2022-05-31T17:40:36Z\n'Same comment here regarding `scan` - we should express this in terms of `vmap` instead to take advantage of parallelism on accelerators.' by a COLLABORATOR of type User on 2022-05-31T17:41:39Z\n'As I have changed to use `vmap`, so now I do the splitting in the vmap argument: `jax.random.split(key, n_resamples)`' by a NONE of type User on 2022-06-02T13:32:54Z\n'should I use a control flow here to apply multiple `vmap` if I find that `theta_hat_b` have more than 2 dimensions?' by a NONE of type User on 2022-06-02T13:34:28Z\n'nice suggestion!\r\nfor `_bootstrap_resample_and_compute_statistic`, `vmap` does run faster than `scan`' by a NONE of type User on 2022-06-02T13:35:21Z\n'I found an issue here @jakevdp \r\n\r\nfor `_jackknife_resample_and_compute_statistic`, I weirdly find that `vmap` runs SLOWER than `scan`.\r\nOn CPU Colab with 10,000 samples, `vmap` speed is ~210 ms while `scan` speed is ~110 ms.\r\n\r\nHere is the Colab notebook that I did my benchmarking:\r\nhttps://colab.research.google.com/drive/1abKv-zyI-CZ4BEtXc3oh5Ebx_rcFq0dp?usp=sharing\r\n\r\nI am not sure if its because my implementation is inefficient, or I have made some errrors.' by a NONE of type User on 2022-06-02T13:37:37Z\n'You can use Python `for` loops to apply multiple `vmap` transforms; something like this:\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\n\r\ndef f(x):\r\n  return x.sum()\r\n\r\nx = jnp.ones((2, 3, 4, 5))\r\nfor i in range(x.ndim - 1):\r\n  f = jax.vmap(f)\r\nprint(f(x).shape)\r\n# (2, 3, 4)\r\n```' by a COLLABORATOR of type User on 2022-06-03T16:20:31Z\n'Interesting! On CPU `scan` and friends are generally not too bad, but I suspect if you benchmarked on GPU/TPU you'd find `scan` to be much slower than `vmap`.' by a COLLABORATOR of type User on 2022-06-03T16:21:53Z\n'This would be more consistent with JAX's typical approach if (1) `key` were the first argument, and (2) batching were handled via `vmap` at the call-site, rather than via an argument passed to the function. So the API would be something like this:\r\n```python\r\ndef _bootstrap_resample_and_compute_statistic(key, sample, statistic):\r\n  ...\r\n```\r\nand rather than calling it like\r\n```python\r\n_bootstrap_resample_and_compute_statistic(sample, statistic, n_resamples, key)\r\n```\r\nyou could instead call it like\r\n```python\r\nkeys = random.split(key, n_resamples)\r\nvmap(_bootstrap_resample_and_compute_statistic, (0, None, None))(keys, sample, statistic)\r\n```\r\nThe benefits would be (1) more explicit handling of key splitting by the user of the function, and (2) `vmap` at the outer levels may be somewhat more efficient (I'm not entirely sure on that, but I think it is the case) and (3) it's more maintainable, because it makes use of JAX's composable transforms in the way they're intended to be used, rather than hiding them behind less flexible batch-aware APIs.' by a COLLABORATOR of type User on 2022-06-21T18:18:08Z\n'Same comment here. Can we define `_jackknife_resample_and_compute_statistic` so it natively handles a single batch, and then use `vmap` as appropriate at the call-site?' by a COLLABORATOR of type User on 2022-06-21T18:20:17Z\n'Rather than vmapping, can we use the `axis` argument to `jnp.percentile`?' by a COLLABORATOR of type User on 2022-06-21T18:24:16Z\n'I don't understand the purpose of empty indexing here.' by a COLLABORATOR of type User on 2022-06-21T18:24:18Z\n'Typically APIs don't require object identity for boolean values.' by a COLLABORATOR of type User on 2022-06-21T18:25:50Z\n'You can use `extra_params` here to document the `key` argument.\r\n\r\nThat said, I'm starting to wonder if this should really be considered a wrapper of `scipy.stats.bootstrap`, because its API is now substantially different. In numpy's case, we don't provide any wrappers for `numpy.random` functionality, instead using a different key-based API in `jax.random`. I'm starting to think that the same treatment would make sense here, because as written `jax.scipy.bootstrap` must be called with a different signature than `scipy.bootstrap`.\r\n\r\nIt also would solve the issue of how to handle irrelevant params like `vectorized`, and we could write the API in a way that is more typical of JAX library functions (i.e. keep batching orthogonal to the implementation, rather than calling `vmap` within.\r\n\r\nWhat do you think?' by a COLLABORATOR of type User on 2022-06-21T18:31:55Z\n'@jakevdp Is there a decision/consensus on whether to adhere to the original API?' by a CONTRIBUTOR of type User on 2023-02-11T03:39:46Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 25,
        "reason_for_closure": "Rejected :- PR functionality is deemed unnecessary and potentially harmful."
    },
    {
        "summary": "Pull Request '10682' titled '[jax2tf] Expand gathers and scatters to slices and dynamic slices where possible' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-05-12T19:45:49Z, and was closed at 2024-05-06T10:40:45Z by a User.\nIt has a body of 'When looking into #9659. They'd hit a conversion problem because of a scatter-add in their jaxpr. The root cause comes from handy numpy indexing:\r\nhttps://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py, line 283\r\n```python\r\n x = x[:, 0]\r\n```\r\nThis is traced to a gather. The grad of a gather is a scatter-add. This can be slower than dynamic_slice on some accelerators and has patchy support in both hardware and converters due to the complex syntax. \r\n\r\nI have rewritten some functions into lax.dynamic_slice to get performance boosts before, it'd be better to avoid the far uglier code it generates. I've had similar issues in scatter where to do `x.at[0].set(y)` I would `concat(y, slice(x,1,n))` to avoid poor scatter implementations. I've written an exit path which catches 'single' gathers into the numpy indexing. \r\n\r\nThis can be applied to gathers and scatters in general, is there any simple way to do an optimisation sweep on the jax trace at the end? This would avoid me having to cover all the entry routes into gather/scatter. Though I see that this means the user loses the fine grained control over if they want a gather/scatter instead of dynamic slice or not due to their own reasons.\r\n\r\nNote: \r\nI know that (nearly) all (commonly used) gather/scatters are just loops around: slice array - calculate update - apply update to slice. I would be solving just for loops of length 1. Though it could be handy occassionally to convert short loops as a fix in special cases JAX should not officially support it. Also, I'm only considering unique indices at the moment.\r\n\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax.lax import GatherScatterMode\r\n# simplify jax_pr's\r\ndef wrap(f):\r\n    def _wrapped(*args, **kwargs):\r\n        with jax.ensure_compile_time_eval():\r\n            return f(*args, **kwargs)\r\n    return _wrapped\r\n\r\ndef check(f, x, eval=True):\r\n    wrap_fn = wrap if eval else lambda f: f\r\n    pr = lambda f: jax.make_jaxpr(wrap_fn(f))\r\n    print(f'val\\n{pr(f)(x)}\\ngrad\\n{pr(jax.grad(f))(x)}')\r\n\r\nx = jnp.arange(6, dtype=float).reshape(2,3)\r\n\r\nf = lambda x: x[:,0].max()\r\ncheck(f, x)\r\n# Output (without pull request)\r\nval\r\n{ lambda a:i32[1]; b:f32[2,3]. let\r\n    c:f32[2] = gather[\r\n      dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,))\r\n      fill_value=None\r\n      indices_are_sorted=True\r\n      mode=GatherScatterMode.PROMISE_IN_BOUNDS\r\n      slice_sizes=(2, 1)\r\n      unique_indices=True\r\n    ] b a\r\n    d:f32[] = reduce_max[axes=(0,)] c\r\n  in (d,) }\r\ngrad\r\n{ lambda a:i32[1] b:f32[2,3]; c:f32[2,3]. let\r\n    d:f32[2] = gather[\r\n      dimension_numbers=GatherDimensionNumbers(offset_dims=(0,), collapsed_slice_dims=(1,), start_index_map=(1,))\r\n      fill_value=None\r\n      indices_are_sorted=True\r\n      mode=GatherScatterMode.CLIP\r\n      slice_sizes=(2, 1)\r\n      unique_indices=True\r\n    ] c a\r\n    e:f32[] = reduce_max[axes=(0,)] d\r\n    f:f32[1] = reshape[dimensions=None new_sizes=(1,)] e\r\n    g:bool[2] = eq d f\r\n    h:f32[2] = convert_element_type[new_dtype=float32 weak_type=False] g\r\n    i:f32[] = reduce_sum[axes=(0,)] h\r\n    j:f32[] = div 1.0 i\r\n    k:f32[2] = broadcast_in_dim[broadcast_dimensions=() shape=(2,)] j\r\n    l:f32[2] = mul k h\r\n    m:f32[2] = reduce_sum[axes=()] l\r\n    n:f32[2,3] = scatter-add[\r\n      dimension_numbers=ScatterDimensionNumbers(update_window_dims=(0,), inserted_window_dims=(1,), scatter_dims_to_operand_dims=(1,))\r\n      indices_are_sorted=True\r\n      mode=GatherScatterMode.CLIP\r\n      unique_indices=True\r\n      update_consts=()\r\n      update_jaxpr={ lambda ; o:f32[] p:f32[]. let q:f32[] = add o p in (q,) }\r\n    ] b a m\r\n  in (n,) }\r\n```\r\n\r\nOutput with pull request\r\n```python\r\nval\r\n{ lambda ; a:f32[2,3]. let\r\n    b:f32[2,1] = dynamic_slice[slice_sizes=(2, 1)] a 0 0\r\n    c:f32[2] = squeeze[dimensions=(1,)] b\r\n    d:f32[] = reduce_max[axes=(0,)] c\r\n  in (d,) }\r\ngrad\r\n{ lambda a:f32[2,3]; b:f32[2,3]. let\r\n    c:f32[2,1] = dynamic_slice[slice_sizes=(2, 1)] b 0 0\r\n    d:f32[2] = squeeze[dimensions=(1,)] c\r\n    e:f32[] = reduce_max[axes=(0,)] d\r\n    f:f32[1] = reshape[dimensions=None new_sizes=(1,)] e\r\n    g:bool[2] = eq d f\r\n    h:f32[2] = convert_element_type[new_dtype=float32 weak_type=False] g\r\n    i:f32[] = reduce_sum[axes=(0,)] h\r\n    j:f32[] = div 1.0 i\r\n    k:f32[2] = broadcast_in_dim[broadcast_dimensions=() shape=(2,)] j\r\n    l:f32[2] = mul k h\r\n    m:f32[2] = reduce_sum[axes=()] l\r\n    n:f32[2,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(2, 1)] m\r\n    o:f32[2,3] = dynamic_update_slice a n 0 0\r\n  in (o,) }\r\n```\r\n'\nPR has comments:\n'Related discussion around the inefficiency of differentiating gathers:\r\n- https://github.com/google/jax/issues/6872\r\n\r\nOn the whole I think this sort of change is probably a good idea, given the major potential performance gains. That said, I appreciate the simplicity of lowering indexing to a single XLA operation.\r\n\r\nI have not thought about the relative merits of lowering to `lax.dynamic_slice` vs `lax.slice`.' by a COLLABORATOR of type User on 2022-05-12T21:29:35Z\n'I would need to hit:\r\n- numpy indexing\r\n- .at[] indexing + ops\r\n- autobatching rules (I’ve noted a dynamic slice vmaps to a gather - will show jaxpr later)\r\n\r\nWhile I like the idea of ‘fixing’ later on, I think just hitting these convenience cases like x[0] would get 99% of cases so we get pretty code and optimal performance.  \r\n\r\nI will add a check if concrete array to catch slice opportunities where available over dynamic_slice.\r\n\r\n \r\n' by a CONTRIBUTOR of type User on 2022-05-12T21:53:05Z\n'Could this be done in the MLIR lowering? That would cover the use cases you want, I believe. @hawkinsp ' by a COLLABORATOR of type User on 2022-05-13T00:14:17Z\n'I don't remember for sure but I think there are cases that are hard to turn from general Gathers back to DynamicSlices once they've been lowered from NumPy-style indexing to (M)HLO Gather (and similar for scatters). So +1 to adding something like this optimization! Maybe it wouldn't be hard to handle tuple of ints and slices too?\r\n\r\n' by a COLLABORATOR of type User on 2022-05-13T02:43:11Z\n'> I don't remember for sure but I think there are cases that are hard to turn from general Gathers back to DynamicSlices once they've been lowered from NumPy-style indexing to (M)HLO Gather (and similar for scatters). So +1 to adding something like this optimization! Maybe it wouldn't be hard to handle tuple of ints and slices too?\r\n\r\nCurrently handles\r\nx[:,None,3:5,2]\r\nI think it handles reversing the dim order because the outer of the parser deals with it. I haven’t looked at strides. Slice supports strides, but dynamic slice does not. \r\n\r\nI think x[(0,1)] already gets converted by the handy functions around into x[0,1] by the time the code sees it, cases when someone does x[((0,1),)] however are not.\r\n\r\nThank you for the speedy responses, it really helps to know it’s of interest, I will work on it further.' by a CONTRIBUTOR of type User on 2022-05-13T06:29:30Z\n'The jax2tf lowering issue is the most compelling reason I can see for doing this change. For compilation via XLA, my argument has always been \"XLA can do this\" and we should optimize for simpler Python code. But I don't feel that strongly about it.' by a COLLABORATOR of type User on 2022-05-13T12:47:48Z\n'### Abbreviated view\r\nGather/Scatter may have poor support/poor implementations on some hardware backends/converters e.g. jax2tf no XLA which is commonly used to export to ONNX or tflite. \r\n\r\nLooking at some ways Jax can be used:\r\n| Conversion path | Exit point | Passes applied |\r\n| ------------- | ------------- | ------------- |\r\n| jax2tf  | jaxpr  | No simplifications |\r\n| tflite.experimental_from_jax  | HLO  | gather pass (presumably as its platform independent) |\r\n| Directly used  | optimised HLO  | scatter pass  (as platform dependent) |\r\n\r\nThis PR reimplements the scatter/gather passes at the jaxpr level. This is not a good general idea as in some cases it will backfire and lead to poor performance, but would help jax2tf. As for tflite, tflite.experimental_from_jax should just apply the optimisation pass to simplify scatters (I will add this as a separate issue). \r\n\r\nI think this pull request should only apply to jax2tf (I'll make the necessary code changes) and only when `enable_xla=False` is enabled. This flag suggests the exit point will not be XLA compiled and hence these simplifications improve support and probably performance.' by a CONTRIBUTOR of type User on 2022-05-16T08:10:03Z\n'This is now a companion PR request to #10653. But instead of mapping scatter to tf-scatter ops as the other PR does, this one expands them into slices. \r\n\r\nIt covers a subset of the cases that #10653 does so I'm aiming to land both.\r\n\r\nI've also placed the expanded gather, which may be replicating some of the rules already implemented.' by a CONTRIBUTOR of type User on 2022-05-16T22:13:18Z\n\nPR has review comments:\n'This now has ugly semantics to support both scatter and gather as they share alot in common. Not sure of best way to proceed here.' by a CONTRIBUTOR of type User on 2022-05-14T10:53:30Z\n'Clunky check' by a CONTRIBUTOR of type User on 2022-05-14T10:53:44Z\n'Splitting methods.' by a CONTRIBUTOR of type User on 2022-05-15T09:13:41Z\n'Splitting method removes this in next commits' by a CONTRIBUTOR of type User on 2022-05-15T09:13:55Z\n'Accidental, needs reverting' by a CONTRIBUTOR of type User on 2022-05-17T18:48:08Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 5,
        "reason_for_closure": "Inconsistent behavior change deemed unnecessary and potentially breaking."
    },
    {
        "summary": "Pull Request '10429' titled 'remove useless&expensive core.get_aval in jnp.power' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-04-23T09:56:20Z, and was closed at 2022-07-07T14:32:00Z by a User.\nIt has a body of 'useless: call operator.index on a concrete Tracer will raise TracerIntegerConversionError as well.\r\nIf anyone want to handle concrete Tracer, I can use\r\ntry: x2 = x2.aval.val\r\nexpensive: `core.get_aval(2)` cost 11us.\r\n\r\nImprovement:\r\n`x = jnp.full((2, 2), 2.0)`\r\n`(x ** 2).block_until_ready()` on CPU\r\n80us -> 48us\r\n`(x ** 2.0).block_until_ready()` on CPU\r\n22us -> 8us'\nPR has comments:\n'Close this because the gain seems marginal.' by a CONTRIBUTOR of type User on 2022-07-07T14:32:00Z\n\nPR has review comments:\n'Thanks - I need to check this, but I think this change would have the side-effect of calling `device_get` on any input `x2` that enters this function. The `__index__` method is implemented via `_forward_to_value`, and `x2._value` is a property that returns the numpy version of the array.\r\n\r\nSo the benefit of the original `if` statement is that for non-concrete arrays, the array is not pulled from device in the course of the check.' by a COLLABORATOR of type User on 2022-04-25T19:58:27Z\n'Good catch, I should fix it(at the cost of code complexity can't be reduced). But IIUC the `device_get` call will only happen for a `DeviceArray`: call `__index__` on Tracer will immediately raise an error.' by a CONTRIBUTOR of type User on 2022-04-25T20:05:26Z\n'I think maybe a compromise would be to simply make the condition something like:\r\n\r\n```\r\nif isinstance(x2, int) or isinstance(core.get_aval(x2), core.ConcreteArray):\r\n```\r\n\r\nThat avoids the `get_aval` call in the common case. (We can and should make `get_aval` faster, but that's not today's problem.)' by a COLLABORATOR of type User on 2022-04-25T20:31:31Z\n'After a closer look:\r\nI agree that we should not pull every array from device. \r\nBut I think the original code is not better: any `jnp.ndarray` which is not a `Tracer` will pass the `isinstance(core.get_aval(x2), core.ConcreteArray)` check. And if it is a `Tracer` it won't forward `__index__` to `_value` but raise an error! Is there any non-concrete array type will forward `__index__` to its `_value`?' by a CONTRIBUTOR of type User on 2022-04-27T12:29:04Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Insufficient gain :- PR closed as the performance improvement was marginal and not worth the code complexity."
    },
    {
        "summary": "Pull Request '10426' titled 'Add fast path for `jnp.diagonal(a, offset=0)`' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-04-22T22:04:00Z, and was closed at 2022-05-09T21:02:29Z by a User.\nIt has a body of '#10160 was reverted since it blew up TPU compilation times. So this PR adds a fast path for `jnp.diagonal(a, offset=0)` that prevents unneeded index manipulations. I am actually not sure if it would fix the concrete problem in #10160, but it definitely simplifies the generated Jaxpr and HLO code and also slightly speeds up compilation on my CPU machine.\r\n\r\nOn `main`\r\n```python\r\nfrom jax import numpy as jnp\r\nimport jax\r\n\r\njax.make_jaxpr(jnp.diagonal)(jnp.zeros((3, 3)))\r\n```\r\ngenerates:\r\n```python\r\n{ lambda a:i32[3]; b:f32[3,3]. let\r\n    c:f32[3] = xla_call[\r\n      call_jaxpr={ lambda ; d:i32[3] e:f32[3,3]. let\r\n          f:i32[3] = iota[dimension=0 dtype=int32 shape=(3,)] \r\n          g:bool[3] = lt f 0\r\n          h:i32[3] = add f 3\r\n          i:i32[3] = select_n g f h\r\n          j:bool[3] = lt d 0\r\n          k:i32[3] = add d 3\r\n          l:i32[3] = select_n j d k\r\n          m:i32[3,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(3, 1)] i\r\n          n:i32[3,1] = broadcast_in_dim[broadcast_dimensions=(0,) shape=(3, 1)] l\r\n          o:i32[3,2] = concatenate[dimension=1] m n\r\n          p:f32[3] = gather[\r\n            dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0, 1), start_index_map=(0, 1))\r\n            fill_value=None\r\n            indices_are_sorted=False\r\n            mode=GatherScatterMode.PROMISE_IN_BOUNDS\r\n            slice_sizes=(1, 1)\r\n            unique_indices=False\r\n          ] e o\r\n        in (p,) }\r\n      name=diagonal\r\n    ] a b\r\n  in (c,) }\r\n```\r\nWith this PR, the index manipulations (including the concatenation which currently isn't folded away by hlo) are replaced by a single call to `iota`:\r\n```python\r\n{ lambda ; a:f32[3,3]. let\r\n    b:f32[3] = xla_call[\r\n      call_jaxpr={ lambda ; c:f32[3,3]. let\r\n          d:i32[3,2] = iota[dimension=0 dtype=int32 shape=(3, 2)] \r\n          e:f32[3] = gather[\r\n            dimension_numbers=GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0, 1), start_index_map=(0, 1))\r\n            fill_value=None\r\n            indices_are_sorted=False\r\n            mode=GatherScatterMode.PROMISE_IN_BOUNDS\r\n            slice_sizes=(1, 1)\r\n            unique_indices=True\r\n          ] c d\r\n        in (e,) }\r\n      name=diagonal\r\n    ] a\r\n  in (b,) }\r\n```\r\n\r\nAfter implementing this it turned out to be much less readable, so I am not sure if a cleaner Jaxpr is worth the added complexity in this case. I guess you are in a much better position to judge this which is why I am posting the PR anyway.'\nPR has comments:\n'Hi! - I find that we can use Cloud TPUs for free: https://sites.research.google/trc/about/' by a CONTRIBUTOR of type User on 2022-04-23T10:00:26Z\n'What do you think?' by a COLLABORATOR of type User on 2022-04-25T20:32:46Z\n\nPR has review comments:\n'I think I'm going to say \"let's leave this the way it was, absent a compelling reason to change it\". We can teach XLA how to optimize the HLO better, should it prove worthwhile.\r\n\r\nHowever the fewer direct calls to `lax.gather` in the world the better my life will be... And overall I think I'd rather optimize for simpler *Python* code if the jaxpr isn't that different.\r\n' by a COLLABORATOR of type User on 2022-04-25T20:26:51Z\n'I agree, the added complexity is probably not worth it in this case. Currently the only real benefit is that it slightly speeds up compilation time (on CPU by around 20%) and removes the `concatenate` which XLA can't optimise away.' by a CONTRIBUTOR of type User on 2022-05-09T21:02:25Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Inconclusive/Withdrawn :- PR abandoned as complex inputs shouldn't raise exceptions, and author offered to add docstrings."
    },
    {
        "summary": "Pull Request '10398' titled 'replace int with operator.index for lax' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-04-21T06:04:54Z, and was closed at 2023-11-02T22:08:58Z by a User.\nIt has a body of 'Note caller of _eye has ensured int.\r\nChange with `jax.numpy.tri` is included in this PR for potential reverting'\nPR has comments:\n'Closing since this has bit-rotted – feel free to re-open if you wish to continue working on this!' by a COLLABORATOR of type User on 2023-11-02T22:08:59Z\n\nPR has review comments:\n'Here and elsewhere: we shouldn't assert that inputs are of type int, because (for example), this fails for numpy integer scalars.' by a COLLABORATOR of type User on 2022-04-21T13:00:13Z\n'Why is this change necessary? Is there any situation in which an element of a shape is not an integer?' by a COLLABORATOR of type User on 2022-04-21T13:01:23Z\n'I don't think the meaning or intent of this warning will be clear to users who see it. How about:\r\n```\r\nwarnings.warn(f\"jnp.tri: k must be an integer. In future JAX versions non-integer values will result in an error.\")\r\n```' by a COLLABORATOR of type User on 2022-04-21T13:06:54Z\n'We should offer the user some clue here about the provenance of the error: imagine running a long & involved script and seeing several copies of this error, reported as coming from line 126 of `jax/_src/lax/utils.py`. What would you do to fix it?\r\n\r\nPerhaps `_int_replaced_by_index` could accept the function name & variable name as strings, and put these in the warning to give context.\r\n\r\nAlternatively, this may be an argument to inline this check in each place we use it, so that the warning is reported in the relevant context.' by a COLLABORATOR of type User on 2022-04-21T13:09:46Z\n'The intent here is to check whether `lower` and `upper` are tracers. We should not raise deprecation warnings here, and we probably should not change this to `operator.index` in the long run. The side effect here is that non-traced floating point array inputs would be switched from using the `scan` implementation to using the `while_loop`.implementation, which is not what we want.' by a COLLABORATOR of type User on 2022-04-21T13:13:09Z\n'I don't think this test change is necessary once you address the comments within the `fori_loop` implementation.' by a COLLABORATOR of type User on 2022-04-21T13:13:38Z\n'I add a to `int` conversion in the only caller of `_tri` -- `jax.numpy.tri`, to be consistent with  `jax.numpy.eye`, which has used `operator.index` before this series of PRs.' by a CONTRIBUTOR of type User on 2022-04-21T13:36:22Z\n'Maybe in some cases like poly shape?\r\nCould I just remove the `int` conversion?' by a CONTRIBUTOR of type User on 2022-04-21T13:39:08Z\n'Thanks! I am not familiar with how to write a clear warning.' by a CONTRIBUTOR of type User on 2022-04-21T13:40:10Z\n'Good catch! This is my mistake!' by a CONTRIBUTOR of type User on 2022-04-21T13:41:52Z\n'Let me put it another way: what problem does this change fix, which makes it worth the small but non-zero potential of introducing unintended consequences?\r\n\r\nOne known unknown for me: if the shape contains poly dimensions, does switching from `int` to `operator.index` change the kind of error that is raised in a way that might impact a user who depends on the current implementation?\r\n\r\nEven scarier are the unknown unknowns.\r\n\r\nIf it's not broken, we shouldn't fix it.' by a COLLABORATOR of type User on 2022-04-21T13:42:38Z\n'> The side effect here is that non-traced floating point array inputs would be switched from using the scan implementation to using the while_loop.implementation, which is not what we want.\r\n\r\nWhat about using concrete check here?' by a CONTRIBUTOR of type User on 2022-04-21T13:44:33Z\n'Thanks! I will follow your suggestion `warnings.warn(f\"jnp.tri: k must be an integer. In future JAX versions non-integer values will result in an error.\")` and accept function&variable name.' by a CONTRIBUTOR of type User on 2022-04-21T13:56:50Z\n'Got it. I will revert this change.' by a CONTRIBUTOR of type User on 2022-04-21T14:13:32Z\n'Ah, after read the code, IIUC, `Tracer` with `concreteArray` `aval` is not allowed to use `scan`?' by a CONTRIBUTOR of type User on 2022-04-21T14:26:21Z\n'@jakevdp  Could you please give some advice?\r\nDoes `fori_loop` not allow `Tracer` with `ConcreteArray` type `aval`?' by a CONTRIBUTOR of type User on 2022-04-21T14:54:18Z\n'I believe that `int(x)` will return the concrete value if `x` is a tracer with a concrete aval.' by a COLLABORATOR of type User on 2022-04-21T17:01:02Z\n'Again, as far as I can tell this code works correctly and does exactly what we want it to do, so unless you have found a bug somewhere I'd say it's not broken, and there's no need to change it.' by a COLLABORATOR of type User on 2022-04-21T17:01:57Z\n'> Again, as far as I can tell this code works correctly and does exactly what we want it to do, so unless you have found a bug somewhere I'd say it's not broken, and there's no need to change it.\r\n\r\nSorry to be dense about this point, I have little experience in working for a real project. -- BTW, I am curious about how to get a tracer with a concrete aval.' by a CONTRIBUTOR of type User on 2022-04-21T17:07:17Z\n'It will mainly come in via autodiff. For example:\r\n```python\r\nfrom jax import grad\r\n\r\ndef f(x):\r\n  print(x.aval)\r\n  return x\r\n\r\n_ = grad(f)(1.0)\r\n# ConcreteArray(1.0, dtype=float32, weak_type=True)\r\n```' by a COLLABORATOR of type User on 2022-04-21T17:14:35Z\n'Thanks for nice example! -- And I find Tracer define appropriate `__int__` method.' by a CONTRIBUTOR of type User on 2022-04-21T17:30:01Z\n'There's little or no downside to repeating `offset = operator.index(offset)` in place of an `isinstance` assertion, even if the only (current) call-site already converts the input to an integer; the upside is that if this low-level function is ever used elsewhere, it will be better behaved.' by a COLLABORATOR of type User on 2022-05-13T21:39:44Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 22,
        "reason_for_closure": "PR '10398' closed as unnecessary: the proposed change was deemed not worth the risk of unintended consequences, and the existing code functioned correctly."
    },
    {
        "summary": "Pull Request '10322' titled 'add annotation for ndarray.at and other functions.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-04-16T12:12:22Z, and was closed at 2023-11-03T22:12:57Z by a User.\nIt has a body of 'see discussion in #10165'\nPR has comments:\n'Maybe we can remove the underscore of `_IndexUpdateHelper` and `_IndexUpdateRef`?\r\nAnd its docs need to update.' by a CONTRIBUTOR of type User on 2022-04-16T13:23:16Z\n'Nice semantic highlight&auto-completion without manually type annotation\r\n![image](https://user-images.githubusercontent.com/83971976/163681871-ebd36297-eb7c-42f5-bde8-83f8f9e8b1e3.png)\r\n' by a CONTRIBUTOR of type User on 2022-04-16T15:52:30Z\n'After replace `Any` with `object`, the intention is clear: we provide return-type guarantee with `ndarray` arguments(which may need some modifications to ensure user can not inherit `ndarray` and exploit the special dispatch. --- though I think we don't need to consider the attack/abuse by user.), while leave others case typed `object`(NOT `Any`), which is not compatible with `ndarray` , and the error can be detected during static type checking. WDYT @jakevdp ?' by a CONTRIBUTOR of type User on 2022-04-18T21:13:25Z\n'But the downside of `object` is that if user use type checker and write something like `tile(x, 2) + 1`, where `x` is not a `ndarray`, then type checker will complain that `object` not support `+`. BUT I think this is intentioned, for example `PRNGKeyArray` doesn't support arithmetic. If I add `np.array` annotation, the only false postive is `__jax_array__`. (not sure it really works with `tile`)' by a CONTRIBUTOR of type User on 2022-04-18T21:45:58Z\n'So I'm still kind of hesitant on this. It seems like it's opening the door to add a lot of extra code (two overloads for every user-facing API function) with very little testing. I know in theory these annotations should be covered by running mypy on our source files, and/or perhaps our test suite, but my main experience with mypy in JAX is having to add `# type: ignore` in seemingly random places across the package in order to fix new false positives that arise  every time we update the mypy version used by our CI. It doesn't give me much confidence in the test coverage provided by mypy's static type checking.\r\n\r\nSo what this PR looks like to me is a proposal to eventually add hundreds of lines of untested type annotations to the API in a manner that makes it very likely the annotations will silently become out-of-date at some point in the future. It's not obvious to me that the benefits here outweigh the increased maintenance burden.' by a COLLABORATOR of type User on 2022-04-20T16:29:18Z\n'Okay, I can remove the \"overload for type annotation\" part in this PR because of the maintenance concern. What do you think about the rest of this PR? At least it will be very useful to have return-type annotation for `zeros`, `ones` and arithmetic operator on `ndarray`.\r\nNevertheless, Pytorch heavily uses stubs(`pyi`) for type annotation and has won lots of endorsement from users, which is almost the same as I do. The only difference is that they place the type annotation in a separate file.' by a CONTRIBUTOR of type User on 2022-04-20T16:57:55Z\n'If another core maintainer feels differently and is willing to take ownership of this, I'd be fine with that!' by a COLLABORATOR of type User on 2022-04-20T17:16:47Z\n'Hi - thanks so much for sticking with a tough review process here – your comments & contribution here have been really helpful for me to learn more about some of the potential benefits of type annotations.\r\n\r\nWe've been having some offline conversation among the core team spurred by this, by #10321, and by other similar requests/contributions in the past. I think before we make any decision on this kind of thing we need to step back and do a more comprehensive look at the benefits & tradeoffs surrounding type annotations. With that in mind I think it would be best to table this contribution for now, until we can come up with a better roadmap for annotations within JAX as a whole.\r\n\r\nHow does that sound?' by a COLLABORATOR of type User on 2022-04-20T19:21:46Z\n'Thanks for your nice review&insight. I'd like to wait for a comprehensive roadmap.\r\nNevertheless, I'd like to create a new PR, which only adds annotation for `ndarray.at` while advoid cyclic import. Do you think such PR can be merged without waiting for the roadmap?' by a CONTRIBUTOR of type User on 2022-04-20T19:37:44Z\n'I would hold off on that – I think it will be easier to review once we've thought more about the broader question.' by a COLLABORATOR of type User on 2022-04-20T20:00:17Z\n'Hi - friendly ping. Is there anything new about the roadmap?' by a CONTRIBUTOR of type User on 2022-05-27T17:51:04Z\n'I think this is all done now via work related to https://jax.readthedocs.io/en/latest/jep/12049-type-annotations.html. Thanks for getting the conversation started here!' by a COLLABORATOR of type User on 2023-11-03T22:12:57Z\n\nPR has review comments:\n'What's the benefit of having a strictly-typed version of this overloaded by a declaration that has no information?\r\n\r\nGiven the issues mypy has with correctly interpreting annotations of JAX code (e.g. #10321), I'm hesitant to add the maintenance burden of these types multiply-overloaded function annotations, particularly if this is meant to be exploratory work toward the goal of doing this treatment to the full jax.numpy API.\r\n\r\nThese overloadings look to me like a lot of untested/poorly-tested code that will easily become out of sync with the main implementations, and whose benefits may not outweight the costs of maintenance. For example, the `type: ignore` statements required in this PR make clear the extent of the false positives introduced by these overloadings.\r\n\r\nGiven that, I'd lean toward not including these annotations in JAX. What do you think?' by a COLLABORATOR of type User on 2022-04-18T16:23:02Z\n'Maybe another more concrete question, since I've not used typing extensively before: is there any best practice for comprehensively unit-testing these sorts of type annotations? Every existing line of code in this file is covered by extensive unit tests in `lax_numpy_test.py`. Is there any way to provide that same level of test coverage for these non-trivial overloaded annotations, so that, for example, if the return type of the implementation is changed, we are alerted by the test suite that the annotations have become out of sync?' by a COLLABORATOR of type User on 2022-04-18T16:26:36Z\n'I have to explain, the `type: ignore` required in this PR has nothing to do with these overloading. IIUC, the reason that original code doesn't require `type: ignore` is that `mypy` won't check type inside a function without any annotation, i.e. the problem have existed before this PR, but `mypy` doesn't check this function(`jax.numpy.array`).' by a CONTRIBUTOR of type User on 2022-04-18T16:50:34Z\n'Actually it isn't a real \"multiple overload\", it actually a return type annotation which  must be satisfied when argument type meet some condition, and a catch-all default for corner case.\r\nIIUC, mypy will try to match overloads in their order of declaration. So if user call `tile` with `ndarray`, `mypy` should infer that the return type must be `ndarray`.\r\nClearly, type annotation in this manner doesn't need a test, it is a part of API interface definition and constraints on implementation. Nobody will expect that calling `tile` on a `ndarray` will get an object isn't an instance of `ndarray`.\r\nIf you want to test if the implementation meet this constraint, then this is what type checking do. (But it seems that `mypy` won't check overloads, I will take a look.)' by a CONTRIBUTOR of type User on 2022-04-18T17:07:13Z\n'Is there something we could change about `ndarray` that would make it so we wouldn't have to overload an annotation that returns `Any`? What will break if we leave out the version with `Any` as a return type?' by a COLLABORATOR of type User on 2022-04-18T17:17:17Z\n'Ah, I intuitive think that `tile` will always return `ndarray`. But I am not sure, thus I add an `Any` version.' by a CONTRIBUTOR of type User on 2022-04-18T17:23:06Z\n'Annotate overload while leave implementation untyped is a common pattern to intentionally let type checker not to check the implementation\r\n> `mypy`: By default the bodies of functions without annotations are not type checked.\r\n\r\nanother way to do so is providing separate stub files, which used by `pytorch` for c/cpp implemented functions, but can also be applied to python implemented functions.' by a CONTRIBUTOR of type User on 2022-04-18T17:40:08Z\n'Well, to be precise, `tile` will return whatever `broadcast_to` returns. And because of [this line](https://github.com/google/jax/blob/a48752a5786e5debda219cfaccb8aa694266e6d5/jax/_src/numpy/util.py#L343-L344) `broadcast_to` can return anything so long as the input object has a `broadcast_to` method. So I think the best we can do for `tile` is to annotate it as returning `Any`.' by a COLLABORATOR of type User on 2022-04-18T17:44:12Z\n'Of course, this may all change, and it might not be clear to the person changing the behavior of `util._broadcast_to` that the changes to that function require synchronous changes in the overloaded type annotations of `jnp.tile`; thus my question about unit testing.\r\n\r\nTo be concrete, suppose this PR had been done a few months ago, before the custom PRNG work required modifying the range of types returned by `broadcast_to`, and we had (correctly, at that time) annotated `jnp.tile` with `ndarray`, without any fallback to `Any`. What test would have told us that our annotations are now wrong? Or would we not have caught that until we made a release with the `_broadcast_to` changes and gotten bug reports from users? The latter is something we should strive to avoid.' by a COLLABORATOR of type User on 2022-04-18T17:45:18Z\n'Good catch. The `Any` argument `tile` return type should be `Any`.\r\nIf you want to prevent any future error, I suggest we can always have a catch-all fallback to `Any`.\r\nFor \"What test would have told us that our annotations are now wrong? Or would we not have caught that until we released the package and gotten bug reports from users?\". IMO, type annotation in python can be viewed as documentation, and doesn't have any runtime effect. Thus, if there is something wrong with type annotation, it won't block any user, but only make them confused, which is the same as the situation when there is something wrong with documentation. Do you think we should have unit-testing for documentation?\r\nNevertheless, if the codebase of JAX is fully typed, we can add annotation on the implementation, and then type-checker can tell us that annotations are now wrong.\r\nToy example:\r\n```python\r\ndef f(x: ndarray) -> ndarray: ... # lax function, doesn't change in the considered PR\r\n# original version of g\r\ndef g(x: Any) -> ndarray:\r\n  assert is_ndarray(x) # type guard\r\n  return f(x)\r\n# new version of g\r\ndef g(x: Any) -> ndarray:\r\n   assert is_ndarray(x) or is_stackable(x)\r\n   return f(x)\r\n```\r\nThen `mypy` will report that there is something wrong.\r\nHowever, currently we don't have `f` typed, thus we need a catch-all fallback to `Any` to ensure safety.\r\n(And `_check_arraylike` is not a `TypeGuard` now)' by a CONTRIBUTOR of type User on 2022-04-18T18:02:02Z\n'> Do you think we should have unit-testing for documentation?\r\n\r\nWe do run doctests on most code samples in the docs, but it's true that documentation prose cannot be tested. The question is: does the benefit of having untested docs outweight the burden of maintaining those docs? Arguably the answer is yes.\r\n\r\nBack to type annotations: does the benefit of having incomplete type annotations with a fundamentally flawed type checking system (`mypy` - referring again to issues like those in https://github.com/google/jax/pull/10321) outweight the burden of maintaining those type annotations? It's not clear to me that the answer is yes, and particularly the more complicated those type annotations must be (protocols, overloads, etc.) I think the weight goes toward \"no\".' by a COLLABORATOR of type User on 2022-04-18T18:11:49Z\n'Actually there are type annotations in documentation, though they are docstring from numpy.\r\n![image](https://user-images.githubusercontent.com/83971976/163854970-b77aa54b-dbbc-42f4-96ae-255ee0daf72e.png)\r\n\r\nAnd again, if we only annotate the trivial case and always let others fallback to `Any`, I don't think we will really have the burden of maintaining those type annotation: Given that `jax.numpy` will be compatible(to some extent) with `numpy`, I can't imagine that the return-type is not `ndarray` when arg-type is.' by a CONTRIBUTOR of type User on 2022-04-18T18:25:03Z\n'OK, so the idea now is that because return types are not well tested, and refactoring such as `broadcast_to` may change things, we need every function we annotate to have multiple overloads, one being the common case of returning an ndarray, and the other being a generic fallback where every input and output is annotated as `Any`. Is that correct?\r\n\r\nIf so, I really don't like that. If the best we can do is say is that our functions accept `Any` and return `Any`, why not simplify things and avoid any type annotations at all?\r\n\r\nAlternatively, this may suggest that the real goal is not static type checking (having a generic `Any` fallback for all functions can never catch errors in your code), but rather documentation. If that's the case, why not continue using `Array = Any`, which serves the documentation purpose very well without having to add gratuitous overloads throughout the codebase?' by a COLLABORATOR of type User on 2022-04-18T18:34:42Z\n'Ah, If I replace return-type `Any` with `object`(I misunderstanding `Any` before), it can catch error in some case.\r\n```python\r\nfrom typing import Any, overload\r\n\r\n@overload\r\ndef f(x: int) -> int:...\r\n@overload\r\ndef f(x: Any) -> object:...\r\ndef f(x):\r\n    if type(x) is int:\r\n        return x + 1\r\n    else:\r\n        return 'a'\r\n\r\nx: int = f(1) # pass\r\ny: int = f(None) # error: Incompatible types in assignment (expression has type \"object\", variable has type \"int\")\r\nz: Any = f(None) # pass\r\nt = f(None) # pass\r\n```\r\nIt means that, if user restrict the output type, but the argument to `f` fallback to the `Any` to `object` case, then they will get an error!' by a CONTRIBUTOR of type User on 2022-04-18T18:45:15Z\n'In addition, type annotation can help IDE to do auto-completion and semantic highlighting. If the return-type is annotated with `ndarray`, then IDE can do auto-completion for properties and methods of return value.\r\n```python\r\njnp.tile(x, 2).reshape(...)\r\n# I only need to type \"re\", and don't need to worry about typo\r\n# if there is a typo, then semantic highlighting won't highlight the wrong method name\r\n# thus I can fix it without run the code\r\njnp.tile(x, 2).rehsape(...) # won't highlight\r\n# BUT if tile has no type annotation, everything won't be highlight!\r\n```' by a CONTRIBUTOR of type User on 2022-04-18T18:59:46Z\n'I think the type annotation will be useful if I replace `Any` with `object`! WDYT?' by a CONTRIBUTOR of type User on 2022-04-18T19:06:40Z\n'Thanks for clarifying - so it seems your main motivation here is not necessarily static type checking or documentation, but rather giving auto-complete tools more information.\r\n\r\nI'll bring it back to my previous question: does the benefit of this outweigh the maintenance burden? Testing is still important (potentially incorrect autocomplete is worse than no autocomplete, in my opinion). What IDE tooling is required to take advantage of this? Can we get any sense of how much of our user-base is likely to benefit? And are there downsides to annotations motivated by autocomplete? e.g. if we tune our annotations to that application, does it have any negative impact on the static type checking use-case?\r\n\r\nI think these issues need to be explored before we start adding significant numbers of non-trivial type annotations to the codebase.' by a COLLABORATOR of type User on 2022-04-18T19:59:57Z\n'Not only auto-complete, but also semantic highlight, which works similarly to type checker when `Any` replaced by `object`. i.e. if I expected the return typed is `ndarray`, but it fallbacks to `object`(`Any` works the same as `object` in semantic highlighting), I can immediately find out this error.\r\nAgain, I can replace `Any` with `object`, so that it can help static type checking as well.' by a CONTRIBUTOR of type User on 2022-04-18T20:11:25Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 12,
        "num_review_comments": 18,
        "reason_for_closure": "Inconsistent behavior deprecated: JAX superset of NumPy; complex inputs supported."
    },
    {
        "summary": "Pull Request '10274' titled 'Make `safe_zip` and `safe_map` work with empty lists. ' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-04-14T01:06:12Z, and was closed at 2022-06-29T17:59:43Z N/A.\nIt has a body of 'Helps avoid error when calling e.g. `lax.reduce_window` on scalar inputs.'\nPR has comments:\n'Looks like the change causes some failures though, will look into it.' by a CONTRIBUTOR of type User on 2022-04-14T22:50:53Z\n\nPR has review comments:\n'Maybe just directly write `[((), (), (), (), (), ())]` for clarity?' by a COLLABORATOR of type User on 2022-04-14T20:42:48Z\n'Is this change necessary for this PR? It doesn't look like `safe_map` with empty args is used anywhere. Do we need to support that?' by a COLLABORATOR of type User on 2022-04-14T20:44:44Z\n'A benefit of the previous behavior was that `safe_zip` errored with empty arguments, which is sometimes an indication of something unexpected happening. What if we handle the empty padding case above explicitly, without modifying `safe_zip`?' by a COLLABORATOR of type User on 2022-04-14T20:45:40Z\n'Good point! I added `VALID` and `SAME` padding values though, to test that these work on empty inputs too though, so kept the product structure.' by a CONTRIBUTOR of type User on 2022-04-14T22:42:06Z\n'It's not, happy to revert it, but I thought it made sense to have it behave consistently with `safe_zip` (and the vanilla python `map`) - lmk if you still would like me to revert it.' by a CONTRIBUTOR of type User on 2022-04-14T22:43:30Z\n'I personally think that the new behavior would be a bit better in that it would be consistent with the behavior of `map` and `zip`, as well as logically correct (an empty set satisfies any condition), so I think it would be better if empty arguments, if they are unexpected, would cause errors elsewhere, but not inside these methods. But I'm not that familiar with JAX codebase, so if you still prefer these to fail on empty entires, happy to revert the changes to both `safe_map` and `safe_zip`, let me know!' by a CONTRIBUTOR of type User on 2022-04-14T22:47:48Z\n'Looping in @mattjj because I think he has opinions...' by a COLLABORATOR of type User on 2022-04-14T22:53:00Z\n'My worry here is that now padding of arbitrary length is accepted. Correct me if I'm wrong, but we only want to accept empty paddings, or paddings of length 2, correct?' by a COLLABORATOR of type User on 2022-04-14T22:53:51Z\n'Actually, it seems like the best fix here is not to make `zip` behave differently, but to replace `zip(*padding)` with `pads_lo, pads_hi = util.unzip2(padding)`, which I think will handle all the corner cases exactly as we need it to.' by a COLLABORATOR of type User on 2022-04-14T22:57:01Z\n'Thanks Jake, this is indeed better, changed!\r\n\r\nHowever, I noticed that having `safe_zip` to work with 0-length inputs is also used in the transpose rule of `reduce_window_sum`:\r\nhttps://github.com/google/jax/blob/f0112ccda5afe5114855d67845db60fb14907de9/jax/_src/lax/windowed_reductions.py#L365\r\n\r\nSo I kept the `safe_map` and `safe_zip` changes for now.\r\n\r\nI then added the identical test case to the respective autodiff test https://github.com/google/jax/pull/10274/files#diff-22c1f2ce24cf39ea41d2fbba031630102a443aa243435c054e0cbafb6b8183ee\r\n\r\nBut now I am getting a rather mysterious `fatal Python error`:\r\nhttps://source.cloud.google.com/results/invocations/768551e1-a1dc-4f3e-9f3f-a35649a3fa0d/targets/jax%2Ftesting%2Finternal_numpy_scipy%2Fpresubmit_github/log\r\nDo you have an idea of what might be causing it?' by a CONTRIBUTOR of type User on 2022-04-16T05:09:54Z\n'I think @mattjj had opinions on this - I'm going to request a review from him' by a COLLABORATOR of type User on 2022-04-18T16:09:58Z\n'@mattjj do you have ideas how to fix these errors?\r\n\r\nThe goal is to make calling `lax.reduce_window` and the respective forward/reverse-mode AD on scalars work, e.g.\r\n```python\r\ndef f(x):\r\n  return jax.lax.reduce_window(x, 0., jax.lax.add, (), (), ())\r\n\r\nf(1.)\r\ngrad(f)(1.)\r\njvp(f (1.,), (1.,))\r\n```\r\n\r\nBut my suggested fix seems to lead to a `fatal Python error` (https://source.cloud.google.com/results/invocations/605f2ece-dadd-4073-a048-6f366be90a3f/targets/jax%2Ftesting%2Fcpu%2Fpresubmit_github/log) - any suggestions on how to figure out this error, or fix the issue in a different way?' by a CONTRIBUTOR of type User on 2022-05-12T22:15:01Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 12,
        "reason_for_closure": "Inconsistent behavior :- PR tried to align jax with numpy but jax supersedes numpy's functionality."
    },
    {
        "summary": "Pull Request '10228' titled 'simplify jax.numpy.tile' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-04-11T14:56:23Z, and was closed at 2022-07-07T14:47:09Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'simplify python execution, Jaxpr and HLO\r\npython execution: original code call a complex function _broadcast_to\r\n```python\r\nimport jax\r\nimport jax.numpy as jnp\r\nfrom jax._src.numpy.lax_numpy import new_tile # proposed implementation\r\n\r\nfrom timeit import timeit\r\n\r\ndef f(x):\r\n    return new_tile(x, 2)\r\n\r\ndef g(x):\r\n    return jnp.tile(x, 2)\r\n\r\nx = jnp.ones((2,) * 4)\r\nprint(timeit('jax.make_jaxpr(f)(x)', globals=globals(), number=1000)) # 0.59s\r\nprint(timeit('jax.make_jaxpr(g)(x)', globals=globals(), number=1000)) # 0.98s\r\n```\r\n\r\nJaxpr: broadcast_to(reshape + squeeze + broadcast_in_dim) + reshape -> broadcast_in_dim + reshape\r\nHLO: avoid useless broadcast\r\n```python\r\njnp.tile(jnp.ones((2,) * 4), 2)\r\n```\r\nBefore:\r\n```\r\n{ lambda ; a:f32[2,2,2,2]. let\r\n    b:f32[1,2,1,2,1,2,1,2] = reshape[\r\n      dimensions=None\r\n      new_sizes=(1, 2, 1, 2, 1, 2, 1, 2)\r\n    ] a\r\n    c:f32[1,2,1,2,1,2,2] = squeeze[dimensions=(6,)] b\r\n    d:f32[1,2,1,2,1,2,2,2] = broadcast_in_dim[\r\n      broadcast_dimensions=(0, 1, 2, 3, 4, 5, 7)\r\n      shape=(1, 2, 1, 2, 1, 2, 2, 2)\r\n    ] c\r\n    e:f32[2,2,2,4] = reshape[dimensions=None new_sizes=(2, 2, 2, 4)] d\r\n  in (e,) }\r\n```\r\n```\r\n%fused_computation (param_0.1: f32[2,2,2,2]) -> f32[2,2,2,4] {\r\n  %param_0.1 = f32[2,2,2,2]{3,2,1,0} parameter(0)\r\n  %broadcast.1 = f32[1,2,1,2,1,2,2,2]{7,6,5,3,1,4,2,0} broadcast(f32[2,2,2,2]{3,2,1,0} %param_0.1), dimensions={1,3,5,7}, metadata={op_name=\"jit(g)/jit(main)/broadcast_in_dim[shape=(1, 2, 1, 2, 1, 2, 2, 2) broadcast_dimensions=(0, 1, 2, 3, 4, 5, 7)]\"}\r\n  ROOT %bitcast.1 = f32[2,2,2,4]{3,2,1,0} bitcast(f32[1,2,1,2,1,2,2,2]{7,6,5,3,1,4,2,0} %broadcast.1), metadata={op_name=\"jit(g)/jit(main)/reshape[new_sizes=(2, 2, 2, 4) dimensions=None]\"}\r\n}\r\n```\r\nAfter:\r\n```\r\n{ lambda ; a:f32[2,2,2,2]. let\r\n    b:f32[2,2,2,2,2] = broadcast_in_dim[\r\n      broadcast_dimensions=(0, 1, 2, 4)\r\n      shape=(2, 2, 2, 2, 2)\r\n    ] a\r\n    c:f32[2,2,2,4] = reshape[dimensions=None new_sizes=(2, 2, 2, 4)] b\r\n  in (c,) }\r\n```\r\n```\r\n%fused_computation (param_0.1: f32[2,2,2,2]) -> f32[2,2,2,4] {\r\n  %param_0.1 = f32[2,2,2,2]{3,2,1,0} parameter(0)\r\n  %broadcast.0 = f32[2,2,2,2,2]{4,3,2,1,0} broadcast(f32[2,2,2,2]{3,2,1,0} %param_0.1), dimensions={0,1,2,4}, metadata={op_name=\"jit(f)/jit(main)/broadcast_in_dim[shape=(2, 2, 2, 2, 2) broadcast_dimensions=(0, 1, 2, 4)]\"}\r\n  ROOT %bitcast.1 = f32[2,2,2,4]{3,2,1,0} bitcast(f32[2,2,2,2,2]{4,3,2,1,0} %broadcast.0), metadata={op_name=\"jit(f)/jit(main)/reshape[new_sizes=(2, 2, 2, 4) dimensions=None]\"}\r\n}\r\n```\r\n'\nPR has comments:\n'Thanks! I don't doubt this is correct, but it's definitely not obvious from reading the code.\r\n\r\nJust to be sure we're covering all corners, can you run locally a more complete set of tests on this?\r\n```\r\n$ JAX_NUM_GENERATED_CASES=500 pytest -n auto tests/lax_numpy_test.py -k testTile\r\n```\r\nThanks!' by a COLLABORATOR of type User on 2022-04-11T17:53:32Z\n'Sure, I'd like to run a more complete set of tests tomorrow. (It is 2a.m. in my timezone😴)' by a CONTRIBUTOR of type User on 2022-04-11T18:50:04Z\n'I explicitly fixed the scalar case: (to prevent mistake in future)\r\n```python\r\nmerged_rs = [(r * s,) if r == 1 or s == 1 else (r, s) for r, s in zip(reps, A_shape)]\r\ntarget_shape = tuple(e for t in merged_rs for e in t) # chain\r\ndims = tuple(np.cumsum([len(t) for t in merged_rs]) - 1)[-ndim(A):] if ndim(A) else ()\r\nresult = lax.broadcast_in_dim(A, target_shape, dims)\r\n```' by a CONTRIBUTOR of type User on 2022-04-12T02:45:35Z\n'@jakevdp I don't know what does the error message mean, could you please take a look and give some advice?\r\n```\r\nJAX_NUM_GENERATED_CASES=500 pytest -n auto tests/lax_numpy_test.py -k testTile\r\n```\r\n```\r\n=============================================================================== test session starts ================================================================================\r\nplatform linux -- Python 3.9.7, pytest-7.1.1, pluggy-1.0.0\r\nbenchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\nrootdir: /home/jiacheng/jax, configfile: pytest.ini\r\nplugins: benchmark-3.4.1, xdist-2.5.0, forked-1.4.0\r\ngw0 I / gw1 I / gw2 I / gw3 I / gw4 I / gw5 I / gw6 I / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 I / gw2 I / gw3 I / gw4 I / gw5 I / gw6 I / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 I / gw3 I / gw4 I / gw5 I / gw6 I / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 I / gw4 I / gw5 I / gw6 I / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 I / gw5 I / gw6 I / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 I / gw6 I / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 I / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 I / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 I / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 I / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 I / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 I / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 I / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 I / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 I / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 I / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 I / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 I / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 I / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 I / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 I / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 C / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 C / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 C / ggw0 C / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 C / ggw0 ok / gw1 C / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 C / gw0 ok / gw1 ok / gw2 C / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 C /gw0 ok / gw1 ok / gw2 ok / gw3 C / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 C gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 C / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 Cgw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 C / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20 gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 C / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw20gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 C / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw2gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 C / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gwgw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 C / gw10 C / gw11 C / gw12 C / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / ggw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 C / gw10 C / gw11 C / gw12 ok / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C / gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 C / gw10 ok / gw11 C / gw12 ok / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C /gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 C / gw12 ok / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 C gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 C / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 Cgw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 C / gw15 C / gw16 C / gw17 C / gw18 C / gw19 gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 C / gw16 C / gw17 C / gw18 C / gw19gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 C / gw17 C / gw18 C / gw1gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 C / gw18 C / gwgw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 C / ggw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok / gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok / gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok / gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok / gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok / gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 ok / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok / gw0 ok / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 [0] / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok /gw0 [0] / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 [0] / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 ok / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ok gw0 [0] / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 [0] / gw6 ok / gw7 ok / gw8 ok / gw9 ok / gw10 ok / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 okgw0 [0] / gw1 ok / gw2 ok / gw3 ok / gw4 ok / gw5 [0] / gw6 ok / gw7 ok / gw8 [0] / gw9 ok / gw10 ok / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 ogw0 [0] / gw1 ok / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 ok / gw7 ok / gw8 [0] / gw9 ok / gw10 ok / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 gw0 [0] / gw1 ok / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 ok / gw7 ok / gw8 [0] / gw9 ok / gw10 ok / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18 gw0 [0] / gw1 ok / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 ok / gw7 [0] / gw8 [0] / gw9 ok / gw10 ok / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw18gw0 [0] / gw1 ok / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 ok / gw10 ok / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw1gw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 ok / gw10 ok / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gwgw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 ok / gw10 [0] / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / ggw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 ok / gw10 [0] / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / ggw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 ok / gw10 [0] / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / ggw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 ok / gw16 ok / gw17 ok / gw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 ok / gw12 [0] / gw13 ok / gw14 ok / gw15 [0] / gw16 ok / gw17 ok /gw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 ok / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 ok / gw12 [0] / gw13 ok / gw14 [0] / gw15 [0] / gw16 ok / gw17 ok gw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 ok / gw12 [0] / gw13 ok / gw14 [0] / gw15 [0] / gw16 ok / gw17 okgw0 [0] / gw1 [0] / gw2 ok / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 ok / gw14 [0] / gw15 [0] / gw16 ok / gw17 ogw0 [0] / gw1 [0] / gw2 [0] / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 ok / gw14 [0] / gw15 [0] / gw16 ok / gw17 gw0 [0] / gw1 [0] / gw2 [0] / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 ok / gw14 [0] / gw15 [0] / gw16 ok / gw17 gw0 [0] / gw1 [0] / gw2 [0] / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 [0] / gw14 [0] / gw15 [0] / gw16 ok / gw17gw0 [0] / gw1 [0] / gw2 [0] / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 [0] / gw14 [0] / gw15 [0] / gw16 [0] / gw1gw0 [0] / gw1 [0] / gw2 [0] / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 [0] / gw14 [0] / gw15 [0] / gw16 [0] / gw1gw0 [0] / gw1 [0] / gw2 [0] / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 [0] / gw14 [0] / gw15 [0] / gw16 [0] / gw1gw0 [0] / gw1 [0] / gw2 [0] / gw3 [0] / gw4 [0] / gw5 [0] / gw6 [0] / gw7 [0] / gw8 [0] / gw9 [0] / gw10 [0] / gw11 [0] / gw12 [0] / gw13 [0] / gw14 [0] / gw15 [0] / gw16 [0] / gw17 [0] / gw18 [0] / gw19 [0] / gw20 [0] / gw21 [0] / gw22 [0] / gw23 [0]\r\n\r\n====================================================================================== ERRORS ======================================================================================\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n_____________________________________________________________________ ERROR collecting tests/lax_numpy_test.py _____________________________________________________________________\r\ntests/lax_numpy_test.py:523: in <module>\r\n    class LaxBackedNumpyTests(jtu.JaxTestCase):\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:577: in __new__\r\n    _update_class_dict_for_param_test_case(\r\n../conda/envs/jax/lib/python3.9/site-packages/absl/testing/parameterized.py:631: in _update_class_dict_for_param_test_case\r\n    raise DuplicateTestNameError(test_class_name, new_name, original_name)\r\nE   absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)' (generated from 'testPadStatValues_shape=float16[4]_mode=mean_pad_width=1_stat_length=((1, 2),)') already exists. Consider using named_parameters() to give your tests unique names and/or renaming the conflicting test method.\r\n============================================================================= short test summary info ==============================================================================\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\nERROR tests/lax_numpy_test.py - absl.testing.parameterized.DuplicateTestNameError: Duplicate parameterized test name in LaxBackedNumpyTests: generated test name 'testPadStatValu...\r\n========================================================================== 24 errors in 68.72s (0:01:08) ===========================================================================\r\n```' by a CONTRIBUTOR of type User on 2022-04-12T03:22:51Z\n'@YouJiacheng I think the randomly generated test cases have some overlap across processes, you could reduce `JAX_NUM_GENERATED_CASES` to 100 or 200 and try again.' by a NONE of type User on 2022-04-12T03:26:03Z\n'@soraros Thanks! I try `JAX_NUM_GENERATED_CASES=100` and `JAX_NUM_GENERATED_CASES=200`, and the fixed code pass all test. (buggy code w/o considering scalar fail in 32/200 cases)' by a CONTRIBUTOR of type User on 2022-04-12T03:36:53Z\n'So, it turns out this breaks a few things that aren't well covered by github CI tests, in particular:\r\n\r\n- custom prng keys, which have special requirements for functions like `tile`\r\n- polymorphic shapes\r\n\r\nYou should be able to run these tests locally with\r\n```\r\n$ JAX_ENABLE_CUSTOM_PRNG=true pyteset -n auto tests/random_test.py\r\n$ pytest -n auto jax/experimental/jax2tf/tests/shape_poly_test.py\r\n```\r\nWe'll need to make sure those are green before merging.' by a COLLABORATOR of type User on 2022-04-13T15:45:41Z\n'I need some advice/help:\r\n1. Should I implement `PRNGKeyArray.tile`, or fall back to original implementation using `broadcast_to` for `PRNGKeyArray`?\r\n2. Should I catch `InconclusiveDimensionOperation`(maybe cause cyclic import) or all exception when check `s == 1`(and return false for this check), or check `type(s) is int`?(Ah, I find `reps` can be poly thus `s` need to be checked as well.)' by a CONTRIBUTOR of type User on 2022-04-13T17:22:02Z\n'I'm actually not sure on this. For question 1, @froystig may have some input. For question 2, @gnecula may have some input.' by a COLLABORATOR of type User on 2022-04-13T17:26:37Z\n'Ah Oh, why DeviceArray has `tile` attribute...' by a CONTRIBUTOR of type User on 2022-04-13T18:15:46Z\n'> Ah Oh, why DeviceArray has tile attribute...\r\n\r\nStrange - we should probably remove that. In general DeviceArray methods are based on numpy array methods, and numpy arrays do not have a `tile` method.' by a COLLABORATOR of type User on 2022-04-13T18:17:53Z\n'In `JAX_ENABLE_CUSTOM_PRNG=true pytest -n auto tests/random_test.py` there are 2 fail case has nothing to do with this PR(IIUC).\r\n```\r\nFAILED tests/random_test.py::LaxRandomTest::test_random_split_doesnt_device_put_during_tracing - FutureWarning: Raw arrays as random keys to jax.random functions are deprecated....\r\nFAILED tests/random_test.py::LaxRandomWithCustomPRNGTest::test_random_split_doesnt_device_put_during_tracing - FutureWarning: Raw arrays as random keys to jax.random functions a...\r\n```' by a CONTRIBUTOR of type User on 2022-04-13T18:22:51Z\n'BTW, by reading the code, I think there is a bug in `PRNGKeyArray.broadcast_to`\r\nhttps://github.com/google/jax/blob/0b898ea627f02410bfae0d52299da6f28bdc1986/jax/_src/prng.py#L212-L214\r\nhttps://github.com/google/jax/blob/0b898ea627f02410bfae0d52299da6f28bdc1986/jax/_src/numpy/util.py#L342-L344\r\nIf `shape` is a scalar, `(*shape, ...)` will raise an error.' by a CONTRIBUTOR of type User on 2022-04-13T18:29:46Z\n'> Ah Oh, why DeviceArray has `tile` attribute...\r\n\r\nCurrently I add an underscore, i.e. using `_tile` attribute as a workaround.' by a CONTRIBUTOR of type User on 2022-04-13T18:40:14Z\n'Deprecating the `tile` method in #10266' by a COLLABORATOR of type User on 2022-04-13T18:51:50Z\n'You could also use the `_stackable` registry to decide when you want to call `arr.tile`.' by a COLLABORATOR of type User on 2022-04-13T18:54:36Z\n'Does this PR still need review from froystig and gnecula?\r\nI have passed these two check locally.' by a CONTRIBUTOR of type User on 2022-04-14T21:30:00Z\n'friendly ping:) @froystig ' by a CONTRIBUTOR of type User on 2022-04-18T17:46:22Z\n'~Ah, it seems that optimized fall-back version has some bugs.~\r\nNope, it is not related to `tile`\r\n```\r\nFAILED tests/random_test.py::LaxRandomTest::test_random_split_doesnt_device_put_during_tracing - FutureWarning: Raw arrays as random keys to jax.random functions are dep...\r\nFAILED tests/random_test.py::LaxRandomWithCustomPRNGTest::test_random_split_doesnt_device_put_during_tracing - FutureWarning: Raw arrays as random keys to jax.random fun...\r\n```\r\nThe same as https://github.com/google/jax/pull/10228#issuecomment-1098351773' by a CONTRIBUTOR of type User on 2022-04-22T08:59:32Z\n'@jakevdp wrote:\r\n> Thanks! I don't doubt this is correct, but it's definitely not obvious from reading the code.\r\n> \r\n> Just to be sure we're covering all corners, can you run locally a more complete set of tests on this?\r\n> \r\n> ```\r\n> $ JAX_NUM_GENERATED_CASES=500 pytest -n auto tests/lax_numpy_test.py -k testTile\r\n> ```\r\n> \r\n> Thanks!\r\n\r\nI am wondering if the added complexity in the code is worth the benefit. There are many situations when JAX generates useless HLO ops assuming that the compiler will squash those. This allows JAX to stay simple and defer complexity to the compiler.\r\n\r\n' by a COLLABORATOR of type User on 2022-04-23T07:11:44Z\n'> @jakevdp wrote:\r\n> \r\n> > Thanks! I don't doubt this is correct, but it's definitely not obvious from reading the code.\r\n> > Just to be sure we're covering all corners, can you run locally a more complete set of tests on this?\r\n> > ```\r\n> > $ JAX_NUM_GENERATED_CASES=500 pytest -n auto tests/lax_numpy_test.py -k testTile\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > Thanks!\r\n> \r\n> I am wondering if the added complexity in the code is worth the benefit. There are many situations when JAX generates useless HLO ops assuming that the compiler will squash those. This allows JAX to stay simple and defer complexity to the compiler.\r\n\r\nHi! - The readability of the code has been improved greatly since that comment. I think it only increases a small amount of complexity now. \r\nCurrently the compiler doesn't squash the useless HLO. The tracing speed is improved in this PR as well.\r\nShorter `jaxpr` may also benefit som users.' by a CONTRIBUTOR of type User on 2022-04-23T08:59:03Z\n'George's point about code complexity is a good one: this PR replaces three very readable lines with 11 lines that are much more complicated.' by a COLLABORATOR of type User on 2022-04-23T15:18:07Z\n'> George's point about code complexity is a good one: this PR replaces three very readable lines with 11 lines that are much more complicated.\r\n\r\nIf complexity is a concern, I can remove the special optimization for numerical array: \r\nThe speed up in tracing and simplification in jaxpr will be lost, while the simplification in HLO is preserved.\r\nAnd there will be only 4 lines. But such \"optimization\" is just do something compiler should do. I prefer to have faster tracing...' by a CONTRIBUTOR of type User on 2022-04-23T17:21:12Z\n'Note that this kind of refactor is not without risk; see e.g. #10437, which outlines the reasons that a similar kind of change needed to be reverted.' by a COLLABORATOR of type User on 2022-04-25T14:57:41Z\n'@jakevdp \r\nAgreed. Every change has non-zero risk. \r\nHowever, for #10221 reverted by #10437 , one of the reason why `tile` generate a much complex jaxpr is the absence of optimization for `tile` in this PR, which can reduce 2 `reshape` to one and remove squeeze. (Though one `reshape` may still break TPU compiler).\r\nActually, in terms of jaxpr or HLO, the proposed implementation in this PR is **strictly simpler** than the original implementation.\r\n' by a CONTRIBUTOR of type User on 2022-04-25T15:19:08Z\n'After 2 months(such that I can read the code as if it isn't written by myself😂), I can judge that the readability decrease significantly. Close this PR.' by a CONTRIBUTOR of type User on 2022-07-07T14:47:09Z\n\nPR has review comments:\n'Maybe something like the following will make this code more obvious\r\n```suggestion\r\n  rs_pairs = [(r * s,) if r == 1 or s == 1 else (r, s) for r, s in zip(reps, A_shape)]\r\n  target_shape = tuple(chain.from_iterable(rs_pairs))\r\n  broadcast_dims = tuple(np.cumsum(list(map(len, rs_pairs))) - 1)[len(A_shape) - ndim(A):]\r\n  result = lax.broadcast_in_dim(A, target_shape, broadcast_dims)\r\n```\r\n\r\n`broadcast_dims[len(A_shape) - ndim(A):]` is needed (also in your code I believe) because `A` may be a scalar.' by a NONE of type User on 2022-04-11T20:23:01Z\n'Great catch, and your suggestion make sense! What do you think @jakevdp ?' by a CONTRIBUTOR of type User on 2022-04-12T02:00:08Z\n'@YouJiacheng  Just a nit, but I really don't like the double list comprehension instead of `chain`, even `np.concatenate` looks better.' by a NONE of type User on 2022-04-12T03:20:30Z\n'Ah, I agree nested-for comprehension is not pretty. \r\nBut it is faster than `np.concatenate`, and doesn't need `from itertools import chain`. \r\n```python\r\nx = jnp.ones((1,) * 100 + (2,) * 10)\r\nprint(timeit('jax.make_jaxpr(lambda a: tile(a, 2))(x)', globals=globals(), number=100))\r\n# 5% faster than np.concatenate\r\n```\r\nOriginal implementation use nested-for comprehension too :P.' by a CONTRIBUTOR of type User on 2022-04-12T03:55:46Z\n'```suggestion\r\n  # We don't need this method for correctness, but including it takes\r\n  # advantage of an optimization in the `jnp.tile` implementation.\r\n  def _tile(self, reps):\r\n```' by a MEMBER of type User on 2022-04-21T18:07:49Z\n'Would it be worth adding a TODO to change this from `_tile()` to `tile()` once the deprecation in #10266 is complete?' by a COLLABORATOR of type User on 2022-04-21T18:14:05Z\n'After this optimization, without `_tile` to handle the special case for `PRNGKeyArray`, the correctness will be broken.\r\nGiven that the future readers may not know the context here: There is another option, making `jnp.tile` handle the `PRNGKeyArray` special case using `reshape` and `broadcast_to`)\r\nWhat about \"We don't need this method for **completeness**, but including it enables an optimized `jnp.tile` implementation not built on the top of `reshape`&`broadcast_to` while takes advantage of this optimization comparing to using `reshape`&`broadcast_to` for `PRNGKeyArray` in `jnp.tile`.\"' by a CONTRIBUTOR of type User on 2022-04-21T18:22:47Z\n'Done.\r\n```python\r\n  # We don't need this method for completeness, but including it enables\r\n  # an optimized `jnp.tile` implementation not built on the top of\r\n  # `reshape`&`broadcast_to` while takes advantage of this optimization\r\n  # comparing to using `reshape`&`broadcast_to` for `PRNGKeyArray` in jnp.\r\n  # TODO: rename to `tile` once `DeviceArray.tile` removed.\r\n```' by a CONTRIBUTOR of type User on 2022-04-21T18:44:43Z\n'Revisiting this, I think it would be simplest to skip the optimization for `PRNGKeyArray` altogether, and instead continue to back off to the reshape/broadcast that would happen without it.\r\n\r\nIdeally we will have solved this all at once when we figure out generalized custom element typed arrays. In the meantime, I don't expect tiling keys to be a common enough operation to make it worth this bit of complexity.\r\n\r\nThe optimization still seems good for numerical arrays. Would you mind reverting this PRNG part and the corresponding `hasattr` check?' by a MEMBER of type User on 2022-04-21T21:59:48Z\n'Thanks, considering generalized custom element typed arrays in future, I can add a general fall back for non-ndarray.' by a CONTRIBUTOR of type User on 2022-04-22T07:46:13Z\n'BTW, do you think we should replace `hasattr` check in `broadcast_to`, `reshape` etc to `_stackable` check?' by a CONTRIBUTOR of type User on 2022-04-22T07:52:06Z\n'Ah, I think I can optimize the fall-back version as well.(But not as much as numerical version)' by a CONTRIBUTOR of type User on 2022-04-22T08:00:40Z\n'This is not a breaking change in most cases\r\nunless `reps` is a user defined iterable whose `__iter__` has side effect.\r\n```python\r\nclass A(list):\r\n  def __iter__(self):\r\n    a = list(self)\r\n    self.pop()\r\n    return iter(a)\r\n```' by a CONTRIBUTOR of type User on 2022-04-22T09:23:16Z\n'Is there any other user-facing behavior that this changes? This corner-case of a strangely defined custom `iter` method doesn't strike me as an important behavior to match.' by a COLLABORATOR of type User on 2022-04-22T16:44:30Z\n'All things being equal, I don't see much downside to the current approach.' by a COLLABORATOR of type User on 2022-04-22T16:45:01Z\n'If you wish I can revert it.' by a CONTRIBUTOR of type User on 2022-04-22T17:02:42Z\n'I actually change it back from `if ndim(reps) == 0: reps = (reps,)`, which doesn't work correctly with iterable(except `range`).' by a CONTRIBUTOR of type User on 2022-04-22T17:08:46Z\n'Yes, in general I think we should avoid changes to the code that are not motivated by a specific bug, optimization, or enhancement.' by a COLLABORATOR of type User on 2022-04-22T17:09:05Z\n'Done.' by a CONTRIBUTOR of type User on 2022-04-22T17:28:12Z\n'Done.' by a CONTRIBUTOR of type User on 2022-04-22T17:34:57Z\n'Is the comparison `type(r) is int` meant to address the case when `r` is a non-constant dimension? In the case, it is better to use `core.is_constant_dim`, as above. Also, `r == 1 or s == 1` should be written as `core.symbolic_equal_dim(r, 1) or core.symbolic_equal_dim(s, 1)`' by a COLLABORATOR of type User on 2022-04-23T07:06:54Z\n'Thanks! I will have a look.' by a CONTRIBUTOR of type User on 2022-04-23T09:03:37Z\n'I use `core.symbolic_equal_dim(r, 1) or core.symbolic_equal_dim(s, 1)` instead of `type(r) is type(s) is int and (r == 1 or s == 1)`. Thanks!' by a CONTRIBUTOR of type User on 2022-04-23T09:19:26Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 26,
        "num_review_comments": 23,
        "reason_for_closure": "Closed as not useful: PR author realized the change decreased readability after two months."
    },
    {
        "summary": "Pull Request '10161' titled 'Add Hilbert matrix to jax.scipy.linalg' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-04-06T07:30:03Z, and was closed at 2023-11-03T18:29:26Z by a User.\nIt has a body of 'This PR adds an implementation of Hilbert matrices of order `n` that mirrors the API of `scipy.linalg.hilbert(n)` (https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.hilbert.html#scipy.linalg.hilbert).\r\n\r\nSee #10144 for context.\r\n\r\nPinging @jakevdp, because he offered to review :) '\nPR has comments:\n'(We'll also need to rebase on an updated main branch now to resolve conflicts)' by a COLLABORATOR of type User on 2022-04-18T17:58:54Z\n'I'm going to close this since it's gone stale – feel free to re-open if you'd like to work on this again!' by a COLLABORATOR of type User on 2023-11-03T18:29:26Z\n\nPR has review comments:\n'`n=0` and `n=1` as corner cases, and if it works for `n=5` then it must be correct' by a CONTRIBUTOR of type User on 2022-04-06T07:36:02Z\n'As detailed in the comment in #10144, through this splitting of the function, the transformation part (`_arange_to_hilbert`) can be jitted completely, whereas `hilbert` must be decorated with `static_argnames`. If this is too verbose, I can change it. :) ' by a CONTRIBUTOR of type User on 2022-04-06T07:38:36Z\n'Just to be sure: I added this manually. If there is some mechanism that makes this change automatically, which I have missed, please let me know :) ' by a CONTRIBUTOR of type User on 2022-04-06T07:39:28Z\n'It's not necessary to split this into a separate helper function. Regarding JIT compilation, you've already decorated the main function wtih jit, so there is no benefit to separately jitting the implementation. Regarding code design, the two functions are so short that in my judgment delegating to a helper obscures rather than aids in understanding the code.\r\n\r\nI'd suggest deleting lines 664-667' by a COLLABORATOR of type User on 2022-04-06T13:15:55Z\n'please alphabetize the imports' by a COLLABORATOR of type User on 2022-04-06T13:16:48Z\n'It surprises me that there's no `dtype` option. I know scipy doesn't provide it, but given the computational characteristics of accelerators, maybe the JAX version should? What do you think?' by a COLLABORATOR of type User on 2022-04-06T13:19:00Z\n'Good question. Would the `dtype` be passed on to `arange`, or would the resulting matrix be cast to the `dtype`, or both?\r\n\r\nI am not as familiar with the computational characteristics of accelerators, but here are some thoughts:\r\n* Other array-creation routines (e.g. https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.fft.fftfreq.html?highlight=fftfreq) don't do that, even though a similar case could be made here, too?\r\n* Which types would even be relevant? Roughly those that are allowed by `arange` (Mostly `float`- and `complex`-style types)? \r\n* I am also wondering whether the `1/(a + aT)` computation might introduce some implicit type promotion that introduces subtle issues which would be hard to catch in tests. But I am just speculating...\r\n\r\nSo intuitively, I would not have added that option. Though I don't have a strong opinion, and if you think it is a good idea I will add a dtype argument :) Thoughts? ' by a CONTRIBUTOR of type User on 2022-04-06T14:02:06Z\n'`1/(a + aT)` would promote to inexact, so any inexact dtype should be fine.\r\n\r\nWe can probably punt on this now and add it if/when someone requests it.' by a COLLABORATOR of type User on 2022-04-06T16:17:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 8,
        "reason_for_closure": "Stale :- PR closed due to inactivity and lack of author follow-up."
    },
    {
        "summary": "Pull Request '9529' titled 'jax.numpy: add ufunc-style APIs for several jnp functions' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-02-10T22:28:10Z, and was closed at 2023-08-10T19:08:41Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This adds a `ufunc` object to the `jax.numpy` API, so that standard unary and binary functions have the extra methods exposed by numpy ufuncs. Example:\r\n```python\r\nIn [1]: import jax.numpy as jnp\r\n\r\nIn [2]: x = jnp.arange(5)\r\n\r\nIn [3]: indices = jnp.array([1, 1, 3])\r\n\r\nIn [4]: jnp.add(x, 1)  # Standard function call works\r\nOut[4]: DeviceArray([1, 2, 3, 4, 5], dtype=int32)\r\n\r\nIn [5]: jnp.add.reduce(x)  # reduce() method applies binary reduction.\r\nOut[5]: DeviceArray(10, dtype=int32)\r\n\r\nIn [6]: jnp.add.accumulate(x)  # accumulate() method is cumulative reduction\r\nOut[6]: DeviceArray([ 0,  1,  3,  6, 10], dtype=int32)\r\n\r\nIn [7]: jnp.add.at(x, indices, 10, inplace=False)  # at() method is similar to JAX's ndarray.at\r\nOut[7]: DeviceArray([ 0, 21,  2, 13,  4], dtype=int32)\r\n\r\nIn [8]: jnp.add.reduceat(x, indices)  # reduceat() is similar to JAX's segment_sum\r\nOut[8]: DeviceArray([0, 3, 7], dtype=int32)\r\n```\r\n\r\nI'll still need to give the ufunc treatment to the rest of the functions in `jax._src.numpy.ufuncs`, but this gives us a good start.'\nPR has comments:\n'Note to self: we could also probably raise better errors for unsupported function arguments, cf. #1328 and #1364' by a COLLABORATOR of type User on 2022-06-21T20:18:59Z\n'For some of these, we should probably compute accumulations via `associative_scan` rather than `scan` for better performance. I'll probably leave that for a followup.' by a COLLABORATOR of type User on 2022-07-08T23:10:57Z\n'I was thinking about this more... maybe a more JAXy way to do this would be to define a `@ufunc` decorator that can turn *any* function into a ufunc instance with the appropriate attributes. That would reduce the complexity of this change, and we could introduce the feature in a more gradual manner. We could potentially wrap basic ufuncs by default, but we wouldn't have to do it right away.' by a COLLABORATOR of type User on 2022-10-31T21:18:58Z\n'Closing in favor of https://github.com/google/jax/pull/17054 for now.' by a COLLABORATOR of type User on 2023-08-10T19:08:41Z\n\nPR has review comments:\n'```suggestion\r\n    jnp_op = getattr(jnp, rec.name)\r\n```\r\n\r\nAny reason to bind the intermediate name `jnp_ufunc`? (I find superfluous intermediate names can make it easy to write bugs, so they stand out to me!)' by a COLLABORATOR of type User on 2022-07-08T19:13:14Z\n'Done, thanks!' by a COLLABORATOR of type User on 2022-07-08T20:37:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 2,
        "reason_for_closure": "Closed in favor of a different approach."
    },
    {
        "summary": "Pull Request '9423' titled 'add core.cur_level() function to get trace level' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-02-03T06:40:36Z, and was closed at 2023-01-05T19:35:26Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'This function may be useful for library authors who want to prevent common user errors related to applying JAX transformations to effectful code. By checking the level one can notice if new transformations have been applied when calling effectful functions (e.g. to get a new PRNG key) in the middle of an effectful context (when transformations should be avoided).\r\n\r\nThis is an experiment in collaboration with particular library authors. If it's useful we may end up documenting this as a \"tier 2: libraray author API\", as opposed to a \"tier 1: public user API\".\r\n\r\nHere's how it can be used:\r\n\r\n```python\r\nimport jax\r\n\r\ndef f():\r\n  print(jax.core.cur_level())\r\n  return 0.\r\n\r\nf()  # 0\r\njax.vmap(f, axis_size=3)()  # 1\r\njax.vmap(jax.vmap(f, axis_size=3), axis_size=2)()  # 2\r\njax.jvp(jax.vmap(f, axis_size=1), (), ())  # 2\r\n```\r\n\r\nThe level reflects proper transformation level and does not reflect staging level for `jit`, `pmap`, or `xmap`:\r\n\r\n```python\r\njax.jit(f)()  # 0\r\n```\r\n\r\nIf one wants to catch whether such staging is happening, one can check whether `jax.core.thread_local_state.trace_state.trace_stack.dynamic` is a `jax.core.EvalTrace` instance. Or one can just write a primitive for which the abstract eval rule raises an error:\r\n\r\n```python\r\ndef no_staging():\r\n  no_staging_p.bind()\r\nno_staging_p = jax.core.Primitive('no_staging')\r\n\r\n@no_staging_p.def_impl\r\ndef no_staging_impl():\r\n  pass\r\n\r\n@no_staging_p.def_abstract_eval\r\ndef no_staging_abstract_eval():\r\n  raise Exception(\"can't be staged!\")\r\n\r\n##\r\nf()  # fine!\r\njax.jit(f)()  # Exception: can't be staged!\r\n```\r\n\r\nBut `jit`/`pmap`/`xmap` staging aside, the `cur_level` function added in this PR should be helpful for catching transforamtions.\r\n\r\ncc @tomhennigan \r\n\r\nTODO:\r\n* [ ] tweak this to something more like `get_hashable_trace_state`, which would include the level, sublevel, and dynamic trace type.'\nPR has comments:\n'I'm going to close this PR as stale.' by a COLLABORATOR of type User on 2023-01-05T19:35:26Z\n\nPR has review comments:\n'As discussed on chat, users will need a little more than this to detect all the things. In deepmind/dm-haiku#301 I've checked in a version that reaches into core to do this (we will update as soon as this PR is merged and a stable release of JAX is made including it). Our impl:\r\n\r\n```python\r\nOpaque = Any\r\n\r\ndef jax_trace_level(cls) -> Opaque:\r\n  trace_stack = jax_core.thread_local_state.trace_state.trace_stack.stack\r\n  top_type = trace_stack[0].trace_type\r\n  level = trace_stack[-1].level\r\n  sublevel = jax_core.cur_sublevel()\r\n  return top_type, level, sublevel\r\n```' by a COLLABORATOR of type User on 2022-02-08T08:20:13Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Stale :- PR abandoned due to lack of activity or progress."
    },
    {
        "summary": "Pull Request '9396' titled 'type promotion: disable uint64->float' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-01-31T23:34:39Z, and was closed at 2022-06-29T19:52:40Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'In the current type promotion lattice, there is a promotion path from `uint64` to `float*`, which means that JAX follows numpy's behavior of promoting `np.uint64`  & `np.int64` to float.\r\n\r\nThis promotion behavior can be surprising, and there are good arguments that we should not do this kind of promotion implicitly.\r\n\r\nThis PR is a draft that removes this behavior, such that `np.uint64` promoting with any signed type results in an error.\r\n\r\nPulling in before review for additional testing.'\nPR has comments:\n'Closing this, it's become far out-of-date.' by a COLLABORATOR of type User on 2022-06-29T19:52:40Z\n\nPR has review comments:\n'This seems surprising. I would have expected `abs` not to promote.' by a COLLABORATOR of type User on 2022-02-03T13:34:04Z\n'This does remove a useful test case, which is u64 // u64. Same for most of the binary operators changed by this PR.\r\n\r\nI wonder if these test harness would be better expressed by providing the complete set of valid types for all arguments, built using some helpers.\r\n\r\nAnother option would be to add another record just for the u64/u64 case.\r\n\r\n(Not to mention: we should probably be testing invalid types also and checking for errors.)' by a COLLABORATOR of type User on 2022-02-03T13:35:44Z\n'This also seems surprising. In classic NumPy, numpy.sign of a `uint64` yields a `uint64`.' by a COLLABORATOR of type User on 2022-02-03T13:36:27Z\n'Looks like you should add this test case for uint64 as well.' by a COLLABORATOR of type User on 2022-02-03T13:38:54Z\n'I think it's probably also worth emphasizing that it's the (uint64, int64)-> float promotion that is particularly odd. (uint64, float) -> float seems sensible to me, but it's the consequence of that in the lattice rules for uint64 and int64 that is troubling.' by a COLLABORATOR of type User on 2022-02-03T13:42:59Z\n'Yeah, this is strange... Semantically it might be nice if `uint64,float -> float` were allowed, but `uint64,int -> float` were not. But I don't think that's possible within a lattice-based promotion approach.' by a COLLABORATOR of type User on 2022-02-03T17:53:49Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 6,
        "reason_for_closure": "Abandoned :- PR abandoned as the related discrepancy was deemed unnecessary to fix."
    },
    {
        "summary": "Pull Request '9313' titled '[WIP] migrate from pocketfft to ducc.fft' was authored by a User, who is associated as a NONE. \nIt was created at 2022-01-25T09:57:50Z, and was closed at 2022-08-26T18:36:52Z by a User.\nIt has a body of 'Related to #7490\r\n\r\nThis is still at a very early stage:\r\n - ~~one FFT test fails with a segfault (I'm looking into that)~~\r\n - I probably have to add BSD licensing headers to some of the ducc source files\r\n - if multi-threading should be used, we have to adjust the interface accordingly\r\n - I haven't done any proper formatting for the source changes in JAX\r\n\r\nHowever I'm happy about any feedback already at this stage!'\nPR has comments:\n'I realized that the `copts` in `third_party/pocketfft/BUILD.bazel` most likely won't achieve anything, since most of the code isin headers and will be compiled as part of  `jaxlib/pocketfft_kernels.cc`. So we should probably adjust the compiler options for this file...' by a NONE of type User on 2022-01-25T11:01:47Z\n'Technical question: the CLA only covers the actual content of this PR, not the library sources themselves, correct? If so, I probably won't have to discuss this with a lawyer ;-) ' by a NONE of type User on 2022-01-25T12:40:01Z\n'There are two failures in `lax_vmap_test.py` where the results are too inaccurate. I don't understand the tests well enough to judge whether this is really a problem. ' by a NONE of type User on 2022-01-25T13:16:05Z\n'Thanks! I'll look at the technical aspects of this shortly.\r\n\r\n> Technical question: the CLA only covers the actual content of this PR, not the library sources themselves, correct? If so, I probably won't have to discuss this with a lawyer ;-)\r\n\r\nI don't speak for Google's lawyers, but yes, my understanding is that the CLA applies only to your contributions, e.g., PRs.\r\n\r\nAs I mentioned before, we don't want to incorporate GPL-licensed code because it would require us to distribute JAX itself under a much more restrictive license than its current license (Apache 2). (Obviously third parties are welcome to plug in GPL code: it's just not something we ourselves want to distribute.)\r\n\r\nTo get this merged, we'd need ducc0's LICENSE information to be explicit about which files are under the BSD license and where the GPL license does not apply, i.e., a clear statement from you as the author in the project's license information that the GPL license doesn't apply to any of the files needed for FFT transformations.\r\n\r\nCurrently it looks like, for example, in ducc0 `fft/fft.h` is marked as BSD-licensed, but some of its dependencies (e.g., in `infra/`) are GPL-licensed and marked as such. We'd need everything in the transitive closure of the FFT library to fall under the more liberal license (e.g.., a BSD license, see https://opensource.google/docs/thirdparty/licenses/#notice) and for that to be clearly stated. Does that seem possible?\r\n\r\n' by a COLLABORATOR of type User on 2022-01-25T14:35:55Z\n'> Currently it looks like, for example, in ducc0 fft/fft.h is marked as BSD-licensed, but some of its dependencies (e.g., in infra/) are GPL-licensed and marked as such. We'd need everything in the transitive closure of the FFT library to fall under the more liberal license (e.g.., a BSD license, see https://opensource.google/docs/thirdparty/licenses/#notice) and for that to be clearly stated. Does that seem possible?\r\n\r\nAbsolutely, that's the plan. As I mentioned above, I just didn't get around to add the change to all affected files. Will do that soon.\r\n' by a NONE of type User on 2022-01-25T14:40:35Z\n'> There are two failures in `lax_vmap_test.py` where the results are too inaccurate. I don't understand the tests well enough to judge whether this is really a problem.\r\n\r\nI think this is simply a case of the tolerances being a bit too strict/overfitted to the current behavior.' by a COLLABORATOR of type User on 2022-01-25T15:30:42Z\n'I'm currently updating all source files required by the FFT functionality to include the following:\r\n\r\n```\r\n/* SPDX-License-Identifier: BSD-3-Clause OR GPL-2.0-or-later */\r\n\r\n/*\r\nAll rights reserved.\r\n\r\nRedistribution and use in source and binary forms, with or without modification,\r\nare permitted provided that the following conditions are met:\r\n\r\n* Redistributions of source code must retain the above copyright notice, this\r\n  list of conditions and the following disclaimer.\r\n* Redistributions in binary form must reproduce the above copyright notice, this\r\n  list of conditions and the following disclaimer in the documentation and/or\r\n  other materials provided with the distribution.\r\n* Neither the name of the copyright holder nor the names of its contributors may\r\n  be used to endorse or promote products derived from this software without\r\n  specific prior written permission.\r\n\r\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\r\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\r\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\r\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\r\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\r\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\r\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\r\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\r\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\r\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n*/\r\n\r\n/*\r\n *  This code is free software; you can redistribute it and/or modify\r\n *  it under the terms of the GNU General Public License as published by\r\n *  the Free Software Foundation; either version 2 of the License, or\r\n *  (at your option) any later version.\r\n *\r\n *  This code is distributed in the hope that it will be useful,\r\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\r\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\r\n *  GNU General Public License for more details.\r\n *\r\n *  You should have received a copy of the GNU General Public License\r\n *  along with this code; if not, write to the Free Software\r\n *  Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\r\n */\r\n```\r\n\r\nWould that be sufficiently clear?\r\n\r\nI'm also updating the README.md file to mention that a subset of the ode is dual-licensed.\r\nI'd rather not mess with the LICENSE file itself, since that is a verbatim copy of GPLv2. As far as I understand, BSD3-licensed code does not require an accompanying license file; just the licensing header in each source file should be enough.' by a NONE of type User on 2022-01-28T09:30:39Z\n'I suspect that's fine, but for our use I'll need to get an ok from our compliance folks. But I think so long as it's clear and well-documented exactly what is covered by the BSD license they will be happy with it.' by a COLLABORATOR of type User on 2022-01-28T14:10:32Z\n'OK. In the extreme case, I could make a reduced package containing only the relevant files (not my favorite oprion, since it means more maintenance), or you can import the FFT subset into your sources directly. I don't expect frequent updates, roughly once per year or so.' by a NONE of type User on 2022-01-28T14:33:15Z\n'It looks like the CLA bot is unhappy. Is it possible you used a different email address on your commits than you used to sign the CLA?' by a COLLABORATOR of type User on 2022-08-25T15:51:13Z\n'If I understood correctly, a Google account is required to sign the CLA. Since I don\"t have one, I suggest that @Edenhofer re-submits the patch as his own. OK?' by a NONE of type User on 2022-08-26T06:24:17Z\n'Since the CLA bot is still unhappy, I filed a new MR at https://github.com/google/jax/pull/12120 .' by a CONTRIBUTOR of type User on 2022-08-26T09:58:48Z\n'#12122 was merged! Thanks for your help!' by a COLLABORATOR of type User on 2022-08-26T18:36:52Z\n\nPR has review comments:\n'Out of curiosity, does fast math have a large impact?\r\n\r\n(Elsewhere in JAX, we usually try for correct NaN/Inf handling, so fast math is something we turn off. Initially we tended to use fast math optimizations, but they tend to surprise people and so we've gradually turned most of them off over time.)' by a COLLABORATOR of type User on 2022-01-25T14:58:07Z\n'Would it make sense to add ducc0 as a parallel dependency to pocketfft rather than replacing pocketfft for the moment?\r\n\r\nI'm concerned that it might take a while to enable C++17 by default, but I don't want that to hold up your PR. One way we could manage that would be to add ducc0 as an alternative to pocketfft and select between the two depending on whether a C++17 compiler is available.' by a COLLABORATOR of type User on 2022-01-25T15:01:58Z\n'This would need to be reformatted, but I can do that as part of the merge.\r\n(We just apply this tool: https://github.com/bazelbuild/buildtools/blob/master/buildifier/README.md if you're curious.)' by a COLLABORATOR of type User on 2022-01-25T15:04:41Z\n'`-ffast-math`(or at least a subset of it) does indeed have a large impact, because without this flag the compiler is not allowed to use FMA instructions ... and FFTs consist of mostly FMAs. I know that there is a `#pragma` which is supposed to allow this, but `gcc` has a history of not dealing with this properly (I don't know about `clang`).\r\nMy idea would be to switch this on only for the  compilation of `jaxlib/pocketfft_kernels.cc`, if that's possible.' by a NONE of type User on 2022-01-25T15:12:17Z\n'I'm not in a hurry to have this merged; we can just wait until C++17 is acceptable. My main goal was to see how big the necessary adjustments will be (surprisingly small, it seems!).\r\nI don't know how to add this as a parallel dependency, but if you help me do that, it's also perfectly fine with me.' by a NONE of type User on 2022-01-25T15:16:56Z\n'Thanks, will have a look!' by a NONE of type User on 2022-01-25T15:17:25Z\n'Maybe we can get away with `-ffp-contract=fast` I'd have to test that with several compilers though. In any case this is the most important component of `-ffast-math` for this code.' by a NONE of type User on 2022-01-25T15:24:53Z\n'Interesting.\r\n\r\nFor clang to use fma instructions I think you need two things:\r\na) to target an architecture with FMA instructions, and\r\nb) to enable `fp-contract`: https://clang.llvm.org/docs/UsersManual.html#cmdoption-ffp-contract\r\nI assume gcc is similar.\r\n\r\n(b) is no problem: we can just turn that flag on for this file: it's just a slightly more nuanced `-ffast-math`.\r\n\r\nHowever, for (a) we don't distribute our Python wheels built for an architecture that has FMA instructions. We currently assume AVX but not AVX2 or FMA. So we'd want to start building for, say, Haswell machines, which introduced AVX2 and FMA, to get the full benefit.\r\n\r\n(Function multi-versioning is technically possible but it's problematic in C++ code because it runs a risk of introducing [ODR problems](https://en.wikipedia.org/wiki/One_Definition_Rule).)\r\n\r\nI think that's doable, but certainly even the AVX requirement has sometimes tripped people up! Apparently plenty of machines from older than 2013 are still out there!' by a COLLABORATOR of type User on 2022-01-25T15:25:40Z\n'If I remember correctly, I also encountered some situations where `gcc` refused to vectorize certain expressions without `-ffast-math`; if that is true, it could have an effect on pure AVX architectures (without FMA) as well. But this is probably more a concern for the other `ducc` components like spherical harmonic transforms, and not so much for FFT.\r\nLet's see how far we get without the sledgehammer `-ffast-math` :-)' by a NONE of type User on 2022-01-25T15:35:06Z\n'We can revert these changes: they are already present at head.' by a COLLABORATOR of type User on 2022-08-15T13:16:07Z\n'ducc instead of pocketfft.' by a COLLABORATOR of type User on 2022-08-15T13:16:18Z\n'We can bump this to a newer release.' by a COLLABORATOR of type User on 2022-08-15T13:17:17Z\n'We should rename these files but it can wait until a subsequent PR.' by a COLLABORATOR of type User on 2022-08-15T13:18:12Z\n'Unused; remove.' by a COLLABORATOR of type User on 2022-08-15T13:20:11Z\n'Great to see this making progress! I think I made the necessary adjustments.' by a NONE of type User on 2022-08-16T05:43:50Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 13,
        "num_review_comments": 15,
        "reason_for_closure": "Inconsistent behavior :- PR breaks existing users with little benefit by dropping complex dtype support."
    },
    {
        "summary": "Pull Request '9289' titled 'Implement jax2tf scatter_* ops with no XLA' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-01-22T09:49:16Z, and was closed at 2022-01-31T14:22:54Z by a User.\nIt has a body of 'Linking to #9269\r\n\r\nThis is a work in progress, it seems to work for the simplest examples. Will update with tests, etc.'\nPR has comments:\n'I'm doing this to convert some JAX code to ONNX. I find that my current fix using\r\n```python\r\ntensor_scatter_nd_add\r\ntensor_scatter_nd_max\r\ntensor_scatter_nd_min\r\n```\r\n\r\ndon't get converted by tf2onnx, while `tensor_scatter_nd_update` works fine and my devolved implementation of `scatter_mul` in JAX converts correctly. So I'm going to reimplement `add,max,min` similarly to how `mul` is currently' by a CONTRIBUTOR of type User on 2022-01-22T10:35:38Z\n'Reimplementation looks prettier in the onnx representation\r\n\r\n(I've implemented scatter_update separately rather than with the other scatters as it looks simpler this way, however it could be done in the same loop definition as the others using `lambda x,y: y` as the op for dealing with the update)\r\n\r\nCurrent implementation is (probably) limited to:\r\n- In bounds indexs\r\n- Strictly positive indexs\r\n- All inputs need to not be known at runtime? - I wasn't sure to how handle the update_jaxpr and update_consts\r\n- Unique indices\r\n\r\nPic below is scatter_add.\r\n![image](https://user-images.githubusercontent.com/44170519/150635380-fb638422-50a0-43d2-8ef9-8bbb15821fed.png)\r\n' by a CONTRIBUTOR of type User on 2022-01-22T10:47:37Z\n'Will add testing, and explicitly say which cases are supported (similarly to gathers exception raising).\r\n\r\nThis is still very much a work in progress, there's dense and a sparse scatter versions. I've only returned the dense as the sparse is not jit-compatible.\r\n\r\nAlso there could be subtyping for simpler implementations where the indices are unique and sorted.' by a CONTRIBUTOR of type User on 2022-01-25T14:00:06Z\n'I have found other routes for the task than no_xla, closing task. Thank you for your suggestions and comments.' by a CONTRIBUTOR of type User on 2022-01-31T14:22:54Z\n'@oliverdutton for reference, what other route did you find?' by a COLLABORATOR of type User on 2022-03-31T09:46:52Z\n'Implementing the XLA op on the backend directly.  This meant I didn't need the format changing to ONNX which is what pushed me to look at no XLA scatter.' by a CONTRIBUTOR of type User on 2022-03-31T10:45:31Z\n\nPR has review comments:\n'I wonder if this for loop would actually read better if you unroll it:\r\n```\r\ntf_impl_no_xla[lax.scatter_add_p] = convert_scatter_jax_to_tf(tf.math.unsorted_segment_sum, tf.add)\r\n...\r\n```' by a COLLABORATOR of type User on 2022-01-25T13:17:50Z\n'Probably best unrolled.' by a CONTRIBUTOR of type User on 2022-01-25T13:55:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 2,
        "reason_for_closure": "Author realized the PR was not needed or beneficial."
    },
    {
        "summary": "Pull Request '9222' titled 'Extend .compiler_ir() to accept kwargs controlling IR emission.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2022-01-18T04:26:15Z, and was closed at 2022-01-19T17:15:07Z by a User.\nIt has a body of 'As a user of this API, is important to have access to 'binary=', 'enable_debug_info=', and 'print_generic_op_form=' as different tasks and integrations call for different things (i.e. 'binary=' saves time/space on large programs, 'print_generic_op_form=' is more compatible with tooling to ensure compatibility, etc).\r\n\r\nI debated just having it return a live Module object, but I think it is still wise for the moment to keep this API narrow, and that would be a large expansion.'\nPR has comments:\n'I wasn't able to find formatting options. Let me know if there is an auto-formatter I should be using.' by a CONTRIBUTOR of type User on 2022-01-18T04:26:48Z\n'Closing in favor of https://github.com/google/jax/pull/9242' by a COLLABORATOR of type User on 2022-01-19T17:15:07Z\n\nPR has review comments:\n'OOC, what does \"binary\" do on the print function? Avoid escaping certain strings?' by a COLLABORATOR of type User on 2022-01-18T18:27:01Z\n'I'm increasingly leaning towards making `compiler_ir` return an `ir.Module` object. This is mostly because MLIR printing is not free. In particular, I recently ran across a case where the printed IR might be disproportionately large compared to the C++ object graph (callsiteloc).\r\n\r\nThis would have a downside that the `ir.Module` object may not agree between different Python extensions. But perhaps that isn't so bad and it could be covered by documentation?' by a COLLABORATOR of type User on 2022-01-18T18:29:25Z\n'> This is mostly because MLIR printing is not free.\r\n\r\n@hawkinsp Something related, don't know if worth creating an issue, so I will just mention it here. Large constants are not hidden in MLIR printing, even a 10 by 10 matrix can bloat printed IR.' by a NONE of type User on 2022-01-18T18:48:49Z\n'@soraros is that causing a noticeable problem for you? If so, file a bug?' by a COLLABORATOR of type User on 2022-01-18T18:59:19Z\n'I'm +1 on returning a Module. Code that crosses boundaries already needs to be aware of such things.\r\n\r\nbinary=True builds into a bytes based IO. For tool interop, the additional python str conversion is pure overhead (shouldn't change the size at rest much but python has to make multiple passes/copies in order to determine and use the minimal encoded string representation).' by a CONTRIBUTOR of type User on 2022-01-18T21:24:25Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 5,
        "reason_for_closure": "Closed as superseded by another PR: PR closed in favor of a different solution."
    },
    {
        "summary": "Pull Request '9188' titled 'add axis env state to cache keys, fixes #9187' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-01-13T05:11:42Z, and was closed at 2022-07-20T22:14:01Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of 'fixes #9187 \r\n\r\nThis change could cause some new and even unnecessary cache misses. That is, while the cache hits in #9187 were buggy and should be cache misses, to correct them we may end up being overly defensive.\r\n\r\nThe reason for defensiveness is that in general, even for a fixed sequence of input abstract values to a jitted function, whenever the named axis environment (which represents the mapped-over names bound by `vmap`, `pmap`, and/or `xmap`) is different from one we've seen before with those input abstract values, we need to re-trace the Python callable. That's because the Python callable could bind a collective like `psum` or `axis_index`, and the result of such a collective depends on the axis environment. (When a mapped axis has no name, i.e. when its name is `core.no_axis_name`, this error can't happen. So for caching purposes unnamed axes can be ignored.)\r\n\r\nBut some Python callables may not involve collectives at all! It'd be a waste to re-trace those. One such instance arose in this PR: the `_multi_slice` function, which is used in `pmap` dispatch, was getting re-traced (and re-compiled) even though it binds no collective primitives. So I introduced a low-level way to, in effect, promise that no collectives are bound: use `core.stash_axis_env` to temporarily empty out the axis environment and thus effectively ignore its value.\r\n\r\nStill, this issue could arise in user code... is the only option to provide such a \"promise\" API to users?\r\n\r\nTODO:\r\n* [ ] wrap (almost) all jax.numpy functions in `core.stash_axis_env` (tried this and ran into a weird `__doc__` issue...)'\nPR has comments:\n'Subsumed by [11298](https://github.com/google/jax/pull/11298).' by a COLLABORATOR of type User on 2022-07-20T22:14:01Z\n\nPR has review comments:\n'_multi_slice (used for dispatch) now gets compiled again, despite the 'warmup' call to f(x) a few lines up, because it's in a new axis env context... not sure what's up with that yet. JAX_LOG_COMPILES=1 prints out a different object id for it!' by a COLLABORATOR of type User on 2022-01-13T06:59:12Z\n'Indeed, on the first call the axis env is empty and on the second it's got something in it. That doesn't affect _multi_slice though.\r\n\r\nMore generally, it might be nice to promise that some functions don't depend on the axis environment (in the sense of #9187, like not using any collectives), so we don't have to cache miss when it changes... one way is we could temporarily set the axis env to empty when we trace such functions.' by a COLLABORATOR of type User on 2022-01-13T07:06:15Z\n'I've added such a mechanism!' by a COLLABORATOR of type User on 2022-01-13T07:13:29Z\n'I don't think this part is correct. In general you need to merge the thread-local state with the global state to get a cache key, I think?' by a COLLABORATOR of type User on 2022-01-14T19:25:11Z\n'Ah good, this is exactly the kind of thing I wanted to check!\r\n\r\nLMK if you have a pointer to where this is already done somewhere (even if in C++) so I can check against it...' by a COLLABORATOR of type User on 2022-01-14T20:33:37Z\n'We think this is actually correct: the other global parts of the state are already introduced in this function by it reading the relevant attributes from the `config` object. But the function name was misleading since it included \"thread_local\". I'll change the name!\r\n\r\nSeparately, I may look at making this faster...' by a COLLABORATOR of type User on 2022-01-15T01:01:38Z\n'Perhaps comment why this is the case? (i.e., there's no such thing as a global axis environment.)' by a COLLABORATOR of type User on 2022-01-18T19:08:29Z\n'I wonder if it would be possible to put a dummy value that caused an error if you were to actually try to use it here?' by a COLLABORATOR of type User on 2022-01-18T19:11:08Z\n'Done!' by a COLLABORATOR of type User on 2022-02-04T01:28:10Z\n'Such things are possible...\r\n\r\nThe current approach would also lead to an error, just with a potentially misleading error message (unbound axis name).' by a COLLABORATOR of type User on 2022-02-04T01:29:13Z\n'I added a comment to this effect but didn't add any fancy error mechanisms. Gotta leave some work for the future, right?' by a COLLABORATOR of type User on 2022-02-04T01:30:47Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 11,
        "reason_for_closure": "Subsumed by another PR :- This PR was closed as the fix was included in a larger, more comprehensive change."
    },
    {
        "summary": "Pull Request '9141' titled 'Implemented mode function' was authored by a User, who is associated as a NONE. \nIt was created at 2022-01-08T20:26:03Z, and was closed at 2022-10-11T16:14:48Z by a User.\nIt has a body of 'this PR is respect to #8978 , @jakevdp  please review it.\r\n\r\nI have implemented the mode function ,and have also created the _core.py file  and have placed the function there.\r\nalso i have added the license required and written unit tests as you guided under scipy_stats_test.py file.\r\n\r\nplease review my PR and let me know if any changes are required.'\nPR has comments:\n'Also, please do not close this PR and open another one. You can make the requested changes right here by pushing to your existing git branch – it's far easier to track the review process in a single PR.' by a COLLABORATOR of type User on 2022-01-11T17:23:39Z\n'@jakevdp  thanks for the reviews ,I will keep that in mind and implement the changes required' by a NONE of type User on 2022-01-11T21:31:45Z\n'@jakevdp  I have implemented all the changes suggested ,please review them.\r\nand let me know if any further changes are required :)' by a NONE of type User on 2022-01-13T14:48:13Z\n'There are still a number of formatting changes unrelated to the `mode` function. Please revert those.' by a COLLABORATOR of type User on 2022-01-13T16:33:14Z\n'@jakevdp i have fixed JIT ,and also reverted the unnecessary changes.\r\nplease review them.\r\nand let me know if any further changes are required :)\r\n' by a NONE of type User on 2022-01-13T16:58:05Z\n'I would suggest running the tests locally – the CI failures suggest that there may be some syntax or import issues.' by a COLLABORATOR of type User on 2022-01-13T17:24:36Z\n'@jakevdp  okay !\r\n How can i run them locally ?' by a NONE of type User on 2022-01-13T17:26:25Z\n'See https://jax.readthedocs.io/en/latest/developer.html#running-the-tests' by a COLLABORATOR of type User on 2022-01-13T17:27:00Z\n'@jakevdp I have fixed the imports and also run the tests locally ,it seems to working fine.\r\nplease review \r\nand let me know if any further changes are required :)' by a NONE of type User on 2022-01-13T19:04:21Z\n'Do you have un-committed changes locally? Because there are undefined variables in the versions of the files that you pushed to this branch (see the errors by clicking on the ❌  icons below)' by a COLLABORATOR of type User on 2022-01-13T19:47:44Z\n'@jakevdp I have fixed undefined variables .\r\nplease review\r\nand let me know if any further changes are required :)' by a NONE of type User on 2022-01-13T20:03:32Z\n'Your PR still contains a large number of unrelated changes, some of which are leading to lint errors (such as adding stray spaces at the end of a number of lines). I've mentioned this a few times already, but please revert these changes.' by a COLLABORATOR of type User on 2022-01-13T20:18:49Z\n'@jakevdp can you tell the formatter that you use for the project, I can run that at once so that it all gets fixed. ' by a NONE of type User on 2022-01-13T20:32:33Z\n'We don't use any automated formatter, unfortunately, so you'll have to manually revert the changes (and disable any auto-formatter you may be using that is causing the stray changes)' by a COLLABORATOR of type User on 2022-01-13T22:33:52Z\n'@jakevdp  I have reverted all the unrelated changes manually.\r\nplease review\r\nand let me know if any further changes are required :)' by a NONE of type User on 2022-01-14T20:48:43Z\n'There are still a number of unnecessary whitespace changes that are leading to lint errors, for example in `jax/_src/numpy/lax_numpy.py` and `jax/_src/api.py`. Please revert these as well.' by a COLLABORATOR of type User on 2022-01-14T20:58:55Z\n'@jakevdp I have reverted the blank lines but i couldn't found any trailing whitespaces in the files\r\nI have compared them from the original JAX repo.\r\n' by a NONE of type User on 2022-01-14T21:39:18Z\n'Click on the \"Files changed\" tab above and you'll see the whitespace diffs. If you can't see those diffs locally, it might be that you have some kind of git hook set up that re-formats your changes when you commit them.' by a COLLABORATOR of type User on 2022-01-14T22:07:52Z\n'@jakevdp  I have fixed all the whitespaces issues.\r\nplease review\r\nand let me know if any further changes are required :)\r\n' by a NONE of type User on 2022-01-15T11:31:56Z\n'Please make sure the lint & typecheck tests pass. You can run them locally following the instructions here: https://jax.readthedocs.io/en/latest/contributing.html#linting-and-type-checking' by a COLLABORATOR of type User on 2022-01-15T16:54:21Z\n'@jakevdp I have fixed the linting errors and it has passed the lint & typecheck tests now :)' by a NONE of type User on 2022-01-15T17:48:30Z\n'The test you added is still failing. I'd suggest running it locally as you debug – you should make certain the test passes before pushing the updates here.' by a COLLABORATOR of type User on 2022-01-15T18:41:32Z\n'@jakevdp I am getting assertion error on running the tests \r\n\r\n  AssertionError: Tuples differ: (1,) != ()\r\nE   \r\nE   First tuple contains 1 additional elements.\r\nE   First extra element 0:\r\nE   1\r\nE   \r\nE   - (1,)\r\nE   + ()\r\n\r\nHow can I track for which testcase it is throwing this error?\r\nand is it returning the shape or actual output of the result ?' by a NONE of type User on 2022-01-15T21:06:41Z\n'The full test output should contain the name of the test that is failing.' by a COLLABORATOR of type User on 2022-01-15T21:32:41Z\n'Thanks for the work on this – closing in favor of #12683' by a COLLABORATOR of type User on 2022-10-11T16:14:48Z\n\nPR has review comments:\n'Please revert changes to this file; they seem unrelated to your change.' by a COLLABORATOR of type User on 2022-01-11T17:00:51Z\n'We should call `_check_arraylike()` here to ensure that the input is not a list (search through the codebase to see how this function is used elsewhere)' by a COLLABORATOR of type User on 2022-01-11T17:01:56Z\n'I think it would be clearer to put the contents of `_chk_asarray` inline – it's only a few lines of code, and is not used elsewhere, so there's not much benefit to wrapping it in a function.' by a COLLABORATOR of type User on 2022-01-11T17:03:03Z\n'Rather than copying the contents of the docstring from scipy (which has licensing implications), we instead use the `_wraps` decorator to add it automatically. You can look at functions defined in `jax/_src/numpy/lax_numpy.py` to see how it is used.' by a COLLABORATOR of type User on 2022-01-11T17:07:22Z\n'`nan_policy = 'raise'` will never work correctly in traced code; we handle this elsewhere by detecting this and raising an appropriate error. See for example the handling of `mode='raise'` here: https://github.com/google/jax/blob/1cf7d4ab5dcea4619a2b6c1b471f2be340b6abfc/jax/_src/numpy/lax_numpy.py#L3462' by a COLLABORATOR of type User on 2022-01-11T17:09:18Z\n'We cannot fall back to numpy/scipy functions here, because they cast all inputs to regular numpy arrays, and thus are incompatible with JAX transforms. We'll have to implement these directly (or, you can punt on it now and raise `NotImplementedError`)' by a COLLABORATOR of type User on 2022-01-11T17:11:10Z\n'dtype is never object in JAX. So perhaps this whole block should be removed, because it seems to handle a case that will never occur?' by a COLLABORATOR of type User on 2022-01-11T17:13:14Z\n'Please revert unrelated formatting changes.' by a COLLABORATOR of type User on 2022-01-11T17:13:52Z\n'JAX arrays are immutable, so you cannot assign their values this way.\r\n\r\nBut it looks like this is an attempt to map a 1D function to multiple dimensions... for performance reasons, we should lean toward doing this with `vmap` rather than by manual iteration.' by a COLLABORATOR of type User on 2022-01-11T17:17:42Z\n'I'm confused by the use of `nanmode` here. In similar tests within `lax_numpy_test.py`, we are testing the functions e.g. `jnp.mean` and `jnp.nanmean`. I don't think there's a `nanmode` function though?' by a COLLABORATOR of type User on 2022-01-11T17:19:37Z\n'We can't just use `jit` here, because the `axis` and `nan_policy` arguments are static. So, for example, you could use\r\n```python\r\n@partial(jit, static_argnames=['axis', 'nan_policy'])\r\n```' by a COLLABORATOR of type User on 2022-01-13T16:31:33Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 25,
        "num_review_comments": 11,
        "reason_for_closure": "Closed as infeasible: PR attempted to deprecate functionality that was deemed valuable despite differing from NumPy."
    },
    {
        "summary": "Pull Request '9108' titled 'Add lax.searchsorted_p primitive' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2022-01-05T22:28:35Z, and was closed at 2022-07-22T19:14:10Z by a User.\nIt has a body of 'Make `searchsorted` a lax primitive, primarily to allow switching to a more efficient implementation on accelerators.\r\n\r\n~Also addresses one of the TODOs left over from #9107 (correct results for complex inputs in the presence of NaNs)~\r\n\r\nStill a few TODOs/things to think about:\r\n\r\n- ~I previously ran some benchmarks on early versions of these implementations that convinced me this was worth doing; I plan to run some benchmarks on top of this change to better convince us that the backend-specific default implementations are suitable.~ Benchmarks here: https://colab.research.google.com/drive/1kt9dwAFWZIkClOrrKr3zZLmAyWBroxj3?usp=sharing\r\n- ~The form of the `batch_dims` argument (a tuple of lists of batch indices) is the most general approach I could think of, but it may be simpler to specify `batch_dims` as an integer and require then to be at the front of both arrays. The downside is this would in general require transposes within the batch rule.~ Done: I like the simplified version better.\r\n- ~Should the primitive be generalized further to take `N` sorted arrays and `N` query arrays? I can think of some applications, e.g. sparse-sparse matmul implementations.~ No, we'll punt on this for now. Variadic searchsorted can be added in a fully backward-compatible way, if desired.\r\n\r\n~This needs to be rebased against #9127 once that is merged.~'\nPR has comments:\n'I ran some benchmarks on the final implementation: https://colab.research.google.com/drive/1kt9dwAFWZIkClOrrKr3zZLmAyWBroxj3?usp=sharing\r\n\r\nTo be honest, they're not totally convincing: The `scan` approach is clearly optimal on CPU, but neither implementation seems to have a clear edge on GPU or TPU.' by a COLLABORATOR of type User on 2022-01-19T19:21:08Z\n'I guess you also need to benchmark for larger query size. It is true a direct call to `searchsorted` tend to have smaller query size, but for instance `vmap(jnp.digitize, in_axes=(0, None))` can generate a call with large query size.' by a NONE of type User on 2022-01-19T19:34:48Z\n'How large is \"large\" to you?' by a COLLABORATOR of type User on 2022-01-19T19:37:21Z\n'> How large is \"large\" to you?\r\n\r\nIndeed, that depends. Let's just say I ran your benchmark code with `len(query) == 10000` and the result doesn't look good for sort, and even worse for 100000, on the GPU. On the TPU, sort is clearly winning.\r\n![visualization](https://user-images.githubusercontent.com/6129707/150204468-6bdfc2c0-207c-4120-a94f-4461140005f3.png)\r\n![visualization-2](https://user-images.githubusercontent.com/6129707/150204494-e58f7ced-b67a-4e2f-bd53-c90d0895b4d8.png)\r\n\r\n' by a NONE of type User on 2022-01-19T19:40:44Z\n'Thanks - I updated the notebook. Looks like TPU clearly benefits from the sort method as the query grows, but GPU does not.' by a COLLABORATOR of type User on 2022-01-19T20:00:00Z\n'I suspect performance tradeoffs here will likely depend on the size of the compute registers on the chip, which makes me think that achieving optimal performance will probably involve pushing the algorithmic choice here down to the XLA or MLIR compiler backend. That said, given the overall trend in these benchmarks we can probably default to `sort` on TPU and to `scan` on CPU/GPU. What do you think?' by a COLLABORATOR of type User on 2022-01-19T20:10:42Z\n'I think I'm going to close this. The sort-based algorithm is now available in  #11576, and it's not clear to me that the choice of algorithm can be made effectively at the level of a translation rule, so the primitive is probably too heavy-handed a solution for now (unless XLA grows a binary search operator, which would be very cool)' by a COLLABORATOR of type User on 2022-07-22T19:14:05Z\n\nPR has review comments:\n'Will changing `_rank(index)` into `_rank(query)` be helpful, since then `index` and `_rank(query)` can be scheduled in parallel (with data dependency removed)?' by a NONE of type User on 2022-01-18T23:11:26Z\n'Considering the sort based approach, it should be the length of the concatenated array that is checked against `1 << 31` right?' by a NONE of type User on 2022-01-18T23:15:24Z\n'Good call! Done.' by a COLLABORATOR of type User on 2022-01-18T23:39:56Z\n'Good catch - done.' by a COLLABORATOR of type User on 2022-01-19T20:06:50Z\n'At what point does it make sense to switch to parallel search (`(a < v).sum()`, vmapped over `v` is necessary) rather than bothering with a sort?\r\n\r\nI don't know if this needs to be built in to searchsorted, but I'm sure this is faster on accelerators, at least for low dimensional inputs.' by a COLLABORATOR of type User on 2022-07-15T22:31:45Z\n'Good question - I think that's hard to answer in general (different hardware will have different behavior) which is why this is difficult to do well without pushing the logic into the compiler.' by a COLLABORATOR of type User on 2022-07-18T15:58:41Z\n'I gave this a try. To my surprise, in my tests using this for many vmapped interpolations on TPU, it didn't seem to really help.' by a COLLABORATOR of type User on 2022-07-18T16:51:33Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 7,
        "reason_for_closure": "Insufficient information :- PR closed due to maintainers suggestion of not raising an error for complex inputs.  Better implementation might be expected."
    },
    {
        "summary": "Pull Request '8991' titled '[sparse] add sparse.map_specified_elements primitive' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-12-16T19:14:17Z, and was closed at 2023-11-06T19:41:41Z by a User.\nIt has a body of 'This PR addresses #8967 (more context in https://github.com/google/jax/issues/8967#issuecomment-996152905).  The idea here is to provide an API whereby a user can apply a unary function elementwise to the defined entries of a sparse matrix. like the `sparse.todense_p` primitive, it must also accept dense inputs so that it can be used within a function that is transformed by sparsify.\r\n\r\nFor example:\r\n```python\r\n>>> from jax.experimental import sparse\r\n>>> import jax.numpy as jnp\r\n\r\n>>> x = sparse.BCOO.fromdense(jnp.arange(4.0))\r\n>>> print(x.todense())\r\n[0. 1. 2. 3.]\r\n\r\n>>> print(sparse.sparsify(jnp.cos)(x))\r\n---------------------------------------------------------------------------\r\nNotImplementedError: sparse rule for cos is not implemented because it would result in dense output.\r\nIf this is your intent, use sparse.todense() to convert your arguments to dense matrices.\r\n\r\n>>> out = sparse.map_specified_elements(jnp.cos, x)\r\n>>> print(out.todense())\r\n[ 0.          0.5403023  -0.41614684 -0.9899925 ]\r\n```'\nPR has comments:\n'Just an idea, not sure how difficult/useful it would be, but is it possible to have an api looks more like `a.at[:].apply(f)` which could even be introduced for regular arrays.' by a NONE of type User on 2021-12-16T22:34:51Z\n'To me `a.at[:].apply(f)` implies that `f` will be applied to all elements of the array, not just the nonzero or defined elements. What we really want is something like `a.at[a != 0].set(f(a[a != 0])`; I discussed this a bit in https://github.com/google/jax/issues/8967#issuecomment-996152905' by a COLLABORATOR of type User on 2021-12-16T22:44:30Z\n\nPR has review comments:\n'> [...] it must also accept dense inputs so that it can be used within a function that is transformed by sparsify\r\n\r\nI understand that we want to express this operation in \"dense mode\", but do we actually want it to execute and have semantics other than \"error\" in that case? As the comment here notes, the meaning is (possibly subtly) different for dense vs sparse operands.' by a MEMBER of type User on 2023-01-24T00:39:38Z\n'My thought on this was that for dense inputs, every element is specified! So this lets you write one function that applies to both sparse and dense inputs (either with direct sparse inputs or implicitly via `sparsify`). We have similar semantics with `sparse.todense`: dense inputs pass through unchanged. It's the `sparsify` case that really motivates this: our story is that you can write a function `f` that works for dense inputs, and then `sparsify(f)` will work for sparse inputs. This is an example of a computation that doesn't (yet) have any dense corollary, so this primitive should be defined for both sparse and dense inputs.' by a COLLABORATOR of type User on 2023-01-24T14:54:48Z\n'In this case, `sparsify(f)` may no longer be a semantic identity, for example if:\r\n```python\r\nf = partial(sparse.map_specified_elements, jnp.cos)\r\n```\r\nRight? It still won't be a semantic identity if it errs vs. runs too, but I'm offering that that might be better than silently-not-being-the-identity.' by a MEMBER of type User on 2023-01-24T15:27:17Z\n'Note you can still `sparsify` a function that calls `map_specified_elements` and have `map_specified_elements` error on execution. It's a matter of a primitive having a valid sparsification rule while, say, its impl/lowering rule is an error.' by a MEMBER of type User on 2023-01-24T15:29:45Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 4,
        "reason_for_closure": "Implemented :- PR added a new sparse primitive with support for both sparse and dense inputs."
    },
    {
        "summary": "Pull Request '8953' titled 'Prototype of PRNG Key re-use checking - eager checks' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-12-14T23:59:46Z, and was closed at 2024-05-09T22:29:54Z by a User.\nThe PR has labels: pull ready - Ready for copybara import and testing. \nIt has a body of '### Introduction\r\n\r\nThis check is to address a common pitfall in JAX where it's easy to re-use a PRNG key and get the same random bits unintentionally (see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#jax-prng).\r\n\r\nThis PR adds a way to check if you've re-used a key when executing code eagerly. It tracks the consumption of a PRNG key, and will throw an error when a key is used after is has already been consumed. A key is consumed if it is passed into a `jax.random.split` call, or any `jax.random` call which generates random_bits from the key (eg. `jax.random.normal`). The error will tell you where the key was consumed for the first time.\r\n\r\n```\r\nKeyReuseError: Re-used a key, the key was previously used at (...)file.py:function (73)\r\n```\r\n\r\nIf you want to re-use a key, you can broadcast it to make copies of the key. The idea is that re-use should be avoided, unless you are explicit about copying the key. We could brain-storm about additional copying APIs, but currently a `keys = jnp.broadcast_to(key, (2,))` will work as a copy.  \r\n\r\n### Coverage\r\n\r\nThis is an incomplete implementation of this kind of check, and does not cover all cases of re-use. As a way of documenting coverage, I've included currently failing cases in `tests/key_reuse_test.py` (all tests decorated with `@unittest.expectedFailure`). These tests can be used to encode the desired final behavior of this check (eg. `test_vmap_broadcast_key_cross_batch`, should we expect this case to pass the re-use check?)\r\n\r\nWith the current implementation, the check will work if you use a key twice while running eagerly, or if you use a key twice while tracing a function.\r\nFor example:\r\n\r\n```python\r\n# example 1 - eager\r\nkey = random.PRNGKey(123)\r\nwith jax.config.debug_prng_key_reuse(True):\r\n  x0 = random.normal(key, ())\r\n  x1 = random.normal(key, ())  # key re-use error!\r\n\r\n# example 2 - re-use while tracing\r\n@jax.jit\r\ndef f(k):\r\n  x0 = random.normal(k, ())\r\n  x1 = random.normal(k, ())  # key re-use error!\r\n\r\nkey = random.PRNGKey(456)\r\nwith jax.config.debug_prng_key_reuse(True):\r\n  f(k)\r\n```\r\nHowever, re-use checking will **not** work if you need to track re-use across a staged-out boundary (ie. the key is _not_ re-used while tracing). These cases will need to be handled in a follow-up:\r\n```python\r\n@jax.jit\r\ndef f(k):\r\n  x0 = random.normal(k, ())\r\n  return x0\r\n\r\nkey = random.PRNGKey(123)\r\nwith jax.config.debug_prng_key_reuse(True):\r\n  f(k)\r\n  f(k)  # no error!\r\n```\r\n### Implementation details\r\n\r\nSome details worth highlighting:\r\n- `PRNGKeyArray._split` consumes itself, but `PRNGKeyArray._random_bits` does not. For `random_bits` the `consume` call is lifted to the wrapper functions in `jax.random`. This is because of the technicality that all `jax.random` calls are internally jitted. Because this PR does not yet handle carrying the consumption flag across the `jit` boundary, the consumption needs to happen outside of that boundary.\r\n- The check is behind a flag called `jax_debug_prng_key_reuse`, which I see as analogous to a flag like `jax_debug_nans`. However, it also needs the new PRNGKeyArray type (enabled through `jax_enable_custom_prng`) to work.\r\n-  If we allow for using the same key on both branches of a `cond` (because semantically only one branch is ever taken), then there is a false positive if those two branches _close over_ the key (see `test_cond_uses_on_both_branches_closed`). This seems undesirable, but maybe we should discourage closing over keys in any case,  or maybe we want to always conservatively reject using one key on both branches of the cond. This is still an open question for me.\r\n\r\n---- \r\nHappy to discuss any of the assumptions made above, but also happy to land this preliminary check and refine in follow-up PRs while it's behind a flag. I think the most interesting point of discussion is the cases in `tests/key_reuse_test.py`, and what the expected behavior for them is (eg. wrt to scan/vmap/slicing into a key array).'\nPR has comments:\n'Closing, since this is superseded by the work in `jax.experimental.key_reuse`' by a COLLABORATOR of type User on 2024-05-09T22:29:54Z\n\nPR has review comments:\n'this need for a double context manager is not as nice as I would have liked, and makes me think maybe I should create this documentation _after_ this check is less experimental, WDYT?' by a CONTRIBUTOR of type User on 2021-12-15T19:24:43Z\n'How about having `debug_prng_key_reuse` require `enable_custom_prng`? It can either err if it isn't already enabled, or enable it and issue a warning that it has done so. That might be convenient anyway, but it would also allow this documentation to be written today the way that we expect it to look in the longer term.' by a MEMBER of type User on 2022-01-05T20:51:01Z\n'Seeing all the use of `key.consume`, I am wondering if the code ends up more readable with a slightly different API where `key.consume()` is the identity function, and you write `return _uniform(key.consume(), shape, ...)`' by a COLLABORATOR of type User on 2022-07-18T14:52:08Z\n'If it does not complicate the checking too much, I think that it is good to allow re-use on different branches of a conditional. ' by a COLLABORATOR of type User on 2022-07-18T14:57:57Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Superseded by other work/implementation."
    },
    {
        "summary": "Pull Request '8699' titled 'add jaxpr parts of simple effect types' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-11-25T06:53:40Z, and was closed at 2023-11-06T12:54:12Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'TODO:\r\n- [ ] switch from boolean to set?\r\n- [ ] add tests\r\n- [ ] write PR message'\nPR has comments:\n'> I am wondering what is the semantics of the effects flag. Is the following true:\r\n\r\nYes those sound right to me.\r\n\r\n> If the above is true, then one can think of Jaxpr.effects also as a property that scans the eqns (maybe this implementation would actually simplify the code because you do not need to propagate the effects flag). Maybe then it is more important to keep an effects flag for eqn.\r\n\r\nThat could work.\r\n\r\nAt a high level, I'm trying to think of effects as encoded on the arrow types. A JaxprType (which we don't have an explicit object for in main JAX, though there is one in Autodidax), being an arrow type, would thus include what effects it has (in addition to the input and output types). Likewise a parameterized primitive (e.g. a cond_p with particular branch jaxprs) would have such a type.\r\n\r\nFor the purpose of efficiently querying and checking types, we might want to add explicit annotations to the jaxpr syntax (i.e. data structure). For that purpose it seems we could arguably put these annotations wherever we find convenient. I first put them on jaxprs so we wouldn't have to recurse into sub-jaxprs when querying the type of a jaxpr. But having them on eqns would serve that purpose also, and has a nice correspondence to how they're computed (i.e. to abstract eval rules).\r\n\r\nI'll give that a shot and see what happens!\r\n\r\n> Nit: wdyt about renaming effects to has_effects\r\n\r\nLove it!' by a COLLABORATOR of type User on 2021-12-01T22:58:13Z\n'Ah, I remembered why I called it `effects` and not `has_effects`: we expect to generalize this to a set of effects rather than just a boolean. Then the `has_effects` name doesn't seem ideal.' by a COLLABORATOR of type User on 2021-12-01T23:41:00Z\n'Is this PR subsumed by the effects stuff that @sharadmv landed?' by a COLLABORATOR of type User on 2023-11-06T12:53:32Z\n'Certainly!' by a COLLABORATOR of type User on 2023-11-06T12:54:10Z\n\nPR has review comments:\n'Shouldn't the `effects` depend on the `eqns`?' by a COLLABORATOR of type User on 2021-11-29T09:49:11Z\n'I think that I am misunderstanding the semantics of the `effects` flag. I thought it signals that at least one of the nested `eqns` has effects. In that case it should be possible for only one branch of a conditional of have effects, (and then the entire conditional has effects). ' by a COLLABORATOR of type User on 2021-11-29T09:52:52Z\n'Shouldn't `effects` here depend on `new_body_eqns`? Eventually we will want the host_callback primitives to be declared as effectful, in which case `effects` here will be `True`.' by a COLLABORATOR of type User on 2021-11-29T09:56:12Z\n'See the above suggestion to keep the `effects` flag on the `eqns`, then we don't need to worry here about what value to set the `effects` for the Jaxpr.' by a COLLABORATOR of type User on 2021-11-29T10:11:05Z\n'I hardcoded this here based on an idea (from the \"IO and RNG\" effects doc draft) that linear jaxprs would be effect-free. That is, we wrote signatures like this:\r\n\r\n```\r\nio_jvp : (a -> {IO} b) -> (a, T a) -> {IO} (b, T b)\r\nio_lin : (a -> {IO} b) -> a -> {IO} (b, T a -o T b)\r\nio_vjp : (a -> {IO} b) -> a -> {IO} (b, T b -o T a)\r\n```\r\n\r\nHowever, while that's likely true for some effects (particularly the ones considered in that IO and RNG doc), it may not be true for all. In particular, we might want debug prints in the backward pass!\r\n\r\nGiven we're now focused on effects which we want to include in linear jaxprs, I will indeed remove this hardcoding!' by a COLLABORATOR of type User on 2021-12-01T22:42:21Z\n'I'm thinking of the effect as part of the type, and we still need all branches to have the same type. That just means that if one branch of a cond has an effect, we need to lift all the other branches to have that effect. Hence why this checking rule requires all branches to have the same type.\r\n\r\nWDYT?' by a COLLABORATOR of type User on 2021-12-01T22:44:23Z\n'I was thinking eventually we may not want to keep the host_callback primitives, replacing them with new primitives (rather than marking them effectful).\r\n\r\nI hardcoded 'False' here just as a placeholder encoding current behavior, but I should've left a TODO. I can update it now...' by a COLLABORATOR of type User on 2021-12-01T22:48:04Z\n'That is reasonable, but then what is the invariant? It cannot be that one of the eqns has effects, unless we are going to add some eqn that has effects as part of the lifting. \r\n\r\nI think that it is useful to have a mental model when when a jaxpr has the `effects` flag, and if we cannot find one that that accounts for sibling branches having the same effects, then perhaps we can skip the lifting.' by a COLLABORATOR of type User on 2021-12-06T12:33:07Z\n'If we replace the host_callback primitives, we will replace them with with effectful primitives. This particular code will not be needed anymore, so it does not matter.\r\n\r\nAs we make progress, my plan was to refactor the host_callback on top of the new mechanisms rather than removing it. At the very least we should keep the tests. But in general we may want to slowly replace the API usage for our users. ' by a COLLABORATOR of type User on 2021-12-06T12:36:26Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 9,
        "reason_for_closure": "PR subsumed by other work that has already landed."
    },
    {
        "summary": "Pull Request '8514' titled 'Update bcoo_dot_general GPU translation rule.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-11-11T07:35:34Z, and was closed at 2021-12-16T00:17:31Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'Co-authored-by: vanderplas@google.com'\nPR has comments:\n'Hi @tlu7 - I saw your responses to some of the suggestions, but not the updates. Do you have some commits to push to the branch?' by a COLLABORATOR of type User on 2021-11-11T22:49:34Z\n'> Hi @tlu7 - I saw your responses to some of the suggestions, but not the updates. Do you have some commits to push to the branch?\r\n\r\nThanks for checking! Just commit the changes. PTAL.' by a CONTRIBUTOR of type User on 2021-11-12T02:31:16Z\n'Thanks for the review! PTAL.\r\n@jakevdp ' by a CONTRIBUTOR of type User on 2021-11-12T05:01:34Z\n'> A couple small things, then we can trigger the GPU CI jobs\r\n\r\nThanks again for the comments!' by a CONTRIBUTOR of type User on 2021-11-12T05:21:39Z\n'The GPU tests are failing with an illegal memory address error. I'm not certain what may be causing it.' by a COLLABORATOR of type User on 2021-11-12T13:05:27Z\n'> The GPU tests are failing with an illegal memory address error. I'm not certain what may be causing it.\r\n\r\nThe failure occurs in the case where `n_sparse = 1`.  The reason for extending the 1D lhs to 2D is that in `cusparse.py`, both `coo_matvec` and `coo_matmat` require `rows, cols = shape`.  \r\nhttps://github.dev/google/jax/blob/8381a4ade0bdff7869e95e1c0ab6b326bf7aca3a/jaxlib/cusparse.py#L276-L277\r\n\r\nI think instead of going from 1D to 2D and then back to 1D , we should add  `coo_vecmat` in `cusparse.py`. This will be helpful for cases where n_batch and n_dense are not zeros.\r\n\r\nWhat do you think?\r\n' by a CONTRIBUTOR of type User on 2021-11-12T15:42:47Z\n'(summarizing offline convo: cusparse doesn't have a sparse vec-mat operation, but it does have a [sparse vec-vec](https://docs.nvidia.com/cuda/cusparse/index.html#cusparse-generic-function-spvv) operation. So for the latter it would probably make sense to expose it in `cusparse.py`, but for the former we'll have to do reshapes at some level anyway. At this point I think the routines in `cusparse.py` should essentially have a 1-to-1 mapping to routines provided by cusparse)' by a COLLABORATOR of type User on 2021-11-12T16:25:06Z\n\nPR has review comments:\n'I would call this something like `_bcoo_dot_general_default_translation_rule`, and then refer to it again in the `xla.register_translation(bcoo_dot_general_p, ...)` line below, rather than creating a second copy of the same lowered function.' by a COLLABORATOR of type User on 2021-11-11T13:30:15Z\n'It looks like this function is not being referenced anywhere. We'll need to add it to the device-specific translation table, in a similar manner to this: https://github.com/google/jax/blob/7f3609f039513360335b2f11ec16ffc2a9519dd0/jax/experimental/sparse/ops.py#L577-L579' by a COLLABORATOR of type User on 2021-11-11T13:32:14Z\n'The shape of `col` here is `(nse, 1)`. Does it need to be collapsed?' by a COLLABORATOR of type User on 2021-11-11T13:34:52Z\n'another thing to check here: the cusparse lowering requires `len(lhs_contract) == 1`' by a COLLABORATOR of type User on 2021-11-11T13:38:18Z\n'It wouldn't hurt I think to cover these in a new GPU-only test method specifically to test cases that should be lowered to GPU. If something changes with cusparse and leads to those failing, it would be a much clearer signal of the root issue.' by a COLLABORATOR of type User on 2021-11-11T13:42:21Z\n'Design-wise, it occurred to me that it might be cleaner to put the cusparse logic in another function, so then this GPU translation rule would just contain something like\r\n```\r\nif <conditions>:\r\n  return _bcoo_dot_general_cuda_translation_rule(...)\r\nelse:\r\n  return _bcoo_dot_general_default_translation_rule(...)\r\n```\r\nWhat do you think?' by a COLLABORATOR of type User on 2021-11-11T13:47:36Z\n'Done.' by a CONTRIBUTOR of type User on 2021-11-11T21:44:27Z\n'Done.' by a CONTRIBUTOR of type User on 2021-11-11T21:44:38Z\n'Fall back to default rule when `len(lhs_contract) != 1` is true.' by a CONTRIBUTOR of type User on 2021-11-11T21:55:40Z\n'Separate the test cases into a standalone test method. Is there a way to only pick up the method on GPU?' by a CONTRIBUTOR of type User on 2021-11-11T23:30:51Z\n'Yes, you can do something similar to this: https://github.com/google/jax/blob/1388dbe25c3b1c003955a9f4743dca72668d2d8d/tests/sparse_test.py#L135' by a COLLABORATOR of type User on 2021-11-12T04:03:36Z\n'nit: match indentation' by a COLLABORATOR of type User on 2021-11-12T04:07:06Z\n'we could probably replace the second one with `self._CompileAndCheck(f_sparse, args_maker)` here\r\n\r\nThis was causing issues with the default impl rule, but I suspect it will be fine in this case.' by a COLLABORATOR of type User on 2021-11-12T04:07:59Z\n'Under what circumstance is `len(lhs_shape) != 1`? I think `n_dense == n_batch == 0` and `n_sparse == 1` is sufficient to guarantee that `len(lhs_shape) == `.' by a COLLABORATOR of type User on 2021-11-12T04:10:51Z\n'It looks like the cuda rule as implemented  handles `n_sparse == 1` as well. Also, I think it requires `rhs_aval.ndim in [1, 2]`?' by a COLLABORATOR of type User on 2021-11-12T04:17:40Z\n'Probably we can remove this `n_dense` logic and just set it to zero, because GPU lowering is not applicable for `n_dense != 0`' by a COLLABORATOR of type User on 2021-11-12T04:19:10Z\n'Maybe `test_bcoo_dot_general_cusparse`?' by a COLLABORATOR of type User on 2021-11-12T04:19:45Z\n'minor: it's slightly cleaner I think to do `props = _validate_bcoo_indices(...)` and then refer to `props.n_sparse` below.' by a COLLABORATOR of type User on 2021-11-12T04:20:56Z\n'remove `n_dense` from argument list' by a COLLABORATOR of type User on 2021-11-12T05:11:34Z\n'remove `n_dense=n_dense` here' by a COLLABORATOR of type User on 2021-11-12T05:11:41Z\n'The tests are failing in this block (the lhs matrix is 1-dimensional). I think this should be `lhs_transpose = False`, yes? We've constructed a `[1, N]` matrix, so it does not need to be transposed. ' by a COLLABORATOR of type User on 2021-11-12T13:09:42Z\n'I would add `assert lhs_contracting == [0]` here just to be sure the input is valid.' by a COLLABORATOR of type User on 2021-11-12T13:10:49Z\n'Also, in this case the cusparse matmat routine will return a `[1, M]` matrix, but we want it to return a length-M vector. So we'll need to do some post-processing of the result if `lhs.ndim == 1`.' by a COLLABORATOR of type User on 2021-11-12T13:12:47Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 23,
        "reason_for_closure": "Implemented :- PR closed as the fix was deemed unnecessary after further discussion."
    },
    {
        "summary": "Pull Request '8504' titled 'jax.experimental.tree_math' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-11-10T07:08:16Z, and was closed at 2021-12-29T00:57:44Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'Fixes #1012\r\n\r\nThe idea behind this PR is to make it easy to implement numerical algorithms that work on pytrees, by transforming code written to handle vectors.\r\n\r\nFor example, we could implement an ODE solver that supports pytrees with something like:\r\n```python\r\nimport functools\r\nfrom jax import lax\r\nfrom jax.experimental import tree_math\r\n\r\n@functools.partial(tree_math.wrap, vector_argnums=(1,))\r\ndef odeint_midpoint(f, y0, dt, steps):\r\n    f = tree_math.unwrap(f, vector_argnums=(0,))\r\n    def step_fun(i, y):\r\n        t = i * dt\r\n        return y + dt * f(y + dt/2 * f(y, t), t + dt/2)\r\n    return lax.fori_loop(0, num_steps, step_fun, y0)\r\n```\r\nAside from `wrap` and `unwrap`, this is exactly how you would write a simple ODE solver in JAX without support for PyTrees. We currently [do something very similar](https://github.com/google/jax-cfd/blob/a92862fb757d122e7c5eee23a3b783997a2aeafe/jax_cfd/spectral/time_stepping.py#L92) for implementing ODE solvers in JAX-CFD. The tests for this PR include a rewriten version of `jax.scipy.sparse.linalg.cg`, which is also much more readable than what we currently have in JAX.\r\n\r\nThe idea here is a simpler alternative to #3263, based on a custom pytree (`tree_math.Vector`) rather than introducing a new final-style transformation:\r\n- The upside of this approach is that writing a custom pytree is easy and entirely decoupled from JAX's other transformations. So we get support for JAX transformations (e.g., `grad`/`jit`/`vmap`) and control flow primitives (e.g., `while_loop`) for free.\r\n- The downside is that it won't be possible to use standard `jax.numpy` functions on `Vector` objects, unless we make `jax.numpy` aware of `tree_math` (ala #8381). Instead, I've added a few specialized helper functions like `where` and `maximum`. For now, this seems like an acceptable trade-off, given that the core use-case for tree math is to\r\nmake it easy to write new algorithms from scientific computing, for which infix arithmetic is important enough that it should easily justify minor deviations from standard `jax.numpy`.\r\n\r\nThe other change from #3263 is that I've only written a simple \"Vector\" class for now rather than making an attempt to support arbitrary higher dimensional arrays. This also considerably simplifies the implementation. In the future, we probably will also want to add a \"Matrix\" class, for methods that need to keep track of multiple vectors (e.g., ODE solvers, GMRES, L-BFGS, Lanczos, etc). I don't think we'll need support for representing pytrees as higher dimensional arrays, which don't come up very often in fundamental numerical algorithms.'\nPR has comments:\n'I think this is ready for review!\r\n\r\nDesign aspects that I'm not entirely sure about:\r\n\r\n- Is it a good idea to expose `tree_math.Vector` as a public API? I think this would be convenient (especially, e.g., for type checking), but this might have backwards compatibility implications if we ever switch to JAX transformation based approaches (constructors for tracer objects are not external APIs).\r\n- Are `wrap`/`unwrap` the right names for the function transforms? And should their names perhaps be flipped? Other options: `tree_wrap`/`tree_unwrap`, `vectorize`/`unvectorize`, `tree_vectorize`/`tree_unvectorize`, `map`/`unmap` (similar to `vmap`, but possibly confusing to the pre-existing `tree_utils.tree_map`), `vector_to_tree_func`/`tree_to_vector_func`, `autotree`/`undotree` ...' by a COLLABORATOR of type User on 2021-11-10T21:06:23Z\n'This is awesome.  Should we favor writing algorithms for trees then and pass them to `tree_math`?' by a CONTRIBUTOR of type User on 2021-11-11T00:53:13Z\n'> Should we favor writing algorithms for trees then and pass them to `tree_math`?\r\n\r\nBy \"favor writing algorithms for trees\" did you mean to say \"favor writing algorithms for vectors\"?\r\n\r\nWriting algorithms for vectors would definitely be my favored approach. Code for vectors is much more readable than code for pytrees (mostly due to support for infix arithmetic). When transformed with `tree_math` it will work for pytrees, too, and under `jit` performance should be identical to writing the transformations by hand.' by a COLLABORATOR of type User on 2021-11-11T01:14:55Z\n'> By \"favor writing algorithms for trees\" did you mean to say \"favor writing algorithms for vectors\"?\r\n\r\nYes, sorry.\r\n\r\n> Writing algorithms for vectors would definitely be my favored approach\r\n\r\nSounds great.  This feature is a fantastic addition in my opinion 😄 ' by a CONTRIBUTOR of type User on 2021-11-11T01:31:18Z\n'For a lot of algorithms one would also need an implementation of `dot` (or `vdot`) for `Vector`s. There is some ambiguity here concerning the axis of the tensors of the Vector but it would be a great addition to `matmul`.' by a CONTRIBUTOR of type User on 2021-11-19T21:02:45Z\n'> For a lot of algorithms one would also need an implementation of `dot` (or `vdot`) for `Vector`s. There is some ambiguity here concerning the axis of the tensors of the Vector but it would be a great addition to `matmul`.\r\n\r\nThe version of \"matmul\" that I implemented only work on vectors, so it's exactly the same as `np.dot` (\"tree matrices\" hasn't been implemented yet). So we could either include an alias `dot = matmul` or encourage using the infix notation `u @ v`.' by a COLLABORATOR of type User on 2021-11-19T21:47:41Z\n'Sorry for the noise! I was too hasty when looking at the implementation :|\r\nI think an alias would be nice but I don't have any strong feelings about any of the two options.' by a CONTRIBUTOR of type User on 2021-11-19T22:21:16Z\n'I hope this comment is more qualified than my last one :|\r\n\r\nI basically built a very similar class a while ago and happened to find the following convenience methods quite useful. Both method names also exist in numpy and these functions are straight-forward extensions of numpy-methods to pytreees.\r\n```python\r\ndef size(tree, axis: Optional[int] = None) -> Union[int, jnp.ndarray]:\r\n    if axis is not None:\r\n        raise TypeError(\"axis of an arbitrary tree is ill defined\")\r\n    sizes = tree_map(jnp.size, tree)\r\n    return tree_reduce(jnp.add, sizes)\r\n\r\n\r\ndef norm(tree, ord, *, ravel: bool = True):\r\n    from jax.numpy.linalg import norm\r\n\r\n    if ravel:\r\n\r\n        def el_norm(x):\r\n            return jnp.abs(x) if jnp.ndim(x) == 0 else norm(x.ravel(), ord=ord)\r\n    else:\r\n\r\n        def el_norm(x):\r\n            return jnp.abs(x) if jnp.ndim(x) == 0 else norm(x, ord=ord)\r\n\r\n    return norm(tree_leaves(tree_map(el_norm, tree)), ord=ord)\r\n```\r\n\r\nNote, `size` does the same thing as `Vector.__len__` (using `np.size(x) = np.prod(np.shape(x))`) but I prefer to use the former since `assert size(Vector(x)) == size(x)` would hold for any tensor while `assert len(Vector(x)) != len(x)`. In your implementation this is of course intentional. However, I find that I almost never want the length of the first axis and instead need the number of elements in the tensor so I use `size` for tensors and pytrees alike.\r\n\r\nOn a related note, I also overload `__getitem__` and `__contains__`. I use these helper methods to e.g. access elements of a math-enabled dictionary but really anything would work as long as `Vector._tree` implements these methods.\r\n\r\nSurely not all helper methods are worth having upstream, but since JAX is experimenting with enabling numerics for pytrees, I though I would share these. I assume overloading `__getitem__` and `__contains__` is a bit too specific to my use case but maybe others might find these methods useful as well.\r\n\r\nEDIT: `Vector.dot` is also one of those methods which I found convenient to have. It as well is part of the numpy.ndarray API and thus works for tensors and `Vector`s alike.' by a CONTRIBUTOR of type User on 2021-11-23T15:30:42Z\n'I'll add a `size` property and `dot` method, those are basically aliases for existing functionality.\r\n\r\nI'm not sure `__getitem__` makes sense. Indexing like `x[y]` on a vector `x` could only work when the indexing object `y` is a Python integer:\r\n- There's no unambiguous way to know the underlying pytree structure for slicing like `x[:3]`.\r\n- Indexing with a JAX tracer doesn't work, because we need to know which array is being indexed from Python to generate do efficient indexing. This means we couldn't looping over an index variable, e.g., with `lax.fori_loop`.\r\n- This leaves indexing like `x[0]` or `x[-1]`. I can see this being handy for debugging, but I can also see it misleadingly suggesting that general purpose indexing work.\r\n\r\nI'm not sure what you're using `__contains__` for, but generally that overload does not make sense on multi-dimensional arrays. It also needs to return a Python builtin `bool`, which is rather restrictive. JAX does not implement it on its other array types and so I doubt it makes sense here, either.' by a COLLABORATOR of type User on 2021-11-23T16:19:24Z\n'Great, thanks!\r\n\r\nIs `norm` also of interest or is it already too specific? It comes in handy for implementing convergence criteria in some minimization schemes which use e.g. the 1-norm or allow the user to set the order of the norm freely.\r\n\r\nSure, in general slicing does not work nor would a loop over all elements work efficiently. However, I don't use it for that. Most of the thinks I wrap end up being nested dictionaries of some sort and I forward `__contains__` and `__getitem__` on to the underlying object. Thus, `math_enabled_dict[some_key]` still accesses the underlying dictionary and yields whatever is at `some_key`.\r\n\r\nMy philosophy behind this decision was that whatever I wrap in a math-enabled pytree should still behave and act exactly as the un-wrapped object with the only constrain being that whatever method I implemented must also work for numpy arrays. For my implementation this means that `Vector(some_jnp_array)` behaves exactly the same as its un-wrapper counterpart. This might be overly specific to my use-case and might not fit the here-proposed `Vector` class. If so, feel free to disregard my comments on this topic.\r\n\r\n\r\n>  JAX does not implement it on its other array types and so I doubt it makes sense here, either.\r\n\r\nI am not sure what you mean by this; `assert 3 in jnp.arange(4)`  does work.' by a CONTRIBUTOR of type User on 2021-11-23T17:31:00Z\n'> Is `norm` also of interest or is it already too specific? It comes in handy for implementing convergence criteria in some minimization schemes which use e.g. the 1-norm or allow the user to set the order of the norm freely.\r\n\r\n`norm` is not a method on numpy arrays. My current approach is that the functionality of `tree_math.Vector` should be a strict subset of the functionality of normal JAX/NumPy arrays.\r\n\r\nInstead you could use infix arithmetic like `(x @ x) ** 0.5`.\r\n\r\n> Sure, in general slicing does not work nor would a loop over all elements work efficiently. However, I don't use it for that. Most of the thinks I wrap end up being nested dictionaries of some sort and I forward `__contains__` and `__getitem__` on to the underlying object. Thus, `math_enabled_dict[some_key]` still accesses the underlying dictionary and yields whatever is at `some_key`.\r\n\r\nAh, I see. I'd like tree_math to be entirely explicit, and _strictly_ allow for manipulating pytrees as if they were 1D NumPy arrays. If you want to do this sort of access, you can pull out the underlying Pytree with the `.tree` attribute and then make a new `Vector`.\r\n\r\n> > JAX does not implement it on its other array types and so I doubt it makes sense here, either.\r\n> \r\n> I am not sure what you mean by this; `assert 3 in jnp.arange(4)` does work.\r\n\r\nThis is Python's fallback implementation of `__contains__`, via iteration with `__iter__`: https://docs.python.org/3/reference/datamodel.html#object.__contains__\r\n\r\nWe could certainly support `__iter__` on `tree_math.Vector`. It won't be very efficient, but I guess not much worse than iterating over normal JAX arrays.' by a COLLABORATOR of type User on 2021-11-23T17:43:19Z\n'> `norm` is not a method on numpy arrays. My current approach is that the functionality of `tree_math.Vector` should be a strict subset of the functionality of normal JAX/NumPy arrays.\r\n\r\nRight, I got that from your implementation. Just to be clear, my proposal was to add `norm` as an additional function to `experimental.tree_math` in the same way `matmul` is exposed as function as well.\r\n\r\nI think you also misunderstood this aspect of the proposed `size` function. Of course having it as a method of `Vector` works too but I think it would also be a great addition to `experimental.tree_math`. Similar to `matmul`, `size` and `norm` both make sense for general pytrees.\r\n\r\n> Instead you could use infix arithmetic like (x @ x) ** 0.5.\r\n\r\nSure, I can of course implement any desired p-norm by hand but I find `norm(x, ord=p)` to be more convenient.\r\n\r\n> This is Python's fallback implementation of __contains__, via iteration with __iter__: https://docs.python.org/3/reference/datamodel.html#object.__contains__\r\n\r\nAh, I see. Thank for the explanation!' by a CONTRIBUTOR of type User on 2021-11-23T18:16:06Z\n'> Similar to `matmul`, `size` and `norm` both make sense for general pytrees.\r\n\r\nJust to be entirely clear, the functions implemented in tree_math don't work for general pytrees -- they only work for explicit `tree_math.Vector` objects. The only reason why I created `matmul` is that it gives a way to explicitly set the `precision` argument, which you can't do with infix notation. In general, I would rather encourage using methods than functions, because the methods will let you write portable code that also works on regular JAX arrays.\r\n\r\n`norm` seems fine to add, but let's save that for a follow-on PR after this gets merged.' by a COLLABORATOR of type User on 2021-11-23T18:29:49Z\n'Ok. Thank you for all the explanations and of course for implementing it in the first place :)' by a CONTRIBUTOR of type User on 2021-11-23T18:47:04Z\n'thanks!' by a CONTRIBUTOR of type User on 2021-12-01T17:53:00Z\n'We decided to implement this as a separate library outside of JAX for now. Please take a look https://github.com/google/tree-math/ !' by a COLLABORATOR of type User on 2021-12-29T00:57:40Z\n\nPR has review comments:\n'I am asking just out of curiosity; is there a specific reason why you prefer `prod(jnp.shape(v))` over `jnp.size`?' by a CONTRIBUTOR of type User on 2021-11-23T18:18:14Z\n'No, they are entirely equivalent' by a COLLABORATOR of type User on 2021-11-23T18:21:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 16,
        "num_review_comments": 2,
        "reason_for_closure": "Implemented Elsewhere:- Functionality moved to a separate library outside JAX."
    },
    {
        "summary": "Pull Request '8290' titled 'WIP: named call context manager' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-10-19T19:41:21Z, and was closed at 2022-02-22T23:43:30Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'This draft PR will track the progress on the WIP named call context manager implementation. \r\n\r\nFeature completeness:\r\n- [x] Simple context manager usage\r\n  - [x] Inline\r\n\r\n    <details>\r\n      <summary>Example</summary>\r\n  \r\n      ```python\r\n      def f(x):\r\n        with extend_name_stack('foo'):\r\n          y = x * 2.\r\n        with extend_name_stack('bar'):\r\n          with extend_name_stack('baz'):\r\n            return jnp.square(y) + 3.\r\n      ```\r\n      Expected jaxpr:\r\n      ```\r\n      { lambda ; a:f32[]. let\r\n          b:f32[] = mul a 2.0          # [foo]\r\n          c:f32[] = integer_pow[y=2] b # [bar/baz]\r\n          d:f32[] = add c 3.0          # [bar/baz]\r\n        in (d,) }\r\n      ```\r\n    </details>\r\n  - [x] Usage with call primitives\r\n    <details>\r\n      <summary>Example</summary>\r\n  \r\n      ```python\r\n      @functools.partial(jax.named_call, name='baz')\r\n      @jax.jit\r\n      def f(x):\r\n        with extend_name_stack('foo'):\r\n          return jax.named_call(lambda x: jnp.square(x * 2.) + 3., name='bar')(x)\r\n      ```\r\n      Expected jaxpr: \r\n      ```\r\n      { lambda ; a:f32[]. let\r\n          b:f32[] = xla_call[                  # [baz]\r\n            backend=None\r\n            call_jaxpr={ lambda ; c:f32[]. let\r\n                d:f32[] = mul c 2.0            # [foo/bar]\r\n                e:f32[] = integer_pow[y=2] d   # [foo/bar]\r\n                f:f32[] = add e 3.0            # [foo/bar]\r\n              in (f,) }\r\n            device=None\r\n            donated_invars=(False,)\r\n            inline=False\r\n            name=f\r\n          ] a\r\n    in (b,) }\r\n      ```\r\n    </details>\r\n  \r\n- [x] Transformations\r\n  - [x] vmap\r\n    <details>\r\n      <summary>Example</summary>\r\n  \r\n      ```python\r\n      @jax.vmap\r\n      def f(x):\r\n        with extend_name_stack('foo'):\r\n          return jnp.square(x * 2.) + 3.\r\n      ```\r\n      Expected jaxpr: \r\n      ```\r\n      { lambda ; a:f32[3]. let\r\n          b:f32[3] = mul a 2.0          # [vmap(foo)]\r\n          c:f32[3] = integer_pow[y=2] b # [vmap(foo)]\r\n          d:f32[3] = add c 3.0          # [vmap(foo)]\r\n        in (d,) }\r\n      ```\r\n      </details>\r\n\r\n  - [x] JVP\r\n    <details>\r\n      <summary>Example</summary>\r\n  \r\n      ```python\r\n      @jax.named_call\r\n      def f(x):\r\n        with extend_name_stack('bar'):\r\n          y = jax.jit(lambda x: x * jnp.ones((5, 2)))(x)\r\n        with extend_name_stack('baz'):\r\n          return jnp.sum(y)\r\n\r\n     (lambda x, t: jax.jvp(f, (x,), (t,)))(jnp.ones((5, 2)), jnp.ones((5, 2)))\r\n      ```\r\n      Expected jaxpr: \r\n      ```\r\n      { lambda ; a:f32[5,2] b:f32[5,2]. let\r\n          c:f32[5,2] = broadcast_in_dim[broadcast_dimensions=() shape=(5, 2)] 1.0 # [jvp(f)/jvp(bar)]\r\n          d:f32[5,2] = mul a c                                                    # [jvp(f)/jvp(bar)]\r\n          e:f32[5,2] = mul b c                                                    # [jvp(f)/jvp(bar)]\r\n          f:f32[] = reduce_sum[axes=(0, 1)] d                                     # [jvp(f)/jvp(baz)]\r\n          g:f32[] = reduce_sum[axes=(0, 1)] e                                     # [jvp(f)/jvp(baz)]\r\n        in (f, g) }\r\n      ```\r\n      </details>\r\n  - [x] grad\r\n    <details>\r\n      <summary>Example</summary>\r\n  \r\n      ```python\r\n      @functools.partial(jax.vmap, out_axes=None)\r\n      @jax.grad\r\n      @functools.partial(jax.named_call, name='foo')\r\n      def f(x):\r\n        with extend_name_stack('bar'):\r\n          y = x * jnp.ones((5, 2))\r\n          return jnp.sum(y)\r\n      ```\r\n      Expected jaxpr: \r\n      ```\r\n      { lambda ; a:f32[3,5,2]. let\r\n          b:f32[5,2] = broadcast_in_dim[broadcast_dimensions=() shape=(5, 2)] 1.0        # [vmap(jvp(foo))/vmap(jvp(bar))]\r\n          c:f32[3,5,2] = broadcast_in_dim[broadcast_dimensions=(1, 2) shape=(3, 5, 2)] b # [vmap(jvp(foo))/vmap(jvp(bar))]\r\n          d:f32[3,5,2] = mul a c                                                         # [vmap(jvp(foo))/vmap(jvp(bar))]\r\n          _:* = reduce_sum[axes=(1, 2)] d                                                # [vmap(jvp(foo))/vmap(jvp(bar))]\r\n          e:f32[5,2] = broadcast_in_dim[broadcast_dimensions=() shape=(5, 2)] 1.0        # [vmap(transpose(jvp(foo)))/vmap(transpose(jvp(bar)))]\r\n          f:f32[5,2] = mul e b                                                           # [vmap(transpose(jvp(foo)))/vmap(transpose(jvp(bar)))]\r\n        in (f,) }\r\n      ```\r\n      </details>\r\n\r\n  - [x] pmap\r\n  \r\n- [x]  Control flow\r\n  - [x] while_loop\r\n  - [x] scan\r\n  - [x] cond\r\n- [ ] Test suite'\nPR has comments:\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F8290) for more info**.\n\n<!-- need_author_cla -->' by a NONE of type Bot on 2021-10-19T19:41:29Z\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F8290) for more info**.\n\n<!-- need_author_cla -->' by a NONE of type Bot on 2021-10-19T19:53:15Z\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F8290) for more info**.\n\n<!-- need_author_cla -->' by a NONE of type Bot on 2021-10-19T23:26:15Z\n\nPR has review comments:\n'```suggestion\r\n           nullcontext())\r\n```' by a COLLABORATOR of type User on 2021-11-19T17:03:39Z\n'```suggestion\r\n           nullcontext())\r\n```' by a COLLABORATOR of type User on 2021-11-19T17:04:14Z\n'```suggestion\r\n    default=\"auto\",\r\n```' by a COLLABORATOR of type User on 2021-11-19T17:05:52Z\n'Unintentionally left in?' by a COLLABORATOR of type User on 2021-11-19T17:07:09Z\n'Put back on one line?' by a COLLABORATOR of type User on 2021-11-19T17:07:47Z\n'What's the logic to this pattern? I wonder if we could abstract it into a well-named function, since it's done a couple times.' by a COLLABORATOR of type User on 2021-11-19T17:08:36Z\n'Maybe `HigherOrderPrimitiveScope`?' by a COLLABORATOR of type User on 2021-11-19T17:09:57Z\n'Some golf: instead of\r\n\r\n```python\r\n    scopes = tuple(map(Scope, name))\r\n    return NameStack(self.stack + scopes)\r\n```\r\n\r\nwe could write\r\n\r\n```python\r\nreturn NameStack((*self.stack, *map(Scope, name)))\r\n```' by a COLLABORATOR of type User on 2021-11-19T17:11:04Z\n'What's the deal here? intended to leave in?' by a COLLABORATOR of type User on 2021-11-19T17:12:25Z\n'```suggestion\r\n    pp.text(\" \") + pp_vars(eqn.invars, context)\r\n```' by a COLLABORATOR of type User on 2021-11-19T17:12:58Z\n'Maybe not intentionally left in?' by a COLLABORATOR of type User on 2021-11-19T17:13:06Z\n'```suggestion\r\n           else contextlib.nullcontext())\r\n```' by a COLLABORATOR of type User on 2021-11-19T17:13:35Z\n'```suggestion\r\n         else contextlib.nullcontext())\r\n```' by a COLLABORATOR of type User on 2021-11-19T17:14:02Z\n'Here and elsewhere, it's unfortunate to reuse the name `transform_stack` for a bool rather than an actual transform stack. How about another name, like `extend_transform_stack` or something?' by a COLLABORATOR of type User on 2021-11-19T17:14:50Z\n'Fix indentation?' by a COLLABORATOR of type User on 2021-11-19T17:16:57Z\n'Syntax error?' by a COLLABORATOR of type User on 2021-11-19T17:17:18Z\n'Keep type annotation on one line?' by a COLLABORATOR of type User on 2021-11-19T17:17:58Z\n'Commented-out code.' by a COLLABORATOR of type User on 2021-11-19T17:18:17Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 18,
        "reason_for_closure": "Closed as not useful :- PR deprecates existing functionality with little benefit.  Functionality mismatch with NumPy is acceptable."
    },
    {
        "summary": "Pull Request '8251' titled 'Added AdamW, AdaBelief Optimizers and tests' was authored by a User, who is associated as a NONE. \nIt was created at 2021-10-17T06:21:06Z, and was closed at 2021-10-20T07:32:25Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Added two variants of Adam optimizers, AdamW and AdaBelief, I've tested the optimizers on my test bench using MLP and resnet18 on MNIST,  and mini-ImageNet. They seem to be working as expected. \r\n\r\nThanks. '\nPR has comments:\n'@levskaya Anselm, can you review these new optimizers, or suggest somebody who can?' by a COLLABORATOR of type User on 2021-10-19T15:52:25Z\n'The Adabelief optimizer is not a standard optimizer and, in my experience, doesn't seem to work better than the baselines in the paper if they are properly tuned. I'm not familiar with the goal of example_libraries/optimizers.py, but it might be worth finding a different home for an AdaBelief implementation if optimizers.py is supposed to only contain relatively standard algorithms and just serve as an inspiration for people writing their own.' by a CONTRIBUTOR of type User on 2021-10-20T06:36:27Z\n\nPR has review comments:\n'If you're doing AdamW you should _not_ be adding a L2 regularization term to the gradient - that's precisely what AdamW tries to avoid doing.' by a COLLABORATOR of type User on 2021-10-20T07:22:13Z\n'Thank you for your feedback! ' by a NONE of type User on 2021-10-21T09:17:48Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Incorrect :- PR tried to remove complex dtype support to match numpy, but JAX supports it.  It was decided to keep the support."
    },
    {
        "summary": "Pull Request '8014' titled 'Schur decomposition lapack wrapper' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-09-27T14:24:35Z, and was closed at 2021-09-28T18:37:59Z by a User.\nThe PR has labels: cla: no - . \nIt has a body of 'This is an implementation of a wrapper for LAPACK's gees routine (in order to close #6478). \r\n\r\nThe schur primitive calls this wrapper on CPU. Tests for this primitive to make sure it matches scipy's schur decomposition and batching tests are added in `tests/linalg_tests.py`.\r\n\r\n'\nPR has comments:\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F8014) for more info**.\n\n<!-- need_author_cla -->' by a NONE of type Bot on 2021-09-27T14:24:51Z\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F8014) for more info**.\n\n<!-- need_author_cla -->' by a NONE of type Bot on 2021-09-27T15:19:54Z\n'Thanks for the contribution!\r\n\r\nI believe that the reason tests are failing is because github CI tests are run against released jaxlib versions, and testing this code requires jaxlib to be built from this branch.\r\n\r\nThe way we work around this generally is to gate the tests on the value of `jax._src.lib._xla_extension_version`, which is defined within the tensorflow source and is [currently at `38`](https://github.com/tensorflow/tensorflow/blob/18cd0a566088ec6aa03ff931bdbc421f0fb1e394/tensorflow/compiler/xla/python/xla_client.py#L51-L53).\r\n\r\nFor this PR, you could specify the tests to be skipped if the version is `_xla_extension_version <= 38`, which will make the tests green for the PR. Then we can do some internal testing against tensorflow with `_xla_extension_version` bumped to `39` before merging. What do you think?' by a COLLABORATOR of type User on 2021-09-27T22:20:37Z\n'There was a reference to the gees wrapper that was just in the body of the `linalg.py` primitives file, I put it in the scope of the schur primitive so it doesn't cause any issues without calls to the primitive. I also added an `if _xla_extension_version > 38` condition that skips the test for older version.' by a CONTRIBUTOR of type User on 2021-09-28T14:45:04Z\n'Also, once you address that final change, please squash the changes into a single commit (see https://jax.readthedocs.io/en/latest/contributing.html#single-change-commits-and-pull-requests) Thanks!' by a COLLABORATOR of type User on 2021-09-28T15:54:22Z\n'> There was a reference to the gees wrapper that was just in the body of the `linalg.py` primitives file, I put it in the scope of the schur primitive so it doesn't cause any issues without calls to the primitive. I also added an `if _xla_extension_version > 38` condition that skips the test for older version.\r\n\r\nThinking about this, I think it's probably even better to use the condition `jax._src.lib.version >= (0, 1, 72)`.\r\n\r\nYou can think about `_xla_extension_version` as the version of the XLA/TF source trees. In this case, this code is in the JAX tree, so I think your best bet is to use a version number defined in that tree also. (Ideally we'd have a monotonic version number in the JAX tree as well, but the jaxlib version number will do fine I think.)' by a COLLABORATOR of type User on 2021-09-28T15:55:53Z\n'You have some flake8 lint failures as well.' by a COLLABORATOR of type User on 2021-09-28T15:58:39Z\n'@hawkinsp - the reason I suggested checking `xla_extension_version` is because I was planning to patch the CL to update the extension version and run internal tests for the new codepaths. If we gate on jaxlib release, we won't see any potential failures in CI until the next version of jaxlib is released (unless kokoro is doing a jaxlib build from source, which I don't think is the case?)' by a COLLABORATOR of type User on 2021-09-28T16:15:32Z\n'Kokoro *is* doing a jaxlib build from source for at least two of the builds. So we should see the failures right here in Github.' by a COLLABORATOR of type User on 2021-09-28T16:19:49Z\n'Thanks, that makes sense. Agreed that checking on `jax._src.lib.version` is the better option here.' by a COLLABORATOR of type User on 2021-09-28T16:21:11Z\n'All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F8014) for more info**.\n\n<!-- need_author_consent -->' by a NONE of type Bot on 2021-09-28T18:36:48Z\n'I think I really messed up my git repo, I am going to do another pull request from a fresh repository. I really apologize.' by a CONTRIBUTOR of type User on 2021-09-28T18:37:59Z\n\nPR has review comments:\n'Slight preference for all of these implementation details to be marked private with a leading underscore, e.g.\r\n```\r\ndef _schur_impl(...)\r\n```\r\nSame below.' by a COLLABORATOR of type User on 2021-09-27T22:19:49Z\n'Thank you for reviewing ! I have marked them private' by a CONTRIBUTOR of type User on 2021-09-28T14:38:22Z\n'Rather than blocking the test code on an `if` statement, it would be better to use `self.skipTest(reason)` so that the test is reported as skipped.' by a COLLABORATOR of type User on 2021-09-28T15:52:42Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 12,
        "num_review_comments": 3,
        "reason_for_closure": "Author realized the PR was not the right approach and closed it after discussion with collaborators."
    },
    {
        "summary": "Pull Request '7731' titled 'Check for aval weak_type attr before accessing it.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-08-26T17:52:43Z, and was closed at 2021-08-28T18:22:21Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nPR has comments:\n'The problem is root-caused to ShardedDeviceArray's device_buffers not getting their `aval` set in the first place. ' by a COLLABORATOR of type User on 2021-08-26T21:42:56Z\n'Nice!' by a COLLABORATOR of type User on 2021-08-26T22:01:52Z\n'fixed in an internal cl' by a COLLABORATOR of type User on 2021-08-28T18:22:21Z\n\nPR has review comments:\n'nit: maybe `if getattr(self.aval, 'weak_type', False):`?' by a COLLABORATOR of type User on 2021-08-26T18:01:16Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 1,
        "reason_for_closure": "Internal API exposure risk addressed; closed as unnecessary."
    },
    {
        "summary": "Pull Request '7729' titled 'Add np.polydiv numpy function' was authored by a User, who is associated as a NONE. \nIt was created at 2021-08-26T16:56:01Z, and was closed at 2022-04-13T19:29:35Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'added the numpy polydiv core function (part of #70)'\nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F7729) for more info**.\n\n<!-- need_sender_cla -->' by a NONE of type Bot on 2021-08-26T16:56:06Z\n'@googlebot I signed it!' by a NONE of type User on 2021-08-26T16:58:39Z\n'Cross-ref #7732, which contains some cleanups of related functions.' by a COLLABORATOR of type User on 2021-08-26T17:58:59Z\n'Superseded by #10226 - thanks!' by a COLLABORATOR of type User on 2022-04-13T19:29:35Z\n\nPR has review comments:\n'I'm assuming adding `0.0` is indended to cast these to float? If so, I'd consider using the `_promote_dtypes_inexact` utility instead; this will also let you remove some of the typecasting logic below.' by a COLLABORATOR of type User on 2021-08-26T17:16:27Z\n'Also, it would be good to add a check that `u` and `v` are one-dimensional.' by a COLLABORATOR of type User on 2021-08-26T17:18:59Z\n'This `while` loop logic will make the function incompatible with JIT, because the shape of the resulting array will depend on its contents.\r\n\r\nPerhaps we could zero-out the entries instead? Maybe something like:\r\n```python\r\nr = r.at[~(abs(r) > 1E-14).cumsum().astype(bool)].set(0)\r\n```\r\nAlso, should we use a different tolerance for float32 vs. float64?\r\n' by a COLLABORATOR of type User on 2021-08-26T17:25:02Z\n'See also the `trim_leading_zeros` logic in jax's implementation of `polymul`; we could do something similar here.' by a COLLABORATOR of type User on 2021-08-26T17:44:54Z\n'I am facing issues with this line \r\n```\r\nq = zeros((max(m - n + 1, 1),), w.dtype)\r\n```\r\nIt is showing me a Value error \r\n\r\nalso what other way would you suggest for assignment in JAX with reference to  r[k:k+n+1] -= d*v' by a NONE of type User on 2021-08-30T06:29:58Z\n'`jax.numpy` overrides `max` to match the behavior of `numpy.max`. If you want the builtin `max` function, use `_max` here.\r\n\r\n`r[k:k+n+1] -= d * v` will not work because JAX arrays are immutable. You'll need to use [index update](https://jax.readthedocs.io/en/latest/jax.ops.html#indexed-update-operators) operations instead; e.g.\r\n```python\r\nr = r.at[k:k+n+1].add(-d*v)\r\n```' by a COLLABORATOR of type User on 2021-08-30T12:24:46Z\n'I think this line can be removed, also the type cast `r = u.astype(...)`.' by a NONE of type User on 2021-08-30T14:32:21Z\n'Some major care still needs to be taken here, since in compiled code we generally don't have the guarantee that `v` is leading-zero-free.' by a NONE of type User on 2021-08-30T14:36:58Z\n'Maybe replace `1E-14` with `something * finfo(r.dtype).eps`?' by a NONE of type User on 2021-08-30T14:39:30Z\n'oh okay just saw\r\n' by a NONE of type User on 2021-08-30T14:51:03Z\n'got it will update' by a NONE of type User on 2021-08-30T14:51:40Z\n'also, what will you suggest to make this code not outdated' by a NONE of type User on 2021-08-30T15:23:57Z\n'> what will you suggest to make this code not outdated\r\n\r\nThat I have no idea. 🤷 ' by a NONE of type User on 2021-08-30T15:30:51Z\n'Since this like is pretty non-trivial, I might factor out a helper function for clarity. Something like\r\n```\r\ndef _zero_out_leading_elements(u, tol=1E-14):\r\n  return u.at[~(abs(u) > 1E-14).cumsum().astype(bool)].set(0)\r\n```\r\nAlso, the tolerance may have to be adjusted depending on the dtype of `u`. In numpy, I believe it's always cast to float64, so hard-coding 1E-14 is probably OK. In JAX, where possible we try to keep computations in 16 or 32 bit, matching the inputs, because these types have better support on some accelerators.' by a COLLABORATOR of type User on 2021-08-30T16:08:13Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 14,
        "reason_for_closure": "Superseded :- PR became irrelevant due to a fix in another PR/dependency."
    },
    {
        "summary": "Pull Request '7719' titled 'Add jax.debugging.make_hlo()' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-08-25T22:36:42Z, and was closed at 2021-11-29T15:33:58Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'Based on code by @levskaya\r\n\r\nSee the generated docs here: https://jax--7719.org.readthedocs.build/en/7719/_autosummary/jax.debugging.make_hlo.html#jax.debugging.make_hlo'\nPR has comments:\n'I think it's worth noting that there is #3837. They also have arguments `num_replicas` and `num_partitions`, are they important?\r\n' by a NONE of type User on 2021-08-26T10:30:57Z\n'We've discussed before making this an API from `jit`, `pmap` et al. For example `jax.jit(f).hlo(a, b, c)` and `jax.jit(f).compile(a, b, c).hlo()`.\r\n\r\nThis has the advantage of capturing the configuration @soraros points out (e.g. num_replicas) in the same way the outer API.\r\n\r\nShould we implement that instead?' by a COLLABORATOR of type User on 2021-08-26T13:25:05Z\n'That's a good thought! It's a bit more involved to implement, though, because for JIT the compiled function object is defined in C++, so implementing this as an attribute there would require a C++ implementation in jaxlib rather than a simpler Python utility.' by a COLLABORATOR of type User on 2021-08-26T16:13:42Z\n'... or maybe it could be monkey-patched.' by a COLLABORATOR of type User on 2021-08-26T16:16:20Z\n'What @tomhennigan is referring to is now tracked as #7733. We could merge this PR (or whatever similar PR) as is. When the work on #7733 is complete, the implementation here can change to use it.\r\n\r\nOverall I think the idea here is to provide a simple utility to handle a special—and maybe very common—case of AOT lowering. I'd encourage us to pick what arguments to offer/hide here on that principle. #7733 will ideally offer the full menu of options.' by a MEMBER of type User on 2021-08-26T20:54:34Z\n'If this PR only deals with the printing part(as it returns a string), I think having an extra `print_option` argument would be helpful. Something along the line of\r\n```python\r\ndef make_hlo(..., print_option: str | HloPrintOptions = 'short_parsable'):\r\n  ...\r\n  if isinstance(print_option, str):\r\n    match print_option:\r\n      case 'short_parsable': ...\r\n      case 'canonical': ...\r\n\t  case 'fingerprint': ...\r\n      case _: raise\r\n  else:\r\n    ...\r\n ...\r\n```\r\n' by a NONE of type User on 2021-08-28T01:34:44Z\n'Closing because I think @froystig 's work on the new AOT APIs replaces it.' by a COLLABORATOR of type User on 2021-11-29T15:33:58Z\n\nPR has review comments:\n'I agree that adding a print option would be nice\r\n\r\nbut we could ask  @soraros to contribute an improvement PR if there's interest' by a COLLABORATOR of type User on 2021-08-28T23:58:55Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 1,
        "reason_for_closure": "Superseded :- Newer AOT APIs replace the functionality of this PR."
    },
    {
        "summary": "Pull Request '7651' titled 'custom_vjp: automatically handle float0 cotangents' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-08-17T22:15:06Z, and was closed at 2021-08-17T23:17:22Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'The main issue here was that when a custom_vjp function took a primal input with integer dtype, we would demand (via an internal assertion) that the user's bwd function would produce a value with float0 dtype (i.e. the tangent type corresponding to an integer primal). But users don't know about float0s, and shouldn't have to learn about them! So instead we just automatically handle producing zeros here (with the right dtype).\r\n\r\nAlso misc cleanups (I'll mark them below in review comments).'\nPR has comments:\n'On second thought, I'm going to be more responsible and break this PR up into two.' by a COLLABORATOR of type User on 2021-08-17T23:17:22Z\n\nPR has review comments:\n'This is an unrelated bug I happened to notice. It's baked in the assumption that primal type equals tangent type, i.e. that `T a == a`, which is probably true for all inputs to custom_jvp/vjp functions given existing JAX datatypes but won't always be true.' by a COLLABORATOR of type User on 2021-08-17T23:13:28Z\n'The `or a != a.at_least_vspace()` is the only thing we needed to fix the bug at hand.\r\n\r\nAnother change on this line was to produce symbolic zeros using `Zero` rather than `zeros_like_aval`. I couldn't resist making this optimization, even though it is unrelated. (I hope it doesn't cause unexpected breakages...)' by a COLLABORATOR of type User on 2021-08-17T23:14:22Z\n'I moved this function to the only file where it's used, and then I realized that we could just inline it.' by a COLLABORATOR of type User on 2021-08-17T23:14:39Z\n'This change is unrelated to the bug at hand, but it is related to producing symbolic Zeros as the output of a custom_vjp bwd function (mentioned above). The reason we weren't producing symbolic zeros before is that we had to make this function, which is only called in the vmap rule for custom_vjp, handle them.' by a COLLABORATOR of type User on 2021-08-17T23:15:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Closed as not useful: PR introduced unwanted behavior change."
    },
    {
        "summary": "Pull Request '7517' titled 'autodidax prototype of user-customizable partial eval strategies' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-08-05T17:11:09Z, and was closed at 2021-08-15T18:27:21Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of '**This PR comment is a copy-and-paste of a design doc (or most of it) that we wrote to accompany this prototype implementation.** If/when this implementation goes through we'll add a separate design doc in a .md file, so take this PR comment as a draft.\r\n\r\n# User-customizable gradient checkpointing policies\r\n\r\n_mattjj@, jekbradbury@_\r\n\r\nstarted 2021.07.07, updated 2021.08.05\r\n\r\n### Intro and motivation {#intro-and-motivation}\r\n\r\nBy default, `jax.grad` (and really the underlying `jax.linearize`) computes and stores as much as possible in the forward pass (i.e. the primal computation). For example:\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/1458824/128421554-089257a1-6c4f-4101-9764-9ed42e7f1fc7.png)\r\n\r\n\r\nNotice that the cosines are evaluated in the primal computation, with the resulting values stored as \"residual\" outputs to be used as constants in the tangent or cotangent jaxpr.\r\n\r\n`jax.checkpoint` (aliased as `jax.remat`) lets users switch this strategy to the opposite extreme: don't store anything except inputs to the function, and instead recompute (i.e rematerialize) all the values needed for the linear computation. At its heart `jax.checkpoint` is a call-like primitive with a special partial evaluation rule, which is why it works for `jax.linearize` as well as `jax.vjp`: both are based on partial evaluation. For example:\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/1458824/128421586-4074408e-087c-4030-a306-3f01e76ac3d0.png)\r\n\r\n\r\nNotice that now the cosines, and even some sines, are evaluated in the tangent and cotangent jaxprs, with no residuals stored beyond the input to the function. (In addition to this custom partial evaluation behavior, the primitive underlying `jax.checkpoint` also has [a special transpose rule](https://github.com/google/jax/blob/78a689bb09799d8b13b28fe90bee6a04bddb7968/jax/interpreters/ad.py#L547-L570) for evaluating these nonlinear expressions first, as well as [a special XLA HLO translation rule](https://github.com/google/jax/blob/78a689bb09799d8b13b28fe90bee6a04bddb7968/jax/interpreters/xla.py#L1465-L1478) to prevent CSE optimizations from foiling its intended behavior. These additional rules are why it makes sense to model `jax.checkpoint` as a call-like primitive.)\r\n\r\nEven if neither the “store everything” nor the “store nothing except inputs” policies is right for a particular application, in principle users can apply `jax.checkpoint` to sub-functions as needed to control exactly what to store and what to recompute. But this API can require refactoring model code just to express the kind of checkpointing desired. Not only does the current API make expressing the right thing hard, but it also makes revising the strategy, for example to experiment with new ones or switch based on hardware backend, untenable.\r\n\r\nIn particular, there’s an intermediate activation-storage policy that’s a good fit for many memory-sensitive neural net applications on accelerators: storing the outputs of dots and convolutions, but recomputing everything else. (The motivation for this policy is that fused element-wise nonlinear operations are generally much cheaper than dots and convolutions on GPUs and TPUs, or, when fused into those operations on TPUs, completely free.) That’s easy to say in words, but it was previously impossible to express at the top level, without diving into neural network layer code that might have been written by others as part of a library. \r\n\r\nThe situation was roughly:\r\n\r\n\r\n```\r\n# in library code\r\ndef apply_layer(W, x):\r\n  return jnp.sin(jnp.dot(W, x))\r\n\r\n# in top-level user code\r\ndef predict(params, x):\r\n  for W in params[:-1]:\r\n    x = apply_layer(W, x)\r\n  return jnp.dot(params[-1], x)\r\n```\r\n\r\n\r\nWe can't get what we want by applying `jax.checkpoint` to `apply_layer`; that would actually recompute the outputs of dots while saving the outputs of the nonlinear operations in the nonlinearity's JVP rule. It's the opposite of what we want! We'd need something more like `jax.checkpoint(lambda W, x: jnp.dot(W, jnp.sin(x)))`, but that's a strange way to organize layers in neural networks. And do we really want to require users to refactor libraries into non-idiomatic organizations just for checkpointing strategies? No way!\r\n\r\nUntil recently, this issue was somewhat hidden because XLA:TPU optimizations often ended up producing an optimized graph that used this intermediate strategy (specifically, XLA:TPU is able to duplicate these free nonlinear operations into multiple dot/conv fusions). But it sprang up recently in two contexts. First, users reported significantly higher memory usage for the same JAX program on GPU versus TPU, and we traced part of this back to XLA:GPU not performing the same duplication-into-fusions. \r\n\r\nSecond, the issue also sprang up recently because of one more element in the large model application: control flow. XLA:TPU can often figure out great rematerialization strategies, regardless of what JAX's AD produces, so manual control over gradient checkpointing isn't always necessary in the toy code above. But in the real example, we couldn't rely on XLA to sort out what should be stored and what shouldn't be because we were actually doing a scan-over-layers. The presence of that control flow primitive foiled XLA’s rematerialization- related optimizations, and thus required JAX's AD to produce more efficient programs.\r\n\r\nOur aim here is **to enable easy-to-revise remat strategies** by improving JAX's AD.\r\n\r\n\r\n### API proposal: parameterize `jax.checkpoint` with a callback {#api-proposal-parameterize-jax-checkpoint-with-a-callback}\r\n\r\n\r\n##### Basic idea {#basic-idea}\r\n\r\n**As an internal API**, the basic idea is to parameterize `jax.checkpoint` with an optional callback function which represents the checkpointing strategy. The callback takes as input a type-level specification of a primitive application, and returns a boolean indicating whether the value of the application should be saved as a residual:\r\n\r\n\r\n```\r\ndef checkpoint_policy(prim: Primitive, *avals: AbstractValue,\r\n                      **params: Any) -> bool:\r\n  ...\r\n\r\n@partial(jax.checkpoint, policy=checkpoint_policy)\r\ndef f(...):\r\n  ...\r\n```\r\n\r\n\r\nThis way policies can depend not only on which primitives are being applied but also on input and output sizes. The default `jax.checkpoint` strategy would then correspond to a callback which always returns False, indicating that no values of any applications should be saved. Moreover, the default strategy of AD without `jax.checkpoint` corresponds to a callback which always returns True, indicating that values of every nonlinear application (which is ultimately used in the application of a partially-linear primitive, i.e. is not dead code) should be saved:\r\n\r\n\r\n```\r\ndef default_checkpoint_strategy(*_, **__):\r\n  return False  # no intermediates stored as residuals\r\n\r\ndef default_ad_strategy(*_, **__):\r\n  return True  # store the value of every nonlinear computation as a residual\r\n```\r\n\r\n\r\nThis general API must be an internal one because it depends on internal details like primitives and abstract values.\r\n\r\n**To provide a public API**, we can offer some pre-baked strategies. The clearest current candidate is the `checkpoint_dots` strategy described above, where we save the outputs only of dots and convolutions. Internally to JAX, it could be implemented as\r\n\r\n\r\n```\r\ndef checkpoint_dots(prim: Primitive, *_, **__) -> bool:\r\n  return prim in (lax.dot_general_p, lax.conv_general_dilated_p)\r\n```\r\n\r\n\r\nTo employ this strategy, the user would write something like\r\n\r\n\r\n```\r\n@partial(jax.checkpoint, policy=jax.checkpoint_policies.checkpoint_dots)\r\ndef f(...):\r\n  ...\r\n```\r\n\r\n\r\n\r\n##### Example {#example}\r\n\r\nHere's an example of how this policy would work on our toy example (residuals and recomputed operations, representing extra memory and extra compute respectively, are highlighted):\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/1458824/128421648-b4666ae5-402a-402b-a377-a74d7cff8779.png)\r\n\r\n\r\nSome points to notice:\r\n\r\n\r\n\r\n*   For clarity, where outer products of vectors are needed in cotangent computations, we inlined some transpose expressions into primitive applications and elided insertion of singleton dimensions. These aren't allowed in real jaxprs but keep the example simpler.\r\n*   The sine JVP rule only involves one nonlinear operation (a cosine), but for other nonlinearities there could be greater storage savings relative to the checkpoint-free baseline shown in (a).\r\n*   Since the `checkpoint_dots` strategy used in (c) indicates that we should only save `a1`, it's the only residual output by the primal jaxpr (beyond the inputs to the function that we need, namely `W1`, `W2`, and `x1`). Correspondingly in the cotangent jaxpr we recompute nonlinear values which depend on the value of a1, namely `x2` and `r1`.\r\n\r\n\r\n##### Nesting: the inner policy wins {#nesting-the-inner-policy-wins}\r\n\r\nOne detail is how these policies behave under nesting. That is, now that each `jax.checkpoint` decorator can have a different policy, if one `jax.checkpoint`-decorated function calls another, which policy is used for the called function?\r\n\r\n\r\n```\r\n@partial(jax.checkpoint, policy=policy1)\r\ndef f(x):\r\n  ...\r\n  @partial(jax.checkpoint, policy=policy2)\r\n  def g(y):\r\n    ...  # policy1 or policy2 for these primitive applications?\r\n  return g(...)\r\n```\r\n\r\n\r\nTo enable more local control over how autodiff behaves, and to enable more expressive mixing-and-matching of one policy within another, we choose to let the inner policy win. If a top-level power user really wants to control checkpointing policy, and override whatever the library author wanted, they can always resort to a simple transformation to prune inner `jax.checkpoint` primitives.\r\n\r\nAn alternative, suggested by dougalm@, is that we allow policies to be tagged with a priority score, and when nesting we follow the policy with the higher priority score. That approach gives strictly more control to users and library writers alike, and works well for e.g. NumPy array API priorities. We'll defer on this approach for now because it can be added later as a strict generalization.\r\n\r\n\r\n### Implementation proposal {#implementation-proposal}\r\n\r\nSee [the full Autodidax prototype in #7517](https://github.com/google/jax/pull/7517).\r\n\r\n\r\n##### Fundamentals and jaxpr-to-jaxpr version {#fundamentals-and-jaxpr-to-jaxpr-version}\r\n\r\nTo explain the implementation of the proposed API, we'll start by thinking in terms of jaxpr-to-jaxpr transformations. The implementation of `jax.checkpoint` would in its partial evaluation rule form a full jaxpr ([like it does currently](https://github.com/google/jax/blob/1646ddaa805625c8acb1385c13de5f111db0aed6/jax/interpreters/partial_eval.py#L754), before this proposal) and then apply this jaxpr-to-jaxpr transformation to build both the eagerly-evaluated jaxpr1 and the staged-out jaxpr2. We'll return to the possibility of doing some transformation work while tracing in a [subsection to follow](#performing-the-non-dce-work-while-tracing-rather-than-jaxpr-to-jaxpr).\r\n\r\nRecall from `partial_eval_jaxpr` ([Autodidax reference](https://github.com/google/jax/blob/806899f81d2a2be76e3a25bd0d543bf05138b724/docs/autodidax.py#L2224-L2272)) that partial evaluation 'unzips' one jaxpr into two. Given a jaxpr and a list of booleans indicating which input binders correspond to unknowns, it:\r\n\r\n\r\n\r\n1. for each variable in the jaxpr, associates a boolean indicating whether it is unknown;\r\n2. for each first-order primitive application adds it either into the resulting jaxpr1 (if all its inputs are known, in which case all outputs are known) or else puts it in the resulting jaxpr2 (if any input is unknown, in which case all outputs are unknown); and\r\n3. for each known variable used as an input to a jaxpr2 primitive application, adds it as a residual output to jaxpr1 and residual input to jaxpr2.\r\n\r\nThe behavior for higher-order primitives is fixed by desiderata like `jit` invariance, which is why we focus on first-order primitives in the descriptions here.\r\n\r\nBy allowing the user to make some values unsaveable, we need to make some changes:\r\n\r\n\r\n\r\n1. for each variable in the jaxpr, we associate a pair of booleans: one indicating whether the variable is known, and another indicating whether it is instantiated in jaxpr2 (as opposed to being saveable from jaxpr1);\r\n2. some first-order primitive applications may need to be staged into both jaxpr1 and jaxpr2 (as shown in previous examples), and whether a primitive application is ultimately necessary in jaxpr1 or respectively jaxpr2 cannot be decided based solely on the known/unknown and saveable/unsaveable labels of its inputs but instead depends on downstream data dependence, namely whether an output of jaxpr1 or respectively jaxpr2 depends on it (see [Examples](#examples-related-to-dead-code) below);\r\n3. only saveable known inputs to jaxpr2 applications are added as residuals, and the others are made available as outputs of primitive applications in jaxpr2 rather than as top-level residual inputs.\r\n\r\nOur implementation is a fairly direct consequence of these new requirements. However, there are a few remaining decisions.\r\n\r\nOne decision is how much of this work to perform at trace time (i.e. how much to adapt JaxprTrace) versus as a jaxpr-to-jaxpr transformation; we'll defer that discussion to a later subsection.\r\n\r\nAnother decision is how to handle the dead code issue described in change 2 above, and in particular whether we should generate dead code and then clean it up, or else try to avoid generating it in the first place. We choose the former: we handle 2 by defensively adding some primitive applications to both jaxpr1 and jaxpr2, though they may not be needed downstream; then we rely on a new dead-code elimination (DCE) pass to clean things up. (Notice we can't apply DCE until the very end, i.e. we can't apply it to subjaxprs within a jit since we don't know how its outputs will be used!) An alternative to relying on DCE would be to form jaxpr1 and jaxpr2 in a reverse-dataflow pass; that's viable, but it still requires a backward pass with its own rules for higher-order primitives (i.e. it doesn't offer much of a simplification), and DCE is a useful tool on its own (whereas this alternative backward pass would only be useful for partial evaluation).\r\n\r\nHere's [the resulting Autodidax version of partial\\_eval\\_jaxpr\\_custom](https://github.com/google/jax/pull/7517/files#diff-bda36e5567d146410fb8b82f722eba347827bb6faae605b5e87f3a32130f3c99R2905-R2959), as well as [its rule for xla\\_call as a higher-order primitive](https://github.com/google/jax/pull/7517/files#diff-bda36e5567d146410fb8b82f722eba347827bb6faae605b5e87f3a32130f3c99R2977-R2994). Here's the implementation of [dce\\_jaxpr](https://github.com/google/jax/pull/7517/files#diff-bda36e5567d146410fb8b82f722eba347827bb6faae605b5e87f3a32130f3c99R3006-R3036) and [its rule for xla\\_call](https://github.com/google/jax/pull/7517/files#diff-bda36e5567d146410fb8b82f722eba347827bb6faae605b5e87f3a32130f3c99R3040-R3049).\r\n\r\n\r\n##### Examples related to dead code {#examples-related-to-dead-code}\r\n\r\n**Example 1: dead code in jaxpr1 and jaxpr2**\r\n\r\nWithout DCE, the `partial_eval_jaxpr_custom` implementation described above can produce primitive applications that are unused on either of its output jaxprs. For example, given this input jaxpr to be partially evaluated with a save-nothing policy:\r\n\r\n\r\n```\r\n{ lambda a:f64[], b:f64[] .\r\n  let c:f64[] = sin a\r\n      d:f64[] = cos a\r\n      e:f64[] = mul d b\r\n  in ( f, e ) }\r\n```\r\n\r\nWith the inputs labeled as known and unknown, respectively, the cosine is added to jaxpr1 defensively because all its inputs are known, and so based on forward-flow information alone its value could be needed in the primal computation.\r\n\r\nIn addition, the sine is added to jaxpr2 because the policy does not allow saving its value as a residual, and its value may later be needed in jaxpr2 (as an input to a first-order primitive application with some unknown inputs).\r\n\r\n**Example 2: non-dead code in a sub-jaxpr of jaxpr2**\r\n\r\nThis example is meant to illustrate that we can't perform DCE on jaxpr2 when we form subjaxprs (e.g. for nested `jit` calls), and instead we must defer it until the end. Consider this jaxpr to be partially evaluated with a save-nothing policy:\r\n\r\n```\r\n{ lambda a:f64[], b:f64[] .\r\n  let c:f64[] d:f64[] = xla_call [ num_consts=0 ] a\r\n        { lambda a:f64[] .\r\n          let b:f64[] = sin a\r\n              c:f64[] = cos a\r\n          in ( b, c ) }\r\n      e:f64[] = mul d b\r\n  in ( c, e ) }\r\n```\r\n\r\nWith top-level inputs labeled known and unknown, respectively, both outputs to the xla_call application are known. Yet the value of the cosine application (and not the sine application) is needed in jaxpr2, downstream of the xla_call application, since it is used as an input to a first-order primitive with some unknown inputs.\r\n'\nPR has comments:\n'HELL YEAH!\r\n\r\nGo @mattjj! Thanks!\r\n\r\n(I was just thinking how useful https://github.com/google/jax/issues/2521#issuecomment-604759386 has been, unrelatedly, so thanks for all your work.)\r\n\r\nMy only regret is that I can't read this PR in github, and I have an interview today so I'll have to wait. But the title is exciting.' by a CONTRIBUTOR of type User on 2021-08-05T17:59:14Z\n'Oh wait, there's an .md along with the notebook. I can read it on github!' by a CONTRIBUTOR of type User on 2021-08-05T18:00:16Z\n'Hmm. Did you just remove the three examples from the .md file? They were very nice; I would keep them.\r\n\r\nIt also seems like you may be force-pushing to the branch rather than writing separate commits, so I'm not quite sure how to go back and get the examples (at least from github's UI).' by a CONTRIBUTOR of type User on 2021-08-05T18:12:13Z\n'Hmm. I think I'll add comments here rather than https://github.com/google/jax/commit/4d1c0ff40aa87e1daa2d28d166cdf8c27726a94e#r54476265 since they seem to vanish with each force push:\r\n\r\n> Before we turn to DCE, let's look at some examples.\r\n> ... 3 examples cut ...\r\n\r\nI think the examples are quite lovely, and I'd hate to see them get cut. But perhaps you could expand on \"examples of what?\" here -- for a casual reader who's skimming, I find it helpful to have a little bit of intro before diving in.\r\n\r\nAlso, I'm not sure if I was supposed to \"start a review\" or just leave comments here -- if me opening a review caused problems (I'm not sure what), feel free to ignore/close it.' by a CONTRIBUTOR of type User on 2021-08-05T18:20:48Z\n'> Hmm. Did you just remove the three examples from the .md file? They were very nice; I would keep them.\r\n\r\nAh yes, I did remove them, but really I _moved_ them, to a Google doc serving as the design doc (shared internally with the team). But that's not useful for external readers like you!\r\n\r\nI'll see if I can copy the doc into a .md file without too much trouble...' by a COLLABORATOR of type User on 2021-08-05T20:57:20Z\n'Now there is a long PR message here! The examples from the autodidax.py file are now distilled at the end of it.' by a COLLABORATOR of type User on 2021-08-05T21:13:48Z\n'I don't think starting a review would cause any problems!' by a COLLABORATOR of type User on 2021-08-05T21:24:28Z\n'Now that the real implementation is up in #7631, I'll close this PR.' by a COLLABORATOR of type User on 2021-08-15T18:27:21Z\n'> An alternative, suggested by dougalm@, is that we allow policies to be tagged with a priority score, and when nesting we follow the policy with the higher priority score. That approach gives strictly more control to users and library writers alike, and works well for e.g. NumPy array API priorities.\r\n\r\nNit: I don't think this really worked out great for NumPy's `__array_priority__`. Users basically picked priorities > 1 at random without any sense of how they could interact.\r\n\r\nNumPy's newer mechanism for overriding operations with NumPy arrays `__array_ufunc__` got rid of array priorities in favor of forcing users to explicitly choose how to handle every pair of array types. That would be a better model to emulate in my opinion (though I also think it's fine to just say \"inner policy wins\" in this case, since there is an ambiguous ordering).' by a COLLABORATOR of type User on 2021-08-17T06:51:48Z\n\nPR has review comments:\n'> The custom rules are basically for handling higher-order primitives. The\r\nuser-supplied callback function is essentially a typing rule for first-order\r\nprimitives, but we define the rules for higher-order primitives because their\r\nproperties are constrained by desiderata like jit invariance.\r\n\r\nnit: `s/desiderata/details/` would be slightly clearer for casual readers. But it doesn't really matter.\r\n\r\nDesiderata sounds like a place I'd like to visit someday, though.' by a CONTRIBUTOR of type User on 2021-08-05T18:09:08Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 9,
        "num_review_comments": 1,
        "reason_for_closure": "Superseded by another PR/implementation.  The prototype PR was closed in favor of a subsequent PR with the actual implementation."
    },
    {
        "summary": "Pull Request '7477' titled 'Jittable eigh' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-08-03T21:24:14Z, and was closed at 2023-11-03T15:25:56Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of '@hawkinsp @tlu7 @rmlarson\r\n\r\nThis commit evades the varying blocksize issue associated with QDWH-SVD by using a different polynomial iteration that computes a rank-k orthogonal projector for given k directly. The specific algorithm is described here: https://core.ac.uk/download/pdf/79508571.pdf.\r\n\r\nI'd been trying to avoid this because the performance and error behaviour, while still not awful, tends not to be quite so good as obtained by QDWH. But the solutions I came up with turned out to be generally even worse.'\nPR has comments:\n'Closing. We've had a `jit`-table `eigh` for some time by using a bucketing and padding strategy.' by a COLLABORATOR of type User on 2023-11-03T15:25:57Z\n\nPR has review comments:\n'JAX style thing: I'd always prefer `X.at[index].update(y)` over `index_update` these days. They are synonyms but I think the terser syntax is more readable.' by a COLLABORATOR of type User on 2021-08-03T21:51:35Z\n'typo: smallest' by a CONTRIBUTOR of type User on 2021-08-30T16:38:39Z\n'wouldn't it be more efficient to compute full row- and column sums of abs(H) and subtract abs(diag) from them as vectors. Here you are forming a whole temporary n-by-n matrix. Perhaps the compiler can optimize it away, but relying on that seems equally verbose and less certain.' by a CONTRIBUTOR of type User on 2021-08-30T16:45:14Z\n'Please add a docstring or comment. This function is rather subtle.' by a CONTRIBUTOR of type User on 2021-08-30T16:47:33Z\n'I'm not sure if the term \"purify\" is particularly meaningful in the general context.  Maybe `_compute_fixed_rank_projector`?\r\n\r\nPlease add a link to the paper in the docstring.\r\n\r\nPerhaps try to relate this to a more widely known iteration. This is a form of accelerated Newton-Schultz or something like that, right?' by a CONTRIBUTOR of type User on 2021-08-30T16:50:21Z\n'should this be '>' ? What happens if a value is right on the split point and ends up in both beta_2 and beta_2_bar?' by a CONTRIBUTOR of type User on 2021-08-30T16:51:36Z\n'nit: s/still_going/maxiter_reached/ ?' by a CONTRIBUTOR of type User on 2021-08-30T16:52:42Z\n'nit: s/coef/coeff/' by a CONTRIBUTOR of type User on 2021-08-30T16:53:53Z\n'Maybe write out the polynomial expression here to aid the reader?' by a CONTRIBUTOR of type User on 2021-08-30T16:56:26Z\n'where does the factor of 2 come from?' by a CONTRIBUTOR of type User on 2021-08-30T16:57:16Z\n'let's use the term \"orthogonal projectors\" instead of \"isometries\", which I have never seen in the numerical linear algebra literature.' by a CONTRIBUTOR of type User on 2021-08-30T16:58:49Z\n'delete?' by a CONTRIBUTOR of type User on 2021-08-30T17:00:46Z\n'revert?' by a CONTRIBUTOR of type User on 2021-08-30T17:00:58Z\n'let's use \"bisection\" instead of \"dac\" (which is hardcoded to translate to \"digital to analog converter\" in my brain ;-) )' by a CONTRIBUTOR of type User on 2021-08-30T17:02:04Z\n'revert? We should test for larger sizes. ' by a CONTRIBUTOR of type User on 2021-08-30T17:03:34Z\n'out of curiosity: Does JAX have a fast O(n) in-place permute implementation?' by a CONTRIBUTOR of type User on 2021-08-30T17:07:07Z\n'We should probably guard against division by zero here. This would blow up for H=eye(n), I think.' by a CONTRIBUTOR of type User on 2021-08-30T17:09:03Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 17,
        "reason_for_closure": "Closed as JAX behavior should be a superset of NumPy's, not a strict match.  The existing implementation is valid."
    },
    {
        "summary": "Pull Request '7425' titled 'Initial commit of jax2iree' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-07-30T23:21:25Z, and was closed at 2021-11-29T15:32:58Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'This is awesome! ❤️ ' by a NONE of type User on 2021-08-03T17:26:22Z\n'> Drive by :)\r\n\r\nThanks - most of these items will go away as part of finishing this PR and flipping it out of Draft :)\r\n' by a CONTRIBUTOR of type User on 2021-08-04T16:42:08Z\n'This work prompted me to finally get IREE publishing a first class C/Python API to its compiler (we were previously doing a hoaky, shell out to a compiler executable thing, which isn't the best for JIT/in-process): https://github.com/google/iree/pull/6683\r\n\r\nI'll combine that with the existing IREE runtime API early next week and should be able to land this POC spike in a couple of days. It'd be good to chat then about next steps.' by a CONTRIBUTOR of type User on 2021-08-07T02:16:16Z\n'Ok, the most recent push pulls in our new in-process compiler API and demonstrates basic runtime integration for e2e execution. Still plenty to do, but this represents a complete POC.' by a CONTRIBUTOR of type User on 2021-08-09T18:19:22Z\n'Thanks for getting this started!\r\n\r\nI was expecting that an IREE integration will be more lightweight from a JAX point of view: export the HLO module, invoke the IREE compiler, plus some runtime integration to run the executables. The current PR seems to use a custom trace to convert each JAX primitive separately. I think that this is very costly: a lot of conversion rules some of which will be very tricky. We do follow this strategy for jax2tf, but there the target language is very far from HLO. Do we really need to do the same for IREE? ' by a COLLABORATOR of type User on 2021-08-10T10:42:22Z\n'Fwiw, I thought as you but was advised differently by Brennan (who says he\nspoke to Matt about this).\n\nI think the table of converters from Jax primitive to chlo/mhlo is the\nleast interesting part of this PR and I wish that were just some code\nshared with Jax core that I could reach in and use in some way (ie. An API\nto provide the op namespaces and build the basic conversion table). Also,\nsome more care could be taken with that than I've done here in order to\nattempt to reduce duplication with the existing HLO proto path (although if\nwe thought that might eventually be replaced by the mhlo path, we probably\ndon't worry about duplication and just code it in the best way).\n\nHowever, since none of that existed, I just wrote the simplest thing to\nshow the approach.\n\nAssuming that such a converter table API existed (or could graduate out of\nthis work), this is a pretty lightweight PR and what is left is most likely\nneeded. IREE's IR has more features than XLA, especially for things related\nto deployment, and we will want to introduce new primitives for python\ndefined kernels, control the way the function boundaries are emitted,\nhandle access to globals, etc. All of those reasons are why we find the\nexisting HLO proto path unsuitable and I think are worth the extra code.\n\nI don't relish building out the primitive conversion tables, but if that is\nthe price of a high quality implementation, it's not *that* hard. It would\nbe nice to share most of it, though.\n\nOn Tue, Aug 10, 2021, 3:42 AM George Necula ***@***.***>\nwrote:\n\n> Thanks for getting this started!\n>\n> I was expecting that an IREE integration will be more lightweight from a\n> JAX point of view: export the HLO module, invoke the IREE compiler, plus\n> some runtime integration to run the executables. The current PR seems to\n> use a custom trace to convert each JAX primitive separately. I think that\n> this is very costly: a lot of conversion rules some of which will be very\n> tricky. We do follow this strategy for jax2tf, but there the target\n> language is very far from HLO. Do we really need to do the same for IREE?\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/7425#issuecomment-895923751>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADYVADWWUM4AITSZ5HMW7TT4D7BRANCNFSM5BJLY6HA>\n> .\n>\n' by a CONTRIBUTOR of type User on 2021-08-10T13:04:57Z\n'Also, I'm about to go out on vacation for a few weeks, and I'm in no hurry to move this forward right this minute. What's here is literally a few hours of coding and if it serves to solidify a plan that I/someone can pick up and start working on in September, it will have done its job, from my perspective.' by a CONTRIBUTOR of type User on 2021-08-10T13:20:19Z\n'On Tue, Aug 10, 2021 at 9:05 AM Stella Laurenzo ***@***.***>\nwrote:\n\n> Assuming that such a converter table API existed (or could graduate out of\n> this work), this is a pretty lightweight PR and what is left is most likely\n> needed. IREE's IR has more features than XLA, especially for things related\n> to deployment, and we will want to introduce new primitives for python\n> defined kernels, control the way the function boundaries are emitted,\n> handle access to globals, etc. All of those reasons are why we find the\n> existing HLO proto path unsuitable and I think are worth the extra code.\n>\n\n\nHowever what you could do is bootstrap your translation rules from the\nexisting translation rules. For every primitive, at least for those not\nparameterized by other computations, you can create a translation rule that\ncalls the existing HLO translation rule and translate its output. You can\nthen replace the rules one-by-one as you need. This should get you fairly\ncomplete coverage for a fairly low initial cost.\n\nSome of the translation rules aren't trivial. Not all JAX primitives are\none-to-one with HLO constructs. You can find the most interesting examples\nof this by searching for xla.lower_fun() which allows you to call back into\nthe tracing machinery in a translation rule: no-one wants to write\nnon-trivial code in the builder API so the non-trivial code typically uses\nthis facility to embed JAX code in a translation rule.  For an example of\nthis, see, for example, how `cumsum` is implemented:\nhttps://github.com/google/jax/blob/a93eaf3c9ef7db7bfaf419953adcd37e81a7b5f7/jax/_src/lax/control_flow.py#L2644\nNow admittedly that one probably should have an HLO primitive but it\ndoesn't.\n\nI mention this because you'd get this for free by calling into the existing\ntranslation rules but you'd have to do some work to make it work without\nthat.\n\n\n> I don't relish building out the primitive conversion tables, but if that is\n> the price of a high quality implementation, it's not *that* hard. It would\n> be nice to share most of it, though.\n>\n\nAnother suggestion:\n\nI'd suggest actually implementing the JAX runtime API so IREE can\ntransparently integrate with the rest of JAX. (e.g., things like `jit(f)`\nwill just work but backed by IREE instead of XLA).\n\nYou can either implement something that duck-types as the Python API:\nhttps://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/python/xla_extension/__init__.pyi#L295\nor implement its C++ version:\nhttps://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/pjrt/pjrt_client.h\n\nYou can implement either, although I'd probably recommend the C++ version\nso you can get access to low-latency JIT dispatch. In essence you would\nneed to implement four classes (Client, Device, Buffer, Executable). Many\nmethods don't need to be implemented (e.g., infeed/outfeed, channels,\ncross-device transfers, etc.) so don't get dissuaded by the fact some APIs\nlook awfully TPU-specific.\n\nThere are a few places where the API may be a little too XLA-specific.\nThat's fine, we can fix that. The most notable one is that Compile() takes\nan XLA computation, but we could, say, make that some sort of opaque object\nor add an overload that takes mHLO.\n\nPeter\n\n\n> On Tue, Aug 10, 2021, 3:42 AM George Necula ***@***.***>\n> wrote:\n>\n> > Thanks for getting this started!\n> >\n> > I was expecting that an IREE integration will be more lightweight from a\n> > JAX point of view: export the HLO module, invoke the IREE compiler, plus\n> > some runtime integration to run the executables. The current PR seems to\n> > use a custom trace to convert each JAX primitive separately. I think that\n> > this is very costly: a lot of conversion rules some of which will be very\n> > tricky. We do follow this strategy for jax2tf, but there the target\n> > language is very far from HLO. Do we really need to do the same for IREE?\n> >\n> > —\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/google/jax/pull/7425#issuecomment-895923751>, or\n> > unsubscribe\n> > <\n> https://github.com/notifications/unsubscribe-auth/AADYVADWWUM4AITSZ5HMW7TT4D7BRANCNFSM5BJLY6HA\n> >\n> > .\n> >\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/7425#issuecomment-896010288>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AACVGBG5Y2AJHJ2RY5HSWBLT4EPYNANCNFSM5BJLY6HA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n' by a COLLABORATOR of type User on 2021-08-10T13:36:43Z\n'> On Tue, Aug 10, 2021 at 9:05 AM Stella Laurenzo ***@***.***> wrote:\r\n> Assuming that such a converter table API existed (or could graduate out of this work), this is a pretty lightweight PR and what is left is most likely needed. IREE's IR has more features than XLA, especially for things related to deployment, and we will want to introduce new primitives for python defined kernels, control the way the function boundaries are emitted, handle access to globals, etc. All of those reasons are why we find the existing HLO proto path unsuitable and I think are worth the extra code.\r\n> However what you could do is bootstrap your translation rules from the existing translation rules. For every primitive, at least for those not parameterized by other computations, you can create a translation rule that calls the existing HLO translation rule and translate its output. You can then replace the rules one-by-one as you need. This should get you fairly complete coverage for a fairly low initial cost. Some of the translation rules aren't trivial. \r\n\r\nWere you thinking of literally calling the existing machinery, producing an XlaComputation and then converting that to  equivalent MHLO ops? Some of that code does exist but it is quite far in terms of build systems and deps (and is old/brittle in other ways that add tax)... and then we are in the space of evaluating how much to invest in bootstrap paths, etc (esp. for OSS builds where dependency boundaries are strict and high cost). One possible simplification: we implement a duck-typed implementation of XlaBuilder in the MHLO repo and have a way to inject that JAX-side. There'd still be typing involved but it would probably be pretty formulaic and relatively easy to rip through and/or meta-program. We could either implement that in Python or as a pybind module against the C-API.\r\n\r\n> Not all JAX primitives are one-to-one with HLO constructs. You can find the most interesting examples of this by searching for xla.lower_fun() which allows you to call back into the tracing machinery in a translation rule: no-one wants to write non-trivial code in the builder API so the non-trivial code typically uses this facility to embed JAX code in a translation rule. For an example of this, see, for example, how `cumsum` is implemented: https://github.com/google/jax/blob/a93eaf3c9ef7db7bfaf419953adcd37e81a7b5f7/jax/_src/lax/control_flow.py#L2644 Now admittedly that one probably should have an HLO primitive but it doesn't. I mention this because you'd get this for free by calling into the existing translation rules but you'd have to do some work to make it work without that.\r\n\r\nNice - that is how it should be (imo).\r\n\r\n> I don't relish building out the primitive conversion tables, but if that is the price of a high quality implementation, it's not *that* hard. It would be nice to share most of it, though.\r\n> Another suggestion: I'd suggest actually implementing the JAX runtime API so IREE can transparently integrate with the rest of JAX. (e.g., things like `jit(f)` will just work but backed by IREE instead of XLA). You can either implement something that duck-types as the Python API: https://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/python/xla_extension/__init__.pyi#L295 or implement its C++ version: https://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/pjrt/pjrt_client.h You can implement either, although I'd probably recommend the C++ version so you can get access to low-latency JIT dispatch. In essence you would need to implement four classes (Client, Device, Buffer, Executable). \r\n\r\nThanks for these references. A C++ implementation is likely where we need to land, but I'm tempted to start with an iteration in pure Python just for the value of seeing something connect a bit more easily (to my eyes). IREE's Python runtime API is really just a toy for functional evaluation, not for a serious integration with a production stack (it takes shortcuts with its modeling of host/device memory, has limited support for multi-device, doesn't expose most of the concurrency/synchronization primitives, etc) -- however, it makes up for that with a lot of convenience :)  IREE's C-API models everything but is, of course, harder to use. There are also real design questions that come up when looking at bridging IREE-C+PJRT in the most efficient way possible, and I would feel better about having something/anything working to compare against while working those out.\r\n\r\nConversely, the IREE/MLIR-based compilers are easier to integrate with from Python if you are already largely homed in Python (as JAX is). There is also a C-API, and we did take that approach [with PyTorch](https://github.com/llvm/mlir-npcomp/tree/main/frontends/pytorch/csrc/builder), although, in that case, it is because PyTorch's IR is implemented in C++ and that is where you get the most unfettered access to it. The complexity and code cost is high, however. For JAX, I think we can largely stay at the Python level for the main compiler and then just use that from the runtime APIs related to compilation. That also makes it easier to do some more interesting things. Do you see any issues with that path? \r\n\r\n> Many methods don't need to be implemented (e.g., infeed/outfeed, channels, cross-device transfers, etc.) so don't get dissuaded by the fact some APIs look awfully TPU-specific. There are a few places where the API may be a little too XLA-specific. That's fine, we can fix that. The most notable one is that Compile() takes an XLA computation, but we could, say, make that some sort of opaque object or add an overload that takes mHLO. Peter\r\n> […](#)\r\n\r\nSince I'm new to your codebase, could you provide some code pointers to what on the JAX side invokes this runtime layer? Reading that code would help my mental model and maybe make me useful in offering concrete suggestions for generalizing.\r\n' by a CONTRIBUTOR of type User on 2021-08-10T15:06:37Z\n'On Tue, Aug 10, 2021, 08:06 Stella Laurenzo ***@***.***>\nwrote:\n\n> On Tue, Aug 10, 2021 at 9:05 AM Stella Laurenzo *@*.***> wrote:\n> Assuming that such a converter table API existed (or could graduate out of\n> this work), this is a pretty lightweight PR and what is left is most likely\n> needed. IREE's IR has more features than XLA, especially for things related\n> to deployment, and we will want to introduce new primitives for python\n> defined kernels, control the way the function boundaries are emitted,\n> handle access to globals, etc. All of those reasons are why we find the\n> existing HLO proto path unsuitable and I think are worth the extra code.\n> However what you could do is bootstrap your translation rules from the\n> existing translation rules. For every primitive, at least for those not\n> parameterized by other computations, you can create a translation rule that\n> calls the existing HLO translation rule and translate its output. You can\n> then replace the rules one-by-one as you need. This should get you fairly\n> complete coverage for a fairly low initial cost. Some of the translation\n> rules aren't trivial.\n>\n> Were you thinking of literally calling the existing machinery, producing\n> an XlaComputation and then converting that to equivalent MHLO ops? Some of\n> that code does exist but it is quite far in terms of build systems and deps\n> (and is old/brittle in other ways that add tax)... and then we are in the\n> space of evaluating how much to invest in bootstrap paths, etc (esp. for\n> OSS builds where dependency boundaries are strict and high cost). One\n> possible simplification: we implement a duck-typed implementation of\n> XlaBuilder in the MHLO repo and have a way to inject that JAX-side.\n>\nNot sure if this is what you mean, but we have an XlaBuilder variant that\nproduces MHLO already. I think it is just a change in constructor of the\nbuilder (of course there may be a caveat with nested builders as we haven't\nused that much, but easy to add). We could effectively move that variant\ninto MHLO side, it could get tricky with helper functions (but not too\nbad/those should perhaps be MHLOs and can expand later, sort of as Peter\nmentioned some of them should be ops).\n\n> There'd still be typing involved but it would probably be pretty formulaic\n> and relatively easy to rip through and/or meta-program. We could either\n> implement that in Python or as a pybind module against the C-API.\n>\n> Not all JAX primitives are one-to-one with HLO constructs. You can find\n> the most interesting examples of this by searching for xla.lower_fun()\n> which allows you to call back into the tracing machinery in a translation\n> rule: no-one wants to write non-trivial code in the builder API so the\n> non-trivial code typically uses this facility to embed JAX code in a\n> translation rule. For an example of this, see, for example, how cumsum is\n> implemented:\n> https://github.com/google/jax/blob/a93eaf3c9ef7db7bfaf419953adcd37e81a7b5f7/jax/_src/lax/control_flow.py#L2644\n> Now admittedly that one probably should have an HLO primitive but it\n> doesn't. I mention this because you'd get this for free by calling into the\n> existing translation rules but you'd have to do some work to make it work\n> without that.\n>\n> Nice - that is how it should be (imo).\n>\n> I don't relish building out the primitive conversion tables, but if that\n> is the price of a high quality implementation, it's not *that* hard. It\n> would be nice to share most of it, though.\n> Another suggestion: I'd suggest actually implementing the JAX runtime API\n> so IREE can transparently integrate with the rest of JAX. (e.g., things\n> like jit(f) will just work but backed by IREE instead of XLA). You can\n> either implement something that duck-types as the Python API:\n> https://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/python/xla_extension/__init__.pyi#L295\n> or implement its C++ version:\n> https://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/pjrt/pjrt_client.h\n> You can implement either, although I'd probably recommend the C++ version\n> so you can get access to low-latency JIT dispatch. In essence you would\n> need to implement four classes (Client, Device, Buffer, Executable).\n>\n> Thanks for these references. A C++ implementation is likely where we need\n> to land, but I'm tempted to start with an iteration in pure Python just for\n> the value of seeing something connect a bit more easily (to my eyes).\n> IREE's Python runtime API is really just a toy for functional evaluation,\n> not for a serious integration with a production stack (it takes shortcuts\n> with its modeling of host/device memory, has limited support for\n> multi-device, doesn't expose most of the concurrency/synchronization\n> primitives, etc) -- however, it makes up for that with a lot of convenience\n> :) IREE's C-API models everything but is, of course, harder to use. There\n> are also real design questions that come up when looking at bridging\n> IREE-C+PJRT in the most efficient way possible, and I would feel better\n> about having something/anything working to compare against while working\n> those out.\n>\n> Conversely, the IREE/MLIR-based compilers are easier to integrate with\n> from Python if you are already largely homed in Python (as JAX is). There\n> is also a C-API, and we did take that approach with PyTorch\n> <https://github.com/llvm/mlir-npcomp/tree/main/frontends/pytorch/csrc/builder>,\n> although, in that case, it is because PyTorch's IR is implemented in C++\n> and that is where you get the most unfettered access to it. The complexity\n> and code cost is high, however. For JAX, I think we can largely stay at the\n> Python level for the main compiler and then just use that from the runtime\n> APIs related to compilation. That also makes it easier to do some more\n> interesting things. Do you see any issues with that path?\n>\n> Many methods don't need to be implemented (e.g., infeed/outfeed, channels,\n> cross-device transfers, etc.) so don't get dissuaded by the fact some APIs\n> look awfully TPU-specific. There are a few places where the API may be a\n> little too XLA-specific. That's fine, we can fix that. The most notable one\n> is that Compile() takes an XLA computation, but we could, say, make that\n> some sort of opaque object or add an overload that takes mHLO. Peter\n> … <#m_-3189520145904455374_>\n>\n> Since I'm new to your codebase, could you provide some code pointers to\n> what on the JAX side invokes this runtime layer? Reading that code would\n> help my mental model and maybe make me useful in offering concrete\n> suggestions for generalizing.\n>\n> —\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/7425#issuecomment-896110089>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFMRTV3PVFNAMOUKD4TS5DT4E6ATANCNFSM5BJLY6HA>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n> .\n>\n' by a NONE of type User on 2021-08-10T15:16:51Z\n'> On Tue, Aug 10, 2021, 08:06 Stella Laurenzo ***@***.***> wrote:\r\n> On Tue, Aug 10, 2021 at 9:05 AM Stella Laurenzo *@*.***> wrote: Assuming that such a converter table API existed (or could graduate out of this work), this is a pretty lightweight PR and what is left is most likely needed. IREE's IR has more features than XLA, especially for things related to deployment, and we will want to introduce new primitives for python defined kernels, control the way the function boundaries are emitted, handle access to globals, etc. All of those reasons are why we find the existing HLO proto path unsuitable and I think are worth the extra code. However what you could do is bootstrap your translation rules from the existing translation rules. For every primitive, at least for those not parameterized by other computations, you can create a translation rule that calls the existing HLO translation rule and translate its output. You can then replace the rules one-by-one as you need. This should get you fairly complete coverage for a fairly low initial cost. Some of the translation rules aren't trivial. Were you thinking of literally calling the existing machinery, producing an XlaComputation and then converting that to equivalent MHLO ops? Some of that code does exist but it is quite far in terms of build systems and deps (and is old/brittle in other ways that add tax)... and then we are in the space of evaluating how much to invest in bootstrap paths, etc (esp. for OSS builds where dependency boundaries are strict and high cost). One possible simplification: we implement a duck-typed implementation of XlaBuilder in the MHLO repo and have a way to inject that JAX-side.\r\n> Not sure if this is what you mean, but we have an XlaBuilder variant that produces MHLO already. I think it is just a change in constructor of the builder (of course there may be a caveat with nested builders as we haven't used that much, but easy to add). We could effectively move that variant into MHLO side, it could get tricky with helper functions (but not too bad/those should perhaps be MHLOs and can expand later, sort of as Peter mentioned some of them should be ops).\r\n\r\nAh, I didn't know we'd made progress on that (I know it had been discussed for a long time). I'll ping you and see if some code organization could make it easier/possible to leverage.\r\n\r\n> […](#)\r\n> There'd still be typing involved but it would probably be pretty formulaic and relatively easy to rip through and/or meta-program. We could either implement that in Python or as a pybind module against the C-API. Not all JAX primitives are one-to-one with HLO constructs. You can find the most interesting examples of this by searching for xla.lower_fun() which allows you to call back into the tracing machinery in a translation rule: no-one wants to write non-trivial code in the builder API so the non-trivial code typically uses this facility to embed JAX code in a translation rule. For an example of this, see, for example, how cumsum is implemented: https://github.com/google/jax/blob/a93eaf3c9ef7db7bfaf419953adcd37e81a7b5f7/jax/_src/lax/control_flow.py#L2644 Now admittedly that one probably should have an HLO primitive but it doesn't. I mention this because you'd get this for free by calling into the existing translation rules but you'd have to do some work to make it work without that. Nice - that is how it should be (imo). I don't relish building out the primitive conversion tables, but if that is the price of a high quality implementation, it's not *that* hard. It would be nice to share most of it, though. Another suggestion: I'd suggest actually implementing the JAX runtime API so IREE can transparently integrate with the rest of JAX. (e.g., things like jit(f) will just work but backed by IREE instead of XLA). You can either implement something that duck-types as the Python API: https://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/python/xla_extension/__init__.pyi#L295 or implement its C++ version: https://github.com/tensorflow/tensorflow/blob/76d68c3f2ce78168ffb59f43c39e1ee718c24be1/tensorflow/compiler/xla/pjrt/pjrt_client.h You can implement either, although I'd probably recommend the C++ version so you can get access to low-latency JIT dispatch. In essence you would need to implement four classes (Client, Device, Buffer, Executable). Thanks for these references. A C++ implementation is likely where we need to land, but I'm tempted to start with an iteration in pure Python just for the value of seeing something connect a bit more easily (to my eyes). IREE's Python runtime API is really just a toy for functional evaluation, not for a serious integration with a production stack (it takes shortcuts with its modeling of host/device memory, has limited support for multi-device, doesn't expose most of the concurrency/synchronization primitives, etc) -- however, it makes up for that with a lot of convenience :) IREE's C-API models everything but is, of course, harder to use. There are also real design questions that come up when looking at bridging IREE-C+PJRT in the most efficient way possible, and I would feel better about having something/anything working to compare against while working those out. Conversely, the IREE/MLIR-based compilers are easier to integrate with from Python if you are already largely homed in Python (as JAX is). There is also a C-API, and we did take that approach with PyTorch <https://github.com/llvm/mlir-npcomp/tree/main/frontends/pytorch/csrc/builder>, although, in that case, it is because PyTorch's IR is implemented in C++ and that is where you get the most unfettered access to it. The complexity and code cost is high, however. For JAX, I think we can largely stay at the Python level for the main compiler and then just use that from the runtime APIs related to compilation. That also makes it easier to do some more interesting things. Do you see any issues with that path? Many methods don't need to be implemented (e.g., infeed/outfeed, channels, cross-device transfers, etc.) so don't get dissuaded by the fact some APIs look awfully TPU-specific. There are a few places where the API may be a little too XLA-specific. That's fine, we can fix that. The most notable one is that Compile() takes an XLA computation, but we could, say, make that some sort of opaque object or add an overload that takes mHLO. Peter … <#m_-3189520145904455374_> Since I'm new to your codebase, could you provide some code pointers to what on the JAX side invokes this runtime layer? Reading that code would help my mental model and maybe make me useful in offering concrete suggestions for generalizing. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#7425 (comment)](https://github.com/google/jax/pull/7425#issuecomment-896110089)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAFMRTV3PVFNAMOUKD4TS5DT4E6ATANCNFSM5BJLY6HA> . Triage notifications on the go with GitHub Mobile for iOS <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email> .\r\n\r\n' by a CONTRIBUTOR of type User on 2021-08-10T15:19:07Z\n'> > On Tue, Aug 10, 2021, 08:06 Stella Laurenzo _**@**_._**> wrote:\r\n> > On Tue, Aug 10, 2021 at 9:05 AM Stella Laurenzo _@_.**_> wrote: Assuming that such a converter table API existed (or could graduate out of this work), this is a pretty lightweight PR and what is left is most likely needed. IREE's IR has more features than XLA, especially for things related to deployment, and we will want to introduce new primitives for python defined kernels, control the way the function boundaries are emitted, handle access to globals, etc. All of those reasons are why we find the existing HLO proto path unsuitable and I think are worth the extra code. However what you could do is bootstrap your translation rules from the existing translation rules. For every primitive, at least for those not parameterized by other computations, you can create a translation rule that calls the existing HLO translation rule and translate its output. You can then replace the rules one-by-one as you need. This should get you fairly complete coverage for a fairly low initial cost. Some of the translation rules aren't trivial. Were you thinking of literally calling the existing machinery, producing an XlaComputation and then converting that to equivalent MHLO ops? Some of that code does exist but it is quite far in terms of build systems and deps (and is old/brittle in other ways that add tax)... and then we are in the space of evaluating how much to invest in bootstrap paths, etc (esp. for OSS builds where dependency boundaries are strict and high cost). One possible simplification: we implement a duck-typed implementation of XlaBuilder in the MHLO repo and have a way to inject that JAX-side.\r\n> > Not sure if this is what you mean, but we have an XlaBuilder variant that produces MHLO already. I think it is just a change in constructor of the builder (of course there may be a caveat with nested builders as we haven't used that much, but easy to add). We could effectively move that variant into MHLO side, it could get tricky with helper functions (but not too bad/those should perhaps be MHLOs and can expand later, sort of as Peter mentioned some of them should be ops).\r\n> \r\n> Ah, I didn't know we'd made progress on that (I know it had been discussed for a long time). I'll ping you and see if some code organization could make it easier/possible to leverage.\r\n\r\nOk, chatted with Jacques offline: https://github.com/tensorflow/tensorflow/blob/dca928101ccac62ff8cdd4a625541b50e76e64a9/tensorflow/compiler/mlir/xla/ir/mlir_hlo_builder.h\r\n\r\nThe way this is layered will make it a pretty awkward integration for anything out-of-tree/out-of-tensorflow. However, we could do some plumbing to use it as a bootstrap mechanism as Peter suggests (at a performance cost -- but fine if a means to an end and likely not a dominant cost on day 1 anyway). The taxes will accrue a bit and may not be worth it when considering that IREE operates on MHLO that has been significantly sanitized (de-tupleized, control flow flattening, etc). The passes that do that are in our XLA importer, not in the IREE compiler itself, and they undo things that are a consequence of fitting through the existing XLA surface area). All doable, but cost/benefit of re-use has to be weighed.\r\n\r\nI'll also point out that this is something like the 4th frontend that we've worked on, and a surface area the size of XlaBuilder is the least frightening of the things we've seen when considering the cost if it just needs to be replaced in full. In my experience, this is the size/scope of thing that you pick a new general purpose structure for and then migrate to over a few iterations vs spending a lot of time trying to leverage in-situ. I can't quite picture the exact API, but it would be really nice if JAX had more plugability for out-of-tree projects at this level. I think it would get used and be a good investment to layer this a bit more for extensibility. Just my 2cents as someone a couple of steps outside of the ecosystem.' by a CONTRIBUTOR of type User on 2021-08-10T16:25:02Z\n'\r\n> Since I'm new to your codebase, could you provide some code pointers to what on the JAX side invokes this runtime layer? Reading that code would help my mental model and maybe make me useful in offering concrete suggestions for generalizing.\r\n\r\nThis may help: https://gist.github.com/hawkinsp/bcbd8375177a5249f2bf8de6a7ef0cda\r\n\r\nThis is a toy backend that doesn't actually compile anything but instead hardwires in a particular computation. However it shows you how to register a backend and use it to run a trivial computation. If you have a utility to convert an XLA HLO proto into IREE, you could probably use this to plug in IREE without too much further complication as a first step.\r\n\r\n(This is mostly orthogonal to what you have done in this PR, which is about avoiding the HLO step.)\r\n' by a COLLABORATOR of type User on 2021-08-10T18:17:39Z\n'Thanks, this is the piece I was missing! Let me give this a shot with our\ncurrent mechanisms for bridging HLO and see where we go.\n\nOn Tue, Aug 10, 2021, 11:17 AM Peter Hawkins ***@***.***>\nwrote:\n\n> Since I'm new to your codebase, could you provide some code pointers to\n> what on the JAX side invokes this runtime layer? Reading that code would\n> help my mental model and maybe make me useful in offering concrete\n> suggestions for generalizing.\n>\n> This may help:\n> https://gist.github.com/hawkinsp/bcbd8375177a5249f2bf8de6a7ef0cda\n>\n> This is a toy backend that doesn't actually compile anything but instead\n> hardwires in a particular computation. However it shows you how to register\n> a backend and use it to run a trivial computation. If you have a utility to\n> convert an XLA HLO proto into IREE, you could probably use this to plug in\n> IREE without too much further complication as a first step.\n>\n> (This is mostly orthogonal to what you have done in this PR, which is\n> about avoiding the HLO step.)\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/7425#issuecomment-896210821>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADYVAAC6WS7AYMKFJCFXQTT4FUM5ANCNFSM5BJLY6HA>\n> .\n>\n' by a CONTRIBUTOR of type User on 2021-08-10T20:15:42Z\n'FYI - I'll be on vacation for a couple of weeks and will pick this up when back.' by a CONTRIBUTOR of type User on 2021-08-12T20:31:24Z\n'I'm back from vacation and am getting caught back up on this. I've gone ahead and landed what is here so far into IREE's samples repository so I can iterate on it a bit more: https://github.com/google/iree-samples/tree/main/pre-release/python\r\n\r\nI'll rework it now to add the discussed runtime interop and a JAXPR interpreter, then we can discuss where to take it.' by a CONTRIBUTOR of type User on 2021-09-03T20:33:29Z\n'> > Since I'm new to your codebase, could you provide some code pointers to what on the JAX side invokes this runtime layer? Reading that code would help my mental model and maybe make me useful in offering concrete suggestions for generalizing.\r\n> \r\n> This may help: https://gist.github.com/hawkinsp/bcbd8375177a5249f2bf8de6a7ef0cda\r\n> \r\n> This is a toy backend that doesn't actually compile anything but instead hardwires in a particular computation. However it shows you how to register a backend and use it to run a trivial computation. If you have a utility to convert an XLA HLO proto into IREE, you could probably use this to plug in IREE without too much further complication as a first step.\r\n> \r\n> (This is mostly orthogonal to what you have done in this PR, which is about avoiding the HLO step.)\r\n\r\nThanks for this sample. Here is our iree-samples commit with it working: https://github.com/google/iree-samples/commit/d0f266a2bf0432e4ea71302c8c3b03b43c72cc9d\r\n\r\nThis is a toy implementation for a few reasons:\r\n\r\n* Just marshalling through numpy ndarray vs using lower level buffers and avoiding copies.\r\n* Uses our HLO->MLIR utility which involves a lot of ASM munging and a process jump.\r\n* Hard-codes device setup and such (just for a single CPU).\r\n* Uses the synchronous Python API vs the lower level C-API, which allows for async dispatch and corresponding buffer lifetime, etc: If ever making this more than a quick way to evaluate IREE, probably want to drop down to C.\r\n\r\nI need to dive in and see how to redirect this away from its reliance on an XlaComputation. Maybe some kind of higher level hook that delegates to the normal Xla way (haven't grok'd the code at all).' by a CONTRIBUTOR of type User on 2021-09-03T23:53:25Z\n'Closing this PR because I think the IREE and MHLO integration that are checked in replace it (e.g,.  https://github.com/google/jax/pull/8529 )' by a COLLABORATOR of type User on 2021-11-29T15:32:58Z\n\nPR has review comments:\n'Dead? Or did you intend to call this from fabs?' by a CONTRIBUTOR of type User on 2021-08-03T17:36:28Z\n'Haha, perhaps TODO: Switch on dtype and build appropriate type rather than just uniformly returning f32. f32 is purely for initial commit to create e2e thin slice.' by a CONTRIBUTOR of type User on 2021-08-03T17:39:56Z\n'Do you need to guard here against unranked?' by a CONTRIBUTOR of type User on 2021-08-03T17:40:49Z\n'OOC when would this not be needed in this flow? E.g., with tracing it would seem that we'd always reach return type last and so would not know what type we return until end [exception I could see is control flow]' by a CONTRIBUTOR of type User on 2021-08-03T17:44:04Z\n'What is this used for? Oh yes I think you mentioned this is just for testing to avoid triggering any optimizations etc while bootstrapping. Is that correct?' by a CONTRIBUTOR of type User on 2021-08-03T17:50:35Z\n'Tablegen the \"old friend\" :) (mlir-hs this has worked quite well for the simple cases)' by a CONTRIBUTOR of type User on 2021-08-03T17:51:00Z\n'Done right, it would be a real debugging aid: For unrepresented ops, actually emit them for inspection (with location info) vs hard-erroring on the first such and making someone parse python exception logs.' by a CONTRIBUTOR of type User on 2021-08-06T19:19:04Z\n'I mean, I was thinking some Python goo. There will end up being a few types of ops that dominate and we should have helpers for those.' by a CONTRIBUTOR of type User on 2021-08-06T19:19:40Z\n'This is a fine way of constructing the trace, but it might be simpler to first trace the function to jaxpr and then write a jaxpr -> IREE module translation (really just a small interpreter). Traces are ok for simple cases but will get more hairy once you get to handling call/map primitives' by a COLLABORATOR of type User on 2021-08-25T14:14:15Z\n'Thanks, Adam - that was my initial instinct as well until nudged in this direction :) Perhaps seeing a prototype of the jaxpr approach would help converge opinions here? I can knock such a thing together when back from vacation if there isn't a strong opinion by then. If we're still trying to converge approach, I may move this to our iree-sampled repo so we can land and see different options.\r\n\r\nOthers, opinions welcome -- this isn't my codebase and the feedback is good for understanding the grain of it better.' by a CONTRIBUTOR of type User on 2021-08-25T17:27:41Z\n'If you use [`based_on_style = yapf`](https://github.com/google/yapf#formatting-style) that will avoid the annoying discrepancies between the internal formatter and the `\"google\"` style (which include some disagreements about hanging indentation on top of the `column_limit` and `indent_width` below).' by a CONTRIBUTOR of type User on 2021-08-25T21:45:39Z\n'We used to have an allergy to jaxpr interpreters but these days we start embracing that style much more. [Here's a very nice tutorial](https://github.com/google/jax/blob/main/docs/notebooks/Writing_custom_interpreters_in_Jax.ipynb). In any case feel free to ping me, I'm happy to help with this part.' by a COLLABORATOR of type User on 2021-08-26T13:44:44Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 18,
        "num_review_comments": 12,
        "reason_for_closure": "Superseded by other changes: The functionality proposed in the PR was ultimately implemented via a different approach, making this PR redundant."
    },
    {
        "summary": "Pull Request '7366' titled 'Fix crash in concretization of ad_util.Zero' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-07-23T21:04:34Z, and was closed at 2021-07-25T11:53:17Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'I propose that we close this PR. I think that the fix is in #7378 ' by a COLLABORATOR of type User on 2021-07-25T11:53:11Z\n\nPR has review comments:\n'This does not look right.  `pytype_aval_mappings` should contain functions that return `core.AbstractValue`. \r\n\r\nI think it would be helpful to come up with a JAX test case that demonstrates this. The test case in #7340 is too complicated. ' by a COLLABORATOR of type User on 2021-07-24T14:53:40Z\n'> This does not look right. pytype_aval_mappings should contain functions that return core.AbstractValue\r\n\r\nOh, I see.  How do we do that from an object of type `Zero`?  Do we just return `z.aval`?\r\n\r\n> I think it would be helpful to come up with a JAX test case that demonstrates this.\r\n\r\nI totally agree.  The issue is that it's incredibly time consuming for me to create MWEs since I have to basically strip out chunks of the giant program until I have a MWE.  Is it possible to look at the tracer types and know how to produce such tracers with a minimal program?' by a CONTRIBUTOR of type User on 2021-07-24T19:39:50Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Superseded by other work: PR closed as the fix was implemented in a different PR."
    },
    {
        "summary": "Pull Request '6993' titled 'Model effects in Jaxpr with an effectful tag.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-06-16T20:21:43Z, and was closed at 2022-06-29T19:43:08Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'I suspect this is subsumed by @sharadmv 's effect work.' by a COLLABORATOR of type User on 2022-06-29T19:43:08Z\n\nPR has review comments:\n'I think `rng_uniform_p` is effectful, but `threefry2x32_p` should be pure?' by a CONTRIBUTOR of type User on 2021-07-01T08:19:29Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Subsumed by other work."
    },
    {
        "summary": "Pull Request '6978' titled 'get_cache_key implementation with unit tests' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-06-15T15:37:19Z, and was closed at 2023-11-06T12:53:53Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nPR has comments:\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F6978) for more info**.\n\n<!-- need_author_cla -->' by a NONE of type Bot on 2021-06-16T22:03:10Z\n\nPR has review comments:\n'You don't need to define a class at all! `get_cache_key` can be a standalone function, since there are no class variables needed. Similarly, the helper functions can also be standalone functions.' by a MEMBER of type User on 2021-06-15T16:46:39Z\n'Call this function `_hash_compile_options`, which I think is more descriptive (it's very common to have a verb in a function name, to describe what the function does). Use the same pattern for the other helper functions too (`_hash_blah`)' by a MEMBER of type User on 2021-06-15T16:48:25Z\n'Nice use of `map`!\r\n\r\nStyle nit: add a line break after `lambda shape: hash_obj.update(shape.to_serialized_proto()),` (i.e. right after the comma) to break up the long line and make it easier to see the two arguments to `map`.' by a MEMBER of type User on 2021-06-15T16:53:29Z\n'This `to_bytes` logic is just complicated enough to warrant its own helper function IMO. Please create `_hash_bool` and `_hash_int` helper functions and call those instead of calling `to_bytes` each time, and test them. This will avoid repeating the byteorder, etc. multiple times, and will make this function easier to read.' by a MEMBER of type User on 2021-06-15T16:56:58Z\n'Same here, I think it's worth making a `_hash_string` helper method (with a test).' by a MEMBER of type User on 2021-06-15T16:57:49Z\n'Please add a docstring to this method briefly explaining what it does and, more importantly, giving an example of what the output looks like (like give an example output string). This will help readers understand what this function actually does, and what to expect with the returned cache key.\r\n\r\nHere are the official guidelines on writing docstrings: https://google.github.io/styleguide/pyguide.html#381-docstrings\r\n\r\nWe don't exactly adhere to these (e.g. we don't always put the \"Args\", \"Returns\", and \"Raises\" sections described in https://google.github.io/styleguide/pyguide.html#383-functions-and-methods), but you should at least have a short summary line followed by more information in a separate paragraph.' by a MEMBER of type User on 2021-06-15T21:27:44Z\n'- You don't need parentheses around the equality check, since it all fits on one line. It's pretty common to use parens around the entire check + string of an assert in order to break across multiple lines though.\r\n- You should break this into multiple shorter lines :)\r\n- Good job writing an explanatory message. I think we can make it a little more direct though.\r\n- It's always good to print the value(s) causing an assertion failure in the message. This is often useful for debugging.\r\n\r\nPutting it all together, he's my suggestion:\r\n```python\r\n    assert (len(dir(compile_options_obj)) == 31,\r\n            f\"Unexpected number of CompileOption fields: \"\r\n            f\"{len(dir(compile_options_obj))}. This likely means that an extra \"\r\n            f\"field was added, and this function needs to be updated.\")\r\n```' by a MEMBER of type User on 2021-06-15T21:49:14Z\n'This line should be indented to match the lambda, like this:\r\n```python\r\n      map(lambda shape: hash_obj.update(shape.to_serialized_proto()),\r\n          compile_options_obj.argument_layouts)\r\n```\r\n(It's possible you can make vim do this automatically for you, I would ask Matt about it)' by a MEMBER of type User on 2021-06-15T21:52:16Z\n'Put a similar assert like in `_hash_compile_options`?' by a MEMBER of type User on 2021-06-15T22:07:40Z\n'You should add unit tests for these new helper function (int, bool, string)' by a MEMBER of type User on 2021-06-15T22:20:18Z\n'I took out the outer parenthesis too because mypy wasn't happy with them...I was getting the error \"Assert is always True\"...likewise I added parenthesis around the string part in order for python to be happy with the line breaks. ' by a CONTRIBUTOR of type User on 2021-06-16T17:24:03Z\n'Please add a short (<= 80 character) summary line to the beginning of the docstring (see https://google.github.io/styleguide/pyguide.html#381-docstrings for examples).' by a MEMBER of type User on 2021-06-17T00:20:16Z\n'Fix `tets` typo' by a MEMBER of type User on 2021-06-17T00:38:53Z\n'I'm not sure I understand the name of this function (I think you might mean \"pseudo\"?). Even though it's creating the hash_obj, I would name it something that more describes the final output of this function, like `get_hashed_value`. You can also rename `input_parameter` to `input` or `value` (we already know it's a parameter to the function just by looking at it).' by a MEMBER of type User on 2021-06-17T00:49:28Z\n'Can you call `.hex()` inside `create_sudo_hash_obj` instead of calling it repeatedly here and below?' by a MEMBER of type User on 2021-06-17T00:53:20Z\n'Please rename this to `filled_compile_options`. I got confused by the current name because \"fill CompileOptions\" makes me think it should take CompileOptions as input, instead of creating + returning the CompileOptions all in one function.' by a MEMBER of type User on 2021-06-17T00:54:56Z\n'Instead of manually creating a hash here, just call `create_sudo_hash_obj` on `8` again. That way you're testing that your helper function consistently returns the same answer, and you're not including details of how the helper function works (in general, you should be able to change the implementation of the function you're testing without changing the test. You only need to change the test if you change the interface/API of the function, like if you're changing what it does at a high level).\r\n\r\n(Same with similar tests below)' by a MEMBER of type User on 2021-06-17T00:57:03Z\n'Rename this function to `_hash_platform`, and `libtpu_obj` to `backend`.' by a MEMBER of type User on 2021-06-17T01:01:58Z\n'Add a line break to make this line shorter (here and similarly-long lines below)' by a MEMBER of type User on 2021-06-17T01:04:27Z\n'If this test is running on a non-CPU backend, you can get the CPU backend to check the non-equal case! Something like this:\r\n```python\r\nif jax.lib.xla_bridge.get_backend().platform != \"cpu\":\r\n  cpu_backend = jax.lib.xla_bridge.get_backend(\"cpu\")\r\n  <create hash of cpu backend>\r\n  <assertNotEqual to non-cpu backend hash>\r\n```\r\nYou need the if statement to avoid accidentally comparing the same backend and asserting it's not equal when it is.' by a MEMBER of type User on 2021-06-17T01:07:36Z\n'Also test that two different computations with the same compile options produce different hashes' by a MEMBER of type User on 2021-06-17T01:08:44Z\n'Yes, however I am using this helper function for tests like \"test_hash_bool\" and I just want the digest part, not .hex()' by a CONTRIBUTOR of type User on 2021-06-17T15:46:29Z\n'I'm seeing now that the project doc only mentions hashing the platform version value as part of the key...would you like me to take out the other things I am hashing for the libtpu in my main file?' by a CONTRIBUTOR of type User on 2021-06-17T16:02:00Z\n'Can you use just the digest here, or use hex() in the other tests? I guess, when do you need one vs. the other?' by a MEMBER of type User on 2021-06-17T21:48:06Z\n'No I think it's good! I only had the platform_version in mind initially, but the other things don't hurt, and will be helpful if we ever extend the cache to work on non-TPU backends.' by a MEMBER of type User on 2021-06-17T21:49:25Z\n'Don't forget to remove this file from your commit(s) :)' by a MEMBER of type User on 2021-06-17T21:50:19Z\n'No you're right I don't know why I thought that wouldn't work for all the tests...I went ahead and changed that helper function.' by a CONTRIBUTOR of type User on 2021-06-17T22:08:15Z\n'Style int: maybe rename these variables from \"hash_obj\" (and it's actually a string at this point, not the hash object thing) to things like: `filled_hash1`, `filled_hash2`, etc. (and similarly for below tests)' by a MEMBER of type User on 2021-06-18T00:45:21Z\n'this is guaranteed by pytype, is the idea of this test to ensure no crash?' by a COLLABORATOR of type User on 2021-06-18T02:11:09Z\n'Yeah, this test was put in just as an extra check really but I can take it out if its redundant' by a CONTRIBUTOR of type User on 2021-06-18T15:57:29Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 30,
        "reason_for_closure": "Unnecessary: JAX behavior is superset of NumPy; complex inputs are valid."
    },
    {
        "summary": "Pull Request '6949' titled 'Updated jax2tf README.md' was authored by a User, who is associated as a NONE. \nIt was created at 2021-06-11T08:08:57Z, and was closed at 2023-11-20T15:52:35Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Updated jax2tf README.md'\nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F6949) for more info**.\n\n<!-- need_sender_cla -->' by a NONE of type Bot on 2021-06-11T08:09:05Z\n'@googlebot I signed it!' by a NONE of type User on 2021-06-11T08:11:31Z\n'Thanks! I think some of these changes may conflict with https://github.com/google/jax/pull/6941, which was just merged this morning. Can you rebase your PR on the current master branch?' by a COLLABORATOR of type User on 2021-06-11T16:47:03Z\n\nPR has review comments:\n'Revert this change.' by a COLLABORATOR of type User on 2021-06-11T17:42:52Z\n'Please do not mix different kinds of changes in the same PR. Bumping the mypy version and improving the docs are both fine things to do, but please do them in separate PRs that try to do one thing only.' by a COLLABORATOR of type User on 2021-06-11T17:43:36Z\n'This still need to be addressed. I would just revert the mypy change from this PR and send it as a separate PR.\r\n\r\nPlease also squash your commits and fix the title and commit comments to reflect that this is a doc-only change.' by a COLLABORATOR of type User on 2021-06-11T23:00:57Z\n'Revert this change. I think the comma makes it read more clearly.' by a COLLABORATOR of type User on 2021-06-11T23:02:42Z\n'Revert this change. It reads better without the \"the\".' by a COLLABORATOR of type User on 2021-06-11T23:02:58Z\n'It's not clear to me this change is a correct usage of \"semantics\". Revert it.' by a COLLABORATOR of type User on 2021-06-11T23:03:25Z\n'Revert this change. It reads better without the additional comma.' by a COLLABORATOR of type User on 2021-06-11T23:03:55Z\n'Revert this change. \"differently than\" is the correct spelling.' by a COLLABORATOR of type User on 2021-06-11T23:04:18Z\n'There's still an unnecessary change to this file.' by a COLLABORATOR of type User on 2021-06-23T13:12:51Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 9,
        "reason_for_closure": "Inconsistent behavior change: PR deprecates supported functionality, breaking existing users with little benefit."
    },
    {
        "summary": "Pull Request '6842' titled 'QDWH' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-05-26T22:56:45Z, and was closed at 2021-07-20T16:10:59Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F6842) for more info**.\n\n<!-- need_sender_cla -->' by a NONE of type Bot on 2021-05-26T22:56:50Z\n'Is this meant as a replacement for #6751?' by a COLLABORATOR of type User on 2021-05-27T00:04:26Z\n'It is; I'd been instructed to resubmit with my corp account in the hopes this would resolve the CLA issue. Unfortunately it seems not to have.' by a CONTRIBUTOR of type User on 2021-05-27T13:58:50Z\n'@googlebot I signed it!' by a CONTRIBUTOR of type User on 2021-06-16T15:18:18Z\n'@hawkinsp Added the eigh stuff - I can put some more tests in, but for now let's see what we can do about the Jitting' by a CONTRIBUTOR of type User on 2021-07-08T21:28:28Z\n'Merged #7335 which is a squashed version of this PR.' by a COLLABORATOR of type User on 2021-07-20T16:10:59Z\n\nPR has review comments:\n'Would you mind moving the tests into the jax/tests directory?' by a COLLABORATOR of type User on 2021-07-09T16:18:43Z\n'I'd avoid using pytest marks. We don't always use pytest to run tests (e.g., in our internal CI), so the tests need to work without pytest as the test runner, which means you can't use marks.\r\n\r\nThe test should work if you just run `python test_eigh.py` without pytest; you'd probably need to add a `__main__`  and move the tests into a class for this to work.\r\n\r\nTypically we use absl's `parameterized` module for parametric tests.\r\n\r\nTake a look at, say, `lax_numpy_test.py` for some examples.' by a COLLABORATOR of type User on 2021-07-09T16:20:43Z\n'Same here.' by a COLLABORATOR of type User on 2021-07-09T16:20:54Z\n'It looks to me like the polar decomposition is jittable, correct?\r\n\r\nOne thing we could probably do is hook up the polar decomposition to the API (because why not, it's useful) and use the existing `eigh` implementation as the basis of an SVD on TPU, which is simply missing at the moment.\r\n\r\nWe can separately work on making the `eigh` above jittable.' by a COLLABORATOR of type User on 2021-07-09T16:23:01Z\n'I'd be tempted to move this into `jax/_src/lax/qdwh.py`. This is mostly because we want to be able to call it from `jax.lax.linalg` primitives like SVD. Optional, though, I can do that in another PR.' by a COLLABORATOR of type User on 2021-07-09T16:24:16Z\n'Yes, it should be jittable' by a CONTRIBUTOR of type User on 2021-07-09T16:32:41Z\n'Done' by a CONTRIBUTOR of type User on 2021-07-19T15:46:20Z\n'Done' by a CONTRIBUTOR of type User on 2021-07-19T15:46:34Z\n'I'm actually missing where we export `polar` in the `jax.scipy.linalg` namespace. Don't we need to add it to the list in `jax/scipy/linalg.py`' by a COLLABORATOR of type User on 2021-07-19T15:50:35Z\n'isn't it in there?' by a CONTRIBUTOR of type User on 2021-07-19T15:54:47Z\n'The namespace adding is up here' by a CONTRIBUTOR of type User on 2021-07-19T15:56:12Z\n'Hmm I must have missed this.' by a COLLABORATOR of type User on 2021-07-19T15:59:29Z\n'Moved' by a CONTRIBUTOR of type User on 2021-07-19T16:11:38Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 13,
        "reason_for_closure": "Inconsistent :- PR deprecates complex dtype support to match numpy, but JAX doesn't need to be a strict numpy subset.  PR closed as the change was deemed unnecessary."
    },
    {
        "summary": "Pull Request '6763' titled 'Improves jax2tf error message for conv_general_dilated' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-05-17T13:42:14Z, and was closed at 2022-08-30T11:31:53Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'When using `jax2tf` with XLA disabled, `conv_general_dilated` may still convert successfully in some special cases. In those cases where it does not, the variable `info_or_result` contains useful information for debugging why. However, currently this information isn't output. This PR makes sure it is output.\r\n\r\nFor instance, when trying to convert a [ResNet18](https://github.com/google/flax/blob/master/examples/imagenet/models.py#L121) model, the original error message was:\r\n\r\n```\r\nNotImplementedError: Call to conv_general_dilated can only be converted through TFXLA, but XLA is disabled\r\n```\r\n\r\nAfter this PR, the error message is:\r\n\r\n```\r\nNotImplementedError: Could not convert conv_general_dilated without using XLA, reason: tf.nn.convolution is not supported for dtype <dtype: 'bfloat16'>\r\n```\r\n\r\nThis PR also simplifies some of the error raising logic in `_try_tr_conv`.'\nPR has comments:\n'@marcvanzee Is this change still applicable?' by a COLLABORATOR of type User on 2021-06-16T11:18:29Z\n\nPR has review comments:\n'I would align this with _xla_path_disabled_error, so that all primitives that cannot be converted with enable_xla=False use the same message pattern.\r\n\r\nIn #6722 I was going to change the error message globally, and add the ability to add additional messages. That PR got rolled back, because of some issues to XlaPad. I expect to be able to re-land it in a couple of day. ' by a COLLABORATOR of type User on 2021-05-17T14:38:17Z\n'Alright, perhaps it then makes sense to wait for your PR to land before adding this one?' by a COLLABORATOR of type User on 2021-05-17T14:42:08Z\n'#6755 got merged. Inspired by your change I have added to it the surfacing of the precise reason why convolution cannot be converted. PTAL.\r\n\r\n(I hope this won't get rolled back again)' by a COLLABORATOR of type User on 2021-05-19T08:50:56Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 3,
        "reason_for_closure": "Inconsistent behavior :- PR aimed for NumPy consistency but JAX behavior is deemed valid."
    },
    {
        "summary": "Pull Request '6746' titled 'Fixed Tuple_replace bug: index not incremented to replace' was authored by a User, who is associated as a NONE. \nIt was created at 2021-05-13T11:25:47Z, and was closed at 2021-05-19T19:31:12Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'closes #6745 \r\n\r\n### Description:\r\n\r\nIn the file:  `jax/_src./util.py`\r\n\r\nThe `tuple_replace` function is identical to the `tuple_insert` function already defined in the same file location (named above). In order to replace the tuple as the function intends the below code should be implemented. \r\n\r\n### Before\r\n\r\n``` py\r\ndef tuple_replace(t, idx, val):\r\n  assert 0 <= idx < len(t), (idx, len(t))\r\n  return t[:idx] + (val,) + t[idx:]\r\n```\r\n### After\r\n\r\n```py\r\ndef tuple_replace(t, idx, val):\r\n  assert 0 <= idx < len(t), (idx, len(t))\r\n  return t[:idx] + (val,) + t[idx+1:]\r\n```\r\n```diff\r\n- return t[:idx] + (val,) + t[idx:]\r\n+ return t[:idx] + (val,) + t[idx+1:]\r\n```'\nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F6746) for more info**.\n\n<!-- need_sender_cla -->' by a NONE of type Bot on 2021-05-13T11:25:53Z\n'@googlebot I signed it!' by a NONE of type User on 2021-05-13T11:28:03Z\n'Closing in favor of #6793 which deletes this function instead.\r\n\r\nThanks for finding the bug!' by a COLLABORATOR of type User on 2021-05-19T19:31:12Z\n\nPR has review comments:\n'In general, you should follow the style of the surrounding code. Put spaces around binary operators like `+`.' by a COLLABORATOR of type User on 2021-05-13T13:25:09Z\n'Thanks for the feedback! \r\n\r\nBest wishes\r\nJosh' by a NONE of type User on 2021-05-13T13:49:21Z\n'Hi Josh, would you like to make the change and close this PR? thanks!' by a COLLABORATOR of type User on 2021-05-18T17:36:48Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 3,
        "reason_for_closure": "Closed as wontfix: jax functionality generally supersedes numpy; complex inputs are valid."
    },
    {
        "summary": "Pull Request '6717' titled '[jax2tf] Change the conversion of dot_general to use XLA op.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-05-11T07:42:13Z, and was closed at 2021-05-12T12:37:52Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'Instead of converting the dot_general to a sea of TF ops, when\r\nwe enable_xla we just use the XLA op. This has the advantage\r\nthat it also supports the preferred_element_type.\r\n\r\nAlso, it turns out that `tf.linalg.matmul` now supports tf.bfloat16 and tf.int32, so change the \r\nconversion for `not _enable_xla` to use `tf.linag.matmul` for those cases as well.'\nPR has comments:\n'Hold off reviewing, I found more problems.' by a COLLABORATOR of type User on 2021-05-11T13:00:41Z\n'This is ready now. I will likely try to merge very soon, because it is blocking other changes. Please do send comments and I will incorporate them in a future CL.' by a COLLABORATOR of type User on 2021-05-12T06:21:18Z\n'> I feel somewhat less qualified to review this PR. I'm preemptively approving, but if you would like a more substantive review, I recommend either a video call, or someone else if you're more pressed for time.\r\n\r\nIndeed, there are many details here. I sent this to you mostly as education. I do think it is worth trying to digest it, but indeed it may be easier over a video call. ' by a COLLABORATOR of type User on 2021-05-12T06:39:14Z\n\nPR has review comments:\n'For my own edification, could you explain why this is the right approach here?' by a CONTRIBUTOR of type User on 2021-05-12T06:11:44Z\n'Doc string? (e.g. to explain the meaning of the tuple values)' by a CONTRIBUTOR of type User on 2021-05-12T06:27:04Z\n'The third argument is surprising to me. ' by a CONTRIBUTOR of type User on 2021-05-12T06:28:09Z\n'The `*`? It says that the rest of the arguments must be passed as kwargs, not positional.' by a COLLABORATOR of type User on 2021-05-12T06:36:13Z\n'We run `mypy` in OSS, and it complains if it cannot find one of the modules. `grpc` is not a standard requirement for JAX, and we don't want it to be. You need it only to run this serving example. For myself, when `mypy` complains, if I do not see the type error, I just `type: ignore`. My thinking is that we want the type annotations for documentation first, and the actual type checking is best effort.' by a COLLABORATOR of type User on 2021-05-12T06:38:21Z\n'I added a docstring. I wish we did have more docstrings. For now, what we are trying to do is to have docstrings for all API functions, then ideally for all module exports, and probably not add them for module locals. I think that we should\r\nadd type annotations for all functions though.' by a COLLABORATOR of type User on 2021-05-12T06:44:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 6,
        "reason_for_closure": "Inconclusive :- PR closed after discussion on complex inputs' support, deemed unnecessary as JAX supersedes NumPy."
    },
    {
        "summary": "Pull Request '6692' titled '[sparse]: add multi-buffer implementation' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-05-07T20:56:40Z, and was closed at 2021-08-25T18:31:31Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of '#6466 added some higher-level sparse objects (`COO`, `CSR`, `CSC`) using a PyTree representation.\r\n\r\nThis PR adds an alternative representation, based on multi-buffer JAX objects. It requires a bit more boilerplate for the type registration, but this approach has some advantages; in particular we can use a single set of sparse primitives for all three sparse representations.\r\n\r\nThe goal in adding this is to make it easier to experiment with implementations of JVP & transpose rules (being explored for the PyTree implementation in #6627)'\nPR has comments:\n'I think the BCOO work has made clear that the multibuffer approach is not necessary. Closing.' by a COLLABORATOR of type User on 2021-08-25T18:31:31Z\n\nPR has review comments:\n'Small suggestion:\r\n\r\n```suggestion\r\n## Higher-level objects\r\n\r\nFor higher-level sparse objects, two different approaches are demonstrated:\r\none is a PyTree-based approach, with a specific class for COO, CSR, and CSC\r\nmatrix representations. These objects are meant to be used directly, similar\r\nto the equivalent `scipy.sparse` objects.\r\n```' by a CONTRIBUTOR of type User on 2021-05-11T17:01:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Superseded by alternate approach :- The multi-buffer approach was deemed unnecessary due to the BCOO work."
    },
    {
        "summary": "Pull Request '6604' titled 'Add `jnp.real_if_close`' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-04-30T19:08:58Z, and was closed at 2021-04-30T23:15:39Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This PR adds `jnp.real_if_close` to match [`np.real_if_close`](https://numpy.org/doc/stable/reference/generated/numpy.real_if_close).'\nPR has comments:\n'Thanks – I think the reason this has not been implemented yet is that this function's semantics are fundamentally incompatible with JAX transforms, because the dtype of the return value depends on the values in the array.\r\n\r\nI think we should avoid adding implementations of functions to `jax.numpy` that cannot be used in JIT or grad or vmap... I think the only option then would be to unconditionally return complex values, but zero-out the imaginary part if it's small. That departs from numpy's behavior (and arguably from the intent of the function), but it would then be compatible with JAX transforms. What do you think?' by a COLLABORATOR of type User on 2021-04-30T19:50:09Z\n'That is a fair point. I don't see any way of easily solving this since XLA requires matching dtypes in control flow.\r\n\r\n> I think the only option then would be to unconditionally return complex values, but zero-out the imaginary part if it's small. That departs from numpy's behavior (and arguably from the intent of the function), but it would then be compatible with JAX transforms. What do you think?\r\n\r\nI think this would be a sensible behaviour. But ultimately I agree that the function with this behaviour would not be very useful in practice.\r\n\r\nSo I think the best way would probably to throw in cases where the functions is jited, or to just keep it unimplemented. Thinking about this again, I'd personally be in favour of closing this PR unless someone has a strong use case for this function.' by a CONTRIBUTOR of type User on 2021-04-30T23:15:39Z\n'Makes sense - thanks for looking into this!' by a COLLABORATOR of type User on 2021-04-30T23:20:40Z\n\nPR has review comments:\n'An alternative would be to just throw if `a` is not an array, however I think casting into arrays matches numpy more closely, let me know what behaviour would be preferred here to be consistent with other JAX APIs.\r\n```suggestion\r\n  _check_arraylike(\"real_if_close\", a)\r\n```' by a CONTRIBUTOR of type User on 2021-04-30T19:11:03Z\n'Would `a.real` be preferred over `lax.real(a)` here?' by a CONTRIBUTOR of type User on 2021-04-30T19:11:52Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 2,
        "reason_for_closure": "Inconsistent behavior: PR makes JAX and NumPy incompatible, unnecessarily restricting JAX."
    },
    {
        "summary": "Pull Request '6545' titled 'Support `dtype` kwarg in `jnp.matmul`' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-04-22T21:35:07Z, and was closed at 2023-11-03T22:07:15Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This PR adds support for the `dtype` keyword argument in `jnp.matmul` to more closely match [`np.matmul`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy.matmul) which accepts ufunc arguments since version 1.16.\r\n\r\nI didn't add a new test for this PR since these changes only pass through the `dtype` parameter to `dot` which is tests this behaviour [here](https://github.com/google/jax/blob/594309c12b39ccc79df142b85af40c53fa780468/tests/lax_test.py#L971-L1002), but let me know if you would prefer to do the same for `jnp.matmul`.'\nPR has comments:\n'There seems to be one difference here that I can see: in Numpy, this works:\r\n```python\r\nx = np.ones((4, 4)).astype('float64')\r\nnp.matmul(x, x, dtype='float32')\r\n```\r\nThe equivalent operation in JAX fails with the error\r\n```\r\nTypeError: `preferred_element_type` must not be narrower than the original type.\r\n```\r\nThere's a similar issue when changing the kind of type via `dtype`:\r\n```python\r\nx = np.ones((4, 4)).astype('int64') \r\nnp.matmul(x, x, dtype='float64')  # <-- no error\r\n\r\nx = jnp.ones((4, 4)).astype('int64') \r\njnp.matmul(x, x, dtype='float64') \r\n# TypeError: `preferred_element_type` and the original type must both be integral, both be floating point, or both complex.\r\n```\r\nI think this keyword needs a bit more handling to match numpy's semantics.' by a COLLABORATOR of type User on 2021-04-22T21:51:48Z\n'> I think this keyword needs a bit more handling to match numpy's semantics.\r\n\r\n@jakevdp Good point, sorry for missing that.\r\n\r\nI updated the PR to handle numpy semantics.\r\nThis now adds quite a bit of additional code to handle type casting.\r\nSo I think in a broader context this PR only makes sense if you plan to add support for the `dtype` keyword argument to other `ufuncs` as well in the future (see e.g #1364). I'll let you decide if support for this argument is worth the added maintenance burden or whether this is something that will be deliberately not supported by JAX in which case I am happy to close this PR.' by a CONTRIBUTOR of type User on 2021-04-23T20:01:23Z\n'Sorry, I looked a bit closer at this and I think it needs a bit more thought. In particular, the handling of the `casting` argument seems different than numpy: here it is only referenced if `dtype is None`. I think (though I may be wrong?) that in numpy the `casting` argument applies to casting of the inputs as well.\r\n\r\nBack to your previous comment, we may want to step back and decide whether it is a goal to handle optional ufunc keywords consistently across jax; I'm not certain what the answer is.' by a COLLABORATOR of type User on 2021-05-03T19:01:11Z\n'>  I think (though I may be wrong?) that in numpy the casting argument applies to casting of the inputs as well.\r\n\r\nGood catch. Yes, I think you are right. I would need to look into it a bit more closely, but I think numpy uses something like the following logic:\r\n```python\r\nresult_dtype = dtype or np.result_type(a, b)\r\nif not (np.can_cast(a.dtype, result_dtype) and np.can_cast(b.dtype, result_dtype))\r\n    raise\r\n```\r\n\r\n> Back to your previous comment, we may want to step back and decide whether it is a goal to handle optional ufunc keywords consistently across jax; I'm not certain what the answer is.\r\n\r\nI agree, this is something that should be handled for all ufuncs consistently. I think a first step would be to decide whether it is within the scope of JAX to support the `casting` argument in all ufuncs. Once JAX follows the same ufunc casting rules as numpy, supporting user definable result dtypes should be pretty easy. But then again the question whether the added maintenance burden for supporting all ufunc arguments is a worthwhile investment is still not obvious.' by a CONTRIBUTOR of type User on 2021-05-03T20:07:38Z\n'Sorry to have let this go stale. In the meantime, `jax.numpy.matmul` has added a `preferred_element_type` argument, which gets passed directly to `lax.dot_general`.\r\n\r\nI'm going to close this for now; feel free to re-open if you'd like to work on adding the `dtype` argument here. Thanks!' by a COLLABORATOR of type User on 2023-11-03T22:07:15Z\n\nPR has review comments:\n'You can replace these two lines with `promoted_dtype = result_type(a, b)`' by a COLLABORATOR of type User on 2021-05-03T16:36:19Z\n'Great! That's much more readable.' by a CONTRIBUTOR of type User on 2021-05-03T17:39:50Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 2,
        "reason_for_closure": "Inconsistent behavior with NumPy; JAX implementation preferred."
    },
    {
        "summary": "Pull Request '6541' titled 'Update linalg.py' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-04-22T17:16:33Z, and was closed at 2021-05-25T13:06:39Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Adding special casing for 3x3 matrices, which speeds up det calculation by ~8x'\nPR has comments:\n'A version of this PR was already merged as https://github.com/google/jax/pull/6543' by a COLLABORATOR of type User on 2021-05-25T13:06:39Z\n\nPR has review comments:\n'I think you can just stick a `@jit` around `det`, no need for another `_det` function.\r\n\r\n(Sometimes we use that pattern when we want to handle static arguments specially, but `det` doesn't have any.)' by a COLLABORATOR of type User on 2021-04-22T17:18:57Z\n'Is it worth adding a 2x2 case also?' by a COLLABORATOR of type User on 2021-04-22T17:19:16Z\n'ooh good call!' by a CONTRIBUTOR of type User on 2021-04-22T17:20:11Z\n'kk--was about to ask about this actually :D' by a CONTRIBUTOR of type User on 2021-04-22T17:20:26Z\n'BTW the jit should probably be innermost (i.e., closest to the `def`)' by a COLLABORATOR of type User on 2021-04-22T17:22:30Z\n'Remove extra space after `(`' by a COLLABORATOR of type User on 2021-04-22T17:24:56Z\n'++' by a CONTRIBUTOR of type User on 2021-04-22T17:25:00Z\n'done, I think!' by a CONTRIBUTOR of type User on 2021-04-22T17:25:44Z\n'also done!' by a CONTRIBUTOR of type User on 2021-04-22T17:25:49Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 9,
        "reason_for_closure": "Superseded :- PR attempts to fix a discrepancy that isn't necessary."
    },
    {
        "summary": "Pull Request '6503' titled 'Raise error in case of xla_computation(pmap) foot gun.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-04-20T09:38:34Z, and was closed at 2021-04-20T16:06:50Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'Thanks for the feedback!' by a COLLABORATOR of type User on 2021-04-20T16:06:58Z\n\nPR has review comments:\n'I'm not sure how I feel about making this an error instead of a warning 😕 @mattjj any preferences?\r\n\r\nOne other minor issue here is that usually `xla_computation` accepts anything that has `.shape` and `.dtype` as an arg, but it doesn't require it to define `__getitem__`, so the recommendation might not always work.' by a COLLABORATOR of type User on 2021-04-20T10:05:12Z\n'I think that this is not a pmap-only issue. sharded_jit, xmap and pjit will end up doing the same thing (though e.g. xmap might just crash, because I don't think it works inside a jit).' by a COLLABORATOR of type User on 2021-04-20T10:06:03Z\n'As discussed on chat, perhaps a better solution is that these compiling functions (jit, pmap, sharded_jit, pjit) should return Callable objects which have an `xla_computation(..)` endpoint accepting the same args as the wrapped function but returning an XlaComputation instance equivalent to what they would use when called regularly. I think then they are able to do their own argument handling and the user facing API is trivial:\r\n\r\n    f = pmap(f, axis)  # could swap out for jit/sharded_jit/xmap etc\r\n    y = f(x)\r\n    xla_comp = f.xla_computation(x)\r\n\r\n@mattjj wdyt?' by a COLLABORATOR of type User on 2021-04-20T10:11:39Z\n'I would definitely prefer that and I think that some folks have actually requested that in the past. Then we could even start deprecating `jax.xla_computation`' by a COLLABORATOR of type User on 2021-04-20T12:10:00Z\n'I am also unsure, and could easily be convinced otherwise, but I lean towards warning instead of error because\r\n1. people will see the warning and so be alerted to the possibly undesirable behavior and be able to rewrite the code to get what they want, disarming the footgun;\r\n2. some folks might _want_ to do xla_computation-of-pmap, perhaps in some nested pmap case (there could even be checked-in tests that already do this...).\r\n\r\nI'm not sure if that first premise is true. Maybe some peoples' eyeballs automatically filter out warnings?\r\n\r\nDo you folks buy those premises, and further that the premises suggest we should warn instead of error? Are there other premises worth considering?' by a COLLABORATOR of type User on 2021-04-20T15:25:50Z\n'That sounds like a good idea, though to find out for sure we'd need to work out details. For the short term it might make sense to still land some kind of error check like in this PR (assuming @tomhennigan still wants to pursue it rather than pursuing this new idea).\r\n\r\nI agree tagging with `_is_pmap` is likely to be pretty limited/brittle, and not just for the reason cited: it would miss things like `xla_computation(lambda x: pmap(f)(x))`. I think a better implementation mechanism would be just to check the jaxpr that gets produced. After all, it's readily available in this function (i.e. in `computation_maker`). One option is we could check whether it's a single-equation jaxpr, and whether that eqn is the application of a parallel primitive, but maybe there are better versions of this.' by a COLLABORATOR of type User on 2021-04-20T15:33:42Z\n'I'm in favour of implementing the `pmap(f).xla_computation(..)` solution, it feels much better to me to have a solution that doesn't allow you to express the footgun than either warning or raising.\r\n\r\nAs for whether we need to first make progress on this PR, I think we can drop it. I think xla_computation is obscure enough that the urgency of disarming the footgun is quite low.' by a COLLABORATOR of type User on 2021-04-20T16:06:47Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 7,
        "reason_for_closure": "Inconsistent :- PR's implementation to raise error was deemed unnecessary and replaced with a better alternative."
    },
    {
        "summary": "Pull Request '6314' titled 'Lower lu_pivots_to_permutation using a custom CUDA kernel.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-04-01T07:53:04Z, and was closed at 2021-04-03T21:44:33Z by a User.\nThe PR has labels: cla: no - . \nIt has a body of 'Fixes google/jax#5880'\nPR has comments:\n'I should add: this is awesome! Thanks for the PR!' by a COLLABORATOR of type User on 2021-04-01T13:57:27Z\n'All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F6314) for more info**.\n\n<!-- need_author_consent -->' by a NONE of type Bot on 2021-04-03T21:40:06Z\n'All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F6314) for more info**.\n\n<!-- need_author_consent -->' by a NONE of type Bot on 2021-04-03T21:42:30Z\n\nPR has review comments:\n'You don't need this backward compatibility code. This function is always the identity in sufficiently new `jax` releases.' by a COLLABORATOR of type User on 2021-04-01T13:41:58Z\n'I'd be tempted to skip out of bounds pivots on the high end also. This should never happen, but it's probably better to avoid corrupting GPU memory in that case.' by a COLLABORATOR of type User on 2021-04-01T13:48:25Z\n'This is fine, but I think we can do even better. I suggest we make `lu_pivots_to_permutation` into a primitive, which allows it in turn to have per-backend lowering rules. You can then always use the `lower_fun` implementation in this translation rule.\r\n\r\nThe reason I suggest this is that some folks are calling `lu_pivots_to_permutation` explicitly:\r\nhttps://cs.opensource.google/search?q=lu_pivots_to_permutation&sq=\r\n\r\nand right now they would still get the slow implementation on GPU. Indeed the `scipy` API almost forces that upon us.\r\n\r\nHappily we would not need, say, autodiff rules for such a primitive, just abstract eval, shape, and batching rules.' by a COLLABORATOR of type User on 2021-04-01T13:53:07Z\n'This is fine, but I have to wonder if this is the last hand-written linear algebra CUDA kernel we will have to add... I might have been tempted to name the module something like `cuda_linalg.py`, but we can always do that if and when we have a second kernel.' by a COLLABORATOR of type User on 2021-04-01T13:56:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 4,
        "reason_for_closure": "Inconsistent behavior change; JAX features should supersede NumPy, not restrict them.  Closing as it breaks existing functionality."
    },
    {
        "summary": "Pull Request '6306' titled 'WIP: scatter via scatter_add' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-03-31T19:50:01Z, and was closed at 2021-05-12T18:35:51Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'xref https://github.com/google/jax/issues/6288'\nPR has comments:\n'Closing this PR, since, as Stephan suggests, it would break some of the semantics of `scatter`.' by a COLLABORATOR of type User on 2021-05-12T18:35:51Z\n\nPR has review comments:\n'We probably shouldn't merge this change, because it breaks this guarantee.' by a COLLABORATOR of type User on 2021-04-02T03:22:17Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Breaks semantics: PR closed as it disrupted expected behavior of the function."
    },
    {
        "summary": "Pull Request '6132' titled 'use c++ jit dispatch path for op-by-op execution' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-03-19T04:41:50Z, and was closed at 2021-04-19T21:23:08Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'Historically we had two distinct computation-building and dispatch paths for op-by-op execution (eager mode) and `jit`-decorated functions. Even though the two paths are fundamentally pretty similar (compile an XLA executable or get it from a cache, then execute it on buffers), we specialized the op-by-op path so that it would have lower overheads.\r\n\r\nThen much of our `jit` dispatch path got faster (thanks mostly to C++ work by @jblespiau, @hawkinsp, and @zhangqiaorjc). In fact, it got way faster than the op-by-op dispatch path! So we can just unify things under the `jit` compilation and dispatch path. That means simpler code (e.g. only one lowering path, fewer compilation caches (pmap still has its own)), and an op-by-op speed boost too.\r\n\r\nBecause this change is only to the impl rules of primitives, it's even simpler than the `jit` dispatch path: where the `jit` dispatch path has to worry about Tracer inputs and outputs (and currently bails out to a Python path if Tracers are present), no Tracers on input or output are possible here.\r\n\r\nThis change mostly consists of\r\n* deleting the separate op-by-op compilation and dispatch paths in xla.py\r\n* replacing them with a C++-API-based dispatch path in xla.py, which looks like a specialized version of some of the `jit` dispatch code in api.py.\r\n\r\nThe replacement code is not super short, just because setting up a C++ dispatch path is nontrivial. (We could _almost_ achieve what we want in a two-liner with monkey patching...)\r\n\r\n~**This change only works with omnistaging.** I marked the change as review-ready because I think we can review it first, even though it's blocked on us deleting the non-omnistaging code paths.~ That's done!\r\n\r\nTODO:\r\n- [x] write PR message\r\n- [x] ask hawkinsp@ about debug_nans stuff\r\n- [x] land omnistaging-only\r\n- [ ] ~in a separate PR, add some eager grad benchmarks, compare~\r\n\r\nCPU eager benchmarks (just added in #6133):\r\n```\r\nname                   old time/op             new time/op             delta\r\neager_unary_dispatch   31.6µs ± 2%             21.6µs ± 2%  -31.67%          (p=0.008 n=5+5)\r\neager_unary            32.3µs ± 1%             22.1µs ± 1%  -31.45%          (p=0.008 n=5+5)\r\neager_binary_dispatch  39.8µs ± 1%             22.5µs ± 2%  -43.40%          (p=0.008 n=5+5)\r\neager_binary           40.3µs ± 1%             23.1µs ± 2%  -42.60%          (p=0.008 n=5+5)\r\n```'\nPR has comments:\n'This is proving too hard to land, and it's not a big enough step towards our intended endpoint.' by a COLLABORATOR of type User on 2021-04-19T21:23:08Z\n\nPR has review comments:\n'I promise you this line will make heads explode.' by a COLLABORATOR of type User on 2021-03-22T19:23:54Z\n'Yes. In general the C++ path can fail and fall back in some cases. We can make it more complete where we need.' by a COLLABORATOR of type User on 2021-03-22T19:25:58Z\n'Without this change, we get a failure in an abstract eval rule, saying \"add does not accept dtype bool\". (This change means we run abstract eval rules when compiling single-primitive jaxprs in cases we may not have before.)\r\n\r\nEither this test is wrong or the abstract eval rule is wrong. That is, should add accept bool inputs?' by a COLLABORATOR of type User on 2021-03-31T22:31:31Z\n'I remember going back and forth on this before. I don't recall what we landed on.\r\n\r\nGiven the abstract eval rule forbids it, it seems likely no-one is using this? I'd run global presubmits to make sure.' by a COLLABORATOR of type User on 2021-03-31T22:52:43Z\n'Must... create... functools.partial instance with self referential... func...\r\n\r\nNah, ignore that :-)' by a COLLABORATOR of type User on 2021-03-31T22:53:20Z\n'`float0` presumably also.' by a COLLABORATOR of type User on 2021-03-31T22:59:29Z\n'Can you say more? I'm not following why the test depends on the cache warmth (and why it should).' by a COLLABORATOR of type User on 2021-03-31T23:04:47Z\n'Great catch!' by a COLLABORATOR of type User on 2021-04-01T01:25:17Z\n'I'm actually not totally sure if float0s can end up here. They may always avoid impl rules entirely.' by a COLLABORATOR of type User on 2021-04-01T01:25:45Z\n'Good idea. I'll improve the comment. Here's a first draft of an explanation.\r\n\r\nAs you know, the situation is different depending on the jaxlib version.\r\n\r\nFor jaxlib==0.1.64 (current on pypi), nan checks are called for `api.jit` [from the Python wrapper code](https://github.com/google/jax/blob/640e62c7dab22b582a729de7b2eea1ce2c6b480d/jax/api.py#L385-L416), whereas for jaxlib>=0.1.65, [nan checks are called from the C++ dispatch code](https://github.com/google/jax/blob/640e62c7dab22b582a729de7b2eea1ce2c6b480d/jax/api.py#L127-L144).\r\n\r\nAn analogous situation holds with this op-by-op dispatch stuff as well: for jaxlib==0.1.64, [we need to call nan checks in the Python wrapper](https://github.com/google/jax/pull/6132/files#diff-121975a1a88d61a259f8423655e23a1b389eb18c399b267395d2aba5a196bae9R276), but after 0.1.65 those will be redundant and [we can delete them](https://github.com/google/jax/pull/6132/files#diff-121975a1a88d61a259f8423655e23a1b389eb18c399b267395d2aba5a196bae9R263).\r\n\r\nUntil we delete them, i.e. until 0.1.65, there are _two_ places which might raise a `FloatingPointException` for a nan error. Which one we hit depends on whether we've already compiled an appropriate version for the primitive that raises an error (e.g. `div` with `f32[]` arguments).\r\n\r\nThe first place is [in `_execute_compiled`](https://github.com/google/jax/blob/640e62c7dab22b582a729de7b2eea1ce2c6b480d/jax/interpreters/xla.py#L349), called inside [`_cache_miss`](https://github.com/google/jax/pull/6132/files#diff-121975a1a88d61a259f8423655e23a1b389eb18c399b267395d2aba5a196bae9R293). We'll hit that call to `check_special` if we have an empty compilation cache when we compile and run the nan-producing computation (since that first time we dispatch via `_execute_compiled`).\r\n\r\nIf the compilation cache is populated, though, we'll just jump to the computation from C++, so we'll never hit `_execute_compiled`. Instead, we have to raise the error [from the Python wrapper](https://github.com/google/jax/pull/6132/files#diff-121975a1a88d61a259f8423655e23a1b389eb18c399b267395d2aba5a196bae9R276).\r\n\r\nThis test and the new one I added just after it together exercise both paths.\r\n\r\nAt jaxlib==0.1.64, since we have the check in the Python wrapper, we don't actually need the check in `_cache_miss`, i.e. the one in `_execute_compiled`. But to avoid it we'd have to avoid or change the current `_execute_compiled` (i.e. `xla_callable`) standard code path. Moreover, at jaxlib==0.1.65, we don't want the check in the Python wrapper; we want it in the `_cache_miss` path. So I think it's easiest to keep the checks redundant in jaxlib==0.1.64, as it means simpler code now and easy-to-update code later.\r\n\r\nPhew, that was complicated. Not sure if I over-complicated it, or if there's just a better way to explain the complexity...' by a COLLABORATOR of type User on 2021-04-01T01:28:38Z\n'Actually I noticed this pattern in the standard library, namely [in `functools._unwrap_partial`](https://github.com/python/cpython/blob/ad493edf5791e7abb2588852e876b8584945c653/Lib/functools.py#L421-L424)! I was looking at that for cl/365189779.\r\n\r\nIf you make a functools.partial with circular refs, I'm going to call your pytree code on a circular list!' by a COLLABORATOR of type User on 2021-04-01T01:32:12Z\n'Just trollin ya, man.' by a COLLABORATOR of type User on 2021-04-01T01:32:24Z\n'I added a comment about it:\r\n\r\n```\r\n# This is a mechanism to pass `prim` into `_apply_primitive` in a way that works\r\n# with the `_xla_callable` cache (relying on `lu.WrappedFun.__hash__`).\r\n```' by a COLLABORATOR of type User on 2021-04-01T02:51:09Z\n'I added this comment:\r\n\r\n```\r\n# See the subsequent test as well. This test covers the case where we get a\r\n# Python compilation cache hit, since the next line ensures an appropriate\r\n# version of the primitive impl rule is compiled. After jaxlib==0.1.65 both\r\n# tests likely hit the same code paths, but at jaxlib==0.1.64 they cover\r\n# different cases (namely whether we hit the nan check in\r\n# `xla._execute_compiled` or the one in `xla.apply_primitive`).\r\n```' by a COLLABORATOR of type User on 2021-04-01T03:03:05Z\n'Actually, [I've written this utility before!](https://github.com/google/jax/blob/d148a57bfae81d46724efc533a4b6515a43ca467/jax/linear_util.py#L287) I'll just call it.' by a COLLABORATOR of type User on 2021-04-01T03:06:07Z\n'I changed it to say \"tokens, etc\"' by a COLLABORATOR of type User on 2021-04-01T03:07:51Z\n'We just made jax==0.1.65 the minimum version, so I can clean this up and these long comments will no longer be necessary!' by a COLLABORATOR of type User on 2021-04-09T19:15:45Z\n'I believe an internal CI test failed on this, so I'm going to update the abstract eval rule.' by a COLLABORATOR of type User on 2021-04-09T19:19:56Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 18,
        "reason_for_closure": "Inconsistent :- PR tried to enforce NumPy behavior where JAX offers broader support."
    },
    {
        "summary": "Pull Request '6000' titled 'dtypes.result_type: ignore weak_type for single argument' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-03-10T00:33:17Z, and was closed at 2021-03-15T22:25:27Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Within the promotion table, all weak dtypes are canonicalized to `jnp.dtype(int)`, `jnp.dtype(float)`, or `jnp.dtype(complex)`. This means, e.g., that if you have a weakly-typed int32 in X64 mode, you get some strange results:\r\n```python\r\nfrom jax import config, dtypes, lax                                                                                                                                     \r\nconfig.update('jax_enable_x64', True)                                                                                                                                   \r\n\r\nx = lax.convert_element_type(2, 'int32', weak_type=True)                                                                                                    \r\nprint(dtypes.result_type(x))\r\n# int64\r\n```\r\nThis is particularly surprising becuase `dtypes.result_type` is used throughout the codebase to find the dtype of single arguments.\r\n\r\nThis change special-cases the single-argument version of `dtypes.result_type` to return the dtype of the value.'\nPR has comments:\n'#6068 has the better fix.' by a COLLABORATOR of type User on 2021-03-15T22:25:27Z\n\nPR has review comments:\n'For both this function and the one above, maybe it'd be better to encode this directly, by making this function return a `(dtype, bool)` pair? That seems like it would reduce risk of surprising the caller, as the caller would have to explicitly handle the weak type tag. WDYT?' by a COLLABORATOR of type User on 2021-03-10T20:30:23Z\n'That's probably a good idea, given how this surprised us!\r\n\r\nThat said, these return values are indices to the type promotion lattice, so the weak versions of various ints will all have to be normalized to a single identifier somewhere before they're used in the lattice code, and it's not clear where that would be done if not here.' by a COLLABORATOR of type User on 2021-03-10T21:33:49Z\n'How hard would it be to make the lattice operate directly on `(dtype, bool)` pairs?' by a COLLABORATOR of type User on 2021-03-10T21:35:04Z\n'We couldn't use multiple aliases of each weak type in the lattice, because they are unorderable.' by a COLLABORATOR of type User on 2021-03-10T21:36:45Z\n'We could normalize all `(np.[u]int*, True)` entries to a single weak int indicator within the lattice, but that's essentially what this function does currently.' by a COLLABORATOR of type User on 2021-03-10T21:37:59Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 5,
        "reason_for_closure": "Superseded :- PR closed in favor of a different solution."
    },
    {
        "summary": "Pull Request '5957' titled 'Initial commit of sixth JAX-101 notebook' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-03-05T21:47:33Z, and was closed at 2021-03-06T00:37:18Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nPR has comments:\n'FYI I think some of my comments got messed up because you pushed changes while I made them. Lemme know if anything's incomprehensible.' by a MEMBER of type User on 2021-03-05T22:27:17Z\n\nPR has review comments:\n'I think this should be ##, it's coming out as a top-level item in the ToC' by a MEMBER of type User on 2021-03-05T22:11:58Z\n'Same here, ##' by a MEMBER of type User on 2021-03-05T22:12:07Z\n'We have authors now!' by a MEMBER of type User on 2021-03-05T22:12:24Z\n'Let's not mention the part about extending to 16 devices, you can only do that internally. I would just say \"(usually 8)\".' by a MEMBER of type User on 2021-03-05T22:24:42Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Inconsistent behavior change, breaks existing users, deprecated desired functionality."
    },
    {
        "summary": "Pull Request '5942' titled 'add experimental nans plumber' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-03-04T21:57:18Z, and was closed at 2021-12-14T22:13:52Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of '![image](https://user-images.githubusercontent.com/1458824/110185782-27d2f500-7dc8-11eb-9459-569ae3f6c906.png)\r\n'\nPR has comments:\n'We're landing a generalized version of this as the `jax.experimental.checkfiy` transform.' by a COLLABORATOR of type User on 2021-12-14T22:13:52Z\n\nPR has review comments:\n'\r\n```suggestion\r\n      main.idx = lax.select(jnp.any(x != x), idx, main.idx)\r\n```' by a CONTRIBUTOR of type User on 2021-03-04T23:51:00Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Generalized NaN check functionality landed in `jax.experimental.checkify`."
    },
    {
        "summary": "Pull Request '5712' titled 'Replace AvalDimSharding, MeshDimAssignment and ShardedDeviceArray with the C++ object.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-02-11T23:44:02Z, and was closed at 2021-03-03T08:31:17Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'The sharding and mesh_mapping fields are lists (instead of the current tuple), but they are immutable (I mean that if you modify it, it won't be reflected on the C++ object. But because it's a list you can decide to modify it).\r\n\r\nIt's internal to JAX, so it limits the issues with the possible confusion of having lists and not tuples, but it massively simplify the C++ as the casts are trivial.\r\n\r\nThis follows https://github.com/google/jax/pull/5634 and should be submitted before after.'\nPR has comments:\n'NOTE:\r\n\r\nI still need to silence mypy for some lines:\r\n```\r\njax/interpreters/pxla.py:1141: error: Argument \"mesh_mapping\" to \"ShardingSpec\" has incompatible type \"List[Replicated]\"; expected \"List[Union[ShardedAxis, Replicated]]\"  [arg-type]\r\njax/interpreters/pxla.py:1141: note: \"List\" is invariant -- see http://mypy.readthedocs.io/en/latest/common_issues.html#variance\r\njax/interpreters/pxla.py:1141: note: Consider using \"Sequence\" instead, which is covariant\r\n```\r\n\r\nI do not want to use `Sequence`, as this would be incorrect, the type *must* be a List. It just does not know Union merging.' by a CONTRIBUTOR of type User on 2021-02-12T15:28:35Z\n'> LGTM once the comments are addressed. Why do you have to be so careful about passing in lists? I thought we're using pybind to do all the conversions, and it should successfully convert tuples to `std::vector`s\r\n\r\nPybind11 converts py::list (i.e. Python lists) into std::vector, and the other way around.\r\n\r\nTuples (py::tuple) are not convertible to std::vector (I imagine they are to std::tuple).\r\n\r\nSo if you pass a tuple to a function that expects a std::vector in C++, it will fail.' by a CONTRIBUTOR of type User on 2021-02-15T22:57:06Z\n'Updated the CL.\r\n\r\nRemoved formatting and set the attribute directly on `ShardingSpec`.\r\n\r\nFor the typing errors, the tuple <-> st::vector conversion, tuple_insert, I explained why I opted for this option (typing errors do occur with mypy, std::vectors are not convertible to tuples, tuple_insert does not work on lists).' by a CONTRIBUTOR of type User on 2021-02-15T23:13:41Z\n'Closing, I will submit this internally to prevent breakages.' by a CONTRIBUTOR of type User on 2021-02-16T08:54:57Z\n'It's being reviewed internally as cr/352187814.' by a CONTRIBUTOR of type User on 2021-03-03T08:31:17Z\n\nPR has review comments:\n'This is unnecessary. GitHub (or really git) has an option to ignore whitespace changes which makes reviews like this one significantly easier. Can you please remove this flag?' by a COLLABORATOR of type User on 2021-02-15T18:52:13Z\n'Why wouldn't `pmap_lib.ShardingSpec.mesh_shape = property(mesh_shape)` work?' by a COLLABORATOR of type User on 2021-02-15T18:53:50Z\n'Please avoid unnecessary formatting changes. No need to revert, but stuff like this creates more noise (unlike the indentation changes). This applies to the whole file...' by a COLLABORATOR of type User on 2021-02-15T18:54:31Z\n'Why `type: ignore`?' by a COLLABORATOR of type User on 2021-02-15T18:56:05Z\n'Why `type: ignore`?' by a COLLABORATOR of type User on 2021-02-15T18:56:15Z\n'FWIW `tuple_insert` should work on lists too, so maybe use that (and convert it to list afterwards to satisfy the type checker)' by a COLLABORATOR of type User on 2021-02-15T18:57:04Z\n'How is this related to this change? I thought it was only supposed to be about sharding specs, not about SDA' by a COLLABORATOR of type User on 2021-02-15T18:58:21Z\n'Also, `super().__init__`?' by a COLLABORATOR of type User on 2021-02-15T19:05:23Z\n'If you mean this constant, I removed it.' by a CONTRIBUTOR of type User on 2021-02-15T22:55:40Z\n'See the comment at the top of the PR:\r\n\r\n```\r\nNOTE:\r\n\r\nI still need to silence mypy for some lines:\r\n\r\njax/interpreters/pxla.py:1141: error: Argument \"mesh_mapping\" to \"ShardingSpec\" has incompatible type \"List[Replicated]\"; expected \"List[Union[ShardedAxis, Replicated]]\"  [arg-type]\r\njax/interpreters/pxla.py:1141: note: \"List\" is invariant -- see http://mypy.readthedocs.io/en/latest/common_issues.html#variance\r\njax/interpreters/pxla.py:1141: note: Consider using \"Sequence\" instead, which is covariant\r\n```' by a CONTRIBUTOR of type User on 2021-02-15T23:01:22Z\n'Same reason as above. mypy does not understand that a List[Replicated] is valid for a List[Union[Replicated, ShardedAxis].' by a CONTRIBUTOR of type User on 2021-02-15T23:02:32Z\n'tuple_insert does not work on lists, because it concanates a 1-element tuple:\r\n\r\ndef tuple_insert(t, idx, val):\r\n  assert 0 <= idx <= len(t), (idx, len(t))\r\n  return t[:idx] + (val,) + t[idx:] <<< This is invalid to concatenate a list and a tuple.' by a CONTRIBUTOR of type User on 2021-02-15T23:03:47Z\n'I renamed the CL to say it is also changing ShardedDEviceArray, which now extends a C++ type.\r\n\r\nIt's done by:\r\n- extending the C++ object.\r\n- it will serve aval, sharding_spec and device_buffers, which are removed from __slots__\r\n- calling this contructor.\r\n' by a CONTRIBUTOR of type User on 2021-02-15T23:04:43Z\n'(In practice, as you will see, it's doing a very poor job at comparing files, and everything appears to be different^^' by a CONTRIBUTOR of type User on 2021-02-15T23:08:29Z\n'Done.' by a CONTRIBUTOR of type User on 2021-02-15T23:10:54Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 15,
        "reason_for_closure": "Closed as not useful: The PR was deemed unnecessary because the underlying C++ changes were handled internally."
    },
    {
        "summary": "Pull Request '5674' titled 'Adds missing reduction in xmap() pydoc example.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-02-08T14:11:17Z, and was closed at 2021-04-12T12:49:50Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nPR has comments:\n'This was submitted as #5688 - discarding this PR.' by a CONTRIBUTOR of type User on 2021-04-12T12:49:50Z\n\nPR has review comments:\n'```suggestion\r\n      y_pred = jnp.einsum('{in_features},{in_features}->{}', w, x) + b\r\n```\r\n\r\nThis is a pre-existing bug but I just noticed it.\r\n\r\n~We could probably do the reduce-sum as part of the einsum, like~\r\n\r\n~y_pred = jnp.einsum('{out_features,in_features},{in_features}->{}', w, x) + b~\r\n\r\n~Not sure which seems nicer. Maybe the version in this PR with the explicit `jnp.sum` reduction.~\r\n\r\nWDYT @apaszke @andsteing ?' by a COLLABORATOR of type User on 2021-02-09T03:28:13Z\n'Adding both in the `einsum` LGTM.\r\n(I ran it locally using `pdot` because `einsum` was not available yet)' by a CONTRIBUTOR of type User on 2021-02-09T07:23:43Z\n'Oh, wait - we would still have the `out_features` dimension in `y`, though, so I think it's better to keep the two separate reductions.' by a CONTRIBUTOR of type User on 2021-02-09T07:28:45Z\n'I think the `out_features` axis would be summed over. Here's a positional axis analogy:\r\n\r\n```python\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: w = np.random.randn(4, 3)\r\n\r\nIn [3]: x = np.random.randn(3)\r\n\r\nIn [4]: np.einsum('oi,i->', w, x).shape\r\nOut[4]: ()\r\n```\r\n\r\nor, with a batch axis:\r\n\r\n```python\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: w = np.random.randn(4, 3)\r\n\r\nIn [3]: x = np.random.randn(10, 3)\r\n\r\nIn [4]: np.einsum('oi,bi->b', w, x).shape\r\nOut[4]: (10,)\r\n```\r\n\r\nThat is, by analogy only the `w` input has the `out_features` axis, and we're summing it out.\r\n\r\n(Unfortunately we're still landing the change where JAX will track axis names for us, so I can't easily show this working on named axes. And our named-axis einsum implementation may have bugs too!)' by a COLLABORATOR of type User on 2021-02-09T16:19:10Z\n'But then in the next step we would have `loss = (y - y_pred)**2` and `y` has the named axis `batch` *and* `out_features` - so if I understand `xmap()` correctly this would result in `y_pred` being broadcast on `out_features` again and then an error when `jnp.sum(loss, axis=\"batch\")` is returned because the return value would still have the named axis `out_features`.\r\n\r\n(BTW : looking forward to have named axis in avals !)' by a CONTRIBUTOR of type User on 2021-02-09T17:15:02Z\n'Oh, of course you're right, sorry! Thanks for explaining. I'm doing too much context switching :)' by a COLLABORATOR of type User on 2021-02-09T17:52:33Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 6,
        "reason_for_closure": "Superseded by another PR/fix.  PR discarded in favor of a different solution. Duplicate of another PR. Obsolete, issue resolved upstream/in dependency."
    },
    {
        "summary": "Pull Request '5626' titled '[avals with names] Propagate presence of name (mapped) vs absence (replicated) in abstract eval based on existing batching rules' was authored by a Bot, who is associated as a NONE. \nIt was created at 2021-02-04T07:20:20Z, and was closed at 2021-02-25T05:08:42Z by a Bot.\nThe PR has labels: cla: yes - . \nIt has a body of '[avals with names] Propagate presence of name (mapped) vs absence (replicated) in abstract eval based on existing batching rules\nThe third change in the avals-with-names stack:\n- https://github.com/google/jax/pull/5524 Revise aval constructor call sites to use a new `aval.update` method\n- https://github.com/google/jax/pull/5541 Add `named_shape` to `ShapedArray` and update typecompat\n- **Propagate presence of name (mapped) vs absence (replicated) in abstract eval based on existing batching rules**\n- Make `mapped_aval`, `unmapped_aval`, and their xmap equivalents swap positional and named axes (rather than just creating and deleting positional ones)\n- Enable `lax.full` to create values with named axes\n- Ensure `grad` and `jacfwd`/`jacrev` consistently act elementwise over named axes (by e.g. using a seed with named axes in `grad`, and prohibiting collectives if TAP isn't too unhappy) and align `vmap(transpose)` with `transpose(vmap)` by moving the `psum` in `transpose(psum)` into `backward_pass`\n- Add `axis_name` kwarg to grad to indicate operating collectively over one or more named axes\n'\nPR has comments:\n'Still need to put name handling in the collective abstract evals that @apaszke recently added.' by a CONTRIBUTOR of type User on 2021-02-04T10:18:53Z\n'This should be ready for review now.\r\n\r\nThe reason I think it's okay to assume commutativity (and break multiply-batched while_loop) is that I think we can fix the while_loop issue before we start to rely on named avals outside xmap (and implementing the non-commutativity-assuming approach was getting pretty hairy).' by a CONTRIBUTOR of type User on 2021-02-05T19:47:25Z\n'Some context on this change, since it includes a major conceptual step in the \"avals with names\" effort:\r\n\r\nOur overall goal in this sequence of changes is to introduce \"mapped\" axes explicitly into the internal type system. We represent these with names, distinct from \"positional\" axes, for a variety of reasons that I won't review here. Why change the type system? One of several reasons is to track the common action of our parallel primitives, independent of how their operation grounds out under any specific map (`pmap`, `vmap`, `xmap`, etc.).\r\n\r\nThis change takes the first step to that end, by making abstract evaluation aware of named axes. Now we need every abstract eval rule to propagate them. We can think of this as pulling an \"abstract\" version of each primitive's batching rule into its abstract eval rule. @jekbradbury actually proposes doing this automatically: partially evaluating the batching rules to extract abstract evaluations. We're not sure that this is necessary, nor that it cover primitives like `cond` that have non-default abstract evaluation rules, so we'll need to follow up on that either in this change or soon after it.' by a MEMBER of type User on 2021-02-09T01:49:28Z\n\nPR has review comments:\n'Should we put a TODO to revise this? Note from James: collectives that can introduce an axis name into the output, could carry the size in their params.' by a COLLABORATOR of type User on 2021-02-08T23:31:24Z\n'Discussed offline: we believe that we'd rather write out these rules than automatically extract them via partial eval of batching rules. If we choose to do that outside of this PR then this can be a stopgap in the meantime. Maybe insert a TODO comment to that end.' by a MEMBER of type User on 2021-02-09T02:12:54Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 2,
        "reason_for_closure": "Implementation mismatch :- PR aimed to match numpy's behavior but JAX's superset functionality offered more, hence rejected."
    },
    {
        "summary": "Pull Request '5618' titled 'Add tests for pytree dataclass support' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-02-03T19:52:15Z, and was closed at 2023-11-03T22:03:08Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'These are the tests for https://github.com/tensorflow/tensorflow/pull/46894. I'm not sure what the best way to support conditional tests for Python 3.6 so I just took a stab at it.\r\n\r\nRelated to https://github.com/google/jax/issues/2371'\nPR has comments:\n'I'm a bit confused about this: tensorflow/tensorflow#46894 is not yet merged, but these tests are already passing. Does that indicate that these tests are not working as intended?' by a COLLABORATOR of type User on 2021-02-03T21:07:01Z\n'@jakevdp I'm not familiar with how it runs in CI, but I will take a look. Running it locally works:\r\n\r\n```console\r\n$ python tests/tree_util_tests.py 2>&1 | grep DataClass\r\n[ RUN      ] TreeTest.testAllLeavesWithTrees16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testAllLeavesWithTrees16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testAllLeavesWithTrees17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testAllLeavesWithTrees17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testCompose16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testCompose16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testCompose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testCompose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtrip16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtrip16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtrip17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtrip17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtripIsLeaf16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtripIsLeaf16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtripIsLeaf17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtripIsLeaf17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtripViaBuild16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtripViaBuild16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtripViaBuild17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtripViaBuild17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtripWithFlattenUpTo16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtripWithFlattenUpTo16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtripWithFlattenUpTo17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtripWithFlattenUpTo17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testTranspose16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testTranspose16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testTranspose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testTranspose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n```' by a CONTRIBUTOR of type User on 2021-02-03T21:13:19Z\n'Right - what I'm saying is it appears that the in-progress tensorflow change is not required in order for these new tests to pass. Is that your understanding?' by a COLLABORATOR of type User on 2021-02-03T21:15:08Z\n'@jakevdp it fails locally, though some tests pass and not sure if this expected or not. I'll take a look later this afternoon.\r\n\r\nedit: the passing tests are expected because they succeed on leaf nodes.\r\n\r\n```console\r\n$ pip install 'jaxlib==0.1.59'\r\nSuccessfully installed jaxlib-0.1.59\r\n$ python tests/tree_util_tests.py 2>&1 | grep DataClass\r\n[ RUN      ] TreeTest.testAllLeavesWithTrees16 (ADataClass(foo='hello', bar=3.5))\r\n[  FAILED  ] TreeTest.testAllLeavesWithTrees16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testAllLeavesWithTrees17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[  FAILED  ] TreeTest.testAllLeavesWithTrees17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testCompose16 (ADataClass(foo='hello', bar=3.5))\r\n[  FAILED  ] TreeTest.testCompose16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testCompose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[  FAILED  ] TreeTest.testCompose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtrip16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtrip16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtrip17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtrip17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtripIsLeaf16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtripIsLeaf16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtripIsLeaf17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtripIsLeaf17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtripViaBuild16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtripViaBuild16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtripViaBuild17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtripViaBuild17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testRoundtripWithFlattenUpTo16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testRoundtripWithFlattenUpTo16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testRoundtripWithFlattenUpTo17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testRoundtripWithFlattenUpTo17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[ RUN      ] TreeTest.testTranspose16 (ADataClass(foo='hello', bar=3.5))\r\n[       OK ] TreeTest.testTranspose16 (ADataClass(foo='hello', bar=3.5))\r\n[ RUN      ] TreeTest.testTranspose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n[       OK ] TreeTest.testTranspose17 (ADataClass(foo='hello', bar=ADataClass(foo=3.5, bar='quux')))\r\n```' by a CONTRIBUTOR of type User on 2021-02-03T21:19:10Z\n'Hi, is this close to being finished? \r\nThe ability to use @dataclass in Jax would make programming much easier :)' by a NONE of type User on 2021-03-22T21:39:15Z\n'Hi @kaiwenw - the relevant change is https://github.com/tensorflow/tensorflow/pull/46894\r\n\r\nOnce that is merged, it would be included in the subsequent jaxlib release, and could then be used in JAX. One barrier to landing this change is that for the time being, tensorflow & jaxlib must maintain Python 3.6 support, and dataclasses were not added until Python 3.7.' by a COLLABORATOR of type User on 2021-03-22T23:04:18Z\n'@jakevdp the code works on Python 3.6 with the dataclasses backport, and will fail gracefully if it's not available. the tests may need tweaking for 3.6, I would need to check again to see. but in any case the Tensorflow issue appears to have fallen into the Google blackhole- do you have any ideas how we can get this moving again?\r\n\r\nhttps://pypi.org/project/dataclasses/' by a CONTRIBUTOR of type User on 2021-03-22T23:17:46Z\n'I'm cathing up on previous discussion of this... it's not 100% clear that automatically treating dataclasses as pytrees is something JAX should do by default; see this comment on the attached issue: https://github.com/google/jax/issues/2371#issuecomment-633680461\r\n\r\nOne possible option is to use the pytree-compatible `dataclass` implementation in `chex`: https://github.com/deepmind/chex/blob/master/chex/_src/dataclass.py' by a COLLABORATOR of type User on 2021-03-23T00:00:30Z\n'I'm going to close this because it's become stale. Feel free to re-open if you'd like to discuss it further!' by a COLLABORATOR of type User on 2023-11-03T22:03:08Z\n\nPR has review comments:\n'Ths looks pretty good. I prefer to keep the `try` block as succinct as possible to prevent catching unintended errors; you could do that here by adding an `else` block:\r\n```python\r\ntry:\r\n  from dataclasses import dataclass  # pylint: disable=g-import-not-at-top\r\nexcept ModuleNotFoundError:\r\n  pass\r\nelse:\r\n  @dataclass\r\n  class ADataClass:\r\n    foo: object\r\n    bar: object\r\n\r\n  TREES += (ADataClass(foo=\"hello\", bar=3.5),),\r\n  TREES += (ADataClass(foo=\"hello\", bar=(ADataClass(foo=3.5, bar=\"quux\"))),),\r\n```' by a COLLABORATOR of type User on 2021-02-03T20:36:58Z\n'fixed, thanks' by a CONTRIBUTOR of type User on 2021-02-03T20:49:32Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 9,
        "num_review_comments": 2,
        "reason_for_closure": "Stale :- PR closed due to staleness and lack of activity."
    },
    {
        "summary": "Pull Request '5459' titled 'add systematic pdot tests, utility functions' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2021-01-18T18:03:28Z, and was closed at 2021-01-21T22:45:40Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Run lots of tests with e.g.\r\n\r\n```\r\nenv JAX_NUM_GENERATED_CASES=1000 python tests/xmap_test.py PDotTests\r\n```\r\n\r\nRight now the new pdot tests are numbered rather than named, just because the names where we just put together the strings of all the test parameters were long and unwieldy (though I checked that there are no duplicates generated!). I may try to generate a nicer string name for tests...\r\n\r\n#5491 is a copy of this PR to try to work around some weird copybara issues.'\nPR has comments:\n'This was merged as part of #5491.' by a COLLABORATOR of type User on 2021-01-21T22:45:40Z\n\nPR has review comments:\n'I am curious how many raw test cases are here? If there are many, then we pay the cost to generate them and then we throw most of them out. When testing jax2tf with this kind of combinatorial test case generation I have run several times into cases when the interesting use cases where systematically ignored.  An alternative strategy is to generate a sequence of families of test cases, each with shallow-nested for-loops.\r\n\r\nHowever, I do not understand the code being tested, and how many interesting paths through it there are. Perhaps there are really lots of paths.' by a COLLABORATOR of type User on 2021-01-18T18:59:25Z\n'That's a good point to raise! It seems the relevant figure of merit is test generation time. On my machine it takes less than 1.5 seconds to generate all the test cases here (computed by comparing pytest's reported time running tests to timing the pytest command itself from my shell).\r\n\r\nEach test case here is a valid pdot call and a valid xmap scheduling for that call, and there are no duplicates. Moreover this seems like the place where we most want exhaustive tests: any of these calls could reasonably appear in the wild, and we want xmap + pdot to be rock solid since we expect them to get a lot of use.\r\n\r\nMaybe we can revisit if the test generation time becomes unreasonable, like tens of seconds?' by a COLLABORATOR of type User on 2021-01-19T02:31:52Z\n'It's a common theme in many of our tests. There's the same problem in other pmap and xmap tests. I've been briefly entertaining an idea of switching the parametrization from generating a full list only to downsample it, to taking a method that generates a single case, parametrized by some PRNG/oracle that determines which choices to make. So instead of saying\r\n```py\r\njtu.cases_form_list(\r\n  {\"x\": x, \"y\": y, \"z\": z}\r\n  for x in range(10000000)\r\n  for y in range(10000000)\r\n  for z in range(10000000))\r\n```\r\nwe would instead say\r\n```py\r\njtu.cases_from_sampler(\r\n  lambda sampler: {\"x\": sampler.sample(10000000),\r\n                   \"y\": sampler.sample(10000000),\r\n                   \"z\": sampler.sample(10000000)})\r\n```\r\nTechnically the latter could still be used to generate an exhaustive list of tests, but we also would have the option to only evaluate its body a few times to generate a small subset of the space of potential test space.' by a COLLABORATOR of type User on 2021-01-19T11:29:53Z\n'1.5s is not a lot if we have a single huge parametrized test, but I expect us to have more of those in the future. And waiting for 20s just to generate a subset of test cases seems unreasonable.' by a COLLABORATOR of type User on 2021-01-19T11:30:29Z\n'Might be a good idea to at least comment this out?' by a COLLABORATOR of type User on 2021-01-19T11:32:37Z\n'What's `map_contract`? Does it talk about the mapped axes? If so, why is it a tuple of integers?' by a COLLABORATOR of type User on 2021-01-19T11:33:33Z\n'Instead of recursing over sets of excluded positions, wouldn't it be simpler to parametrize `helper` by lhs and rhs shapes, and pass sub-shapes to recursive calls?' by a COLLABORATOR of type User on 2021-01-19T11:35:50Z\n'Why is the `j <= i` check necessary?' by a COLLABORATOR of type User on 2021-01-19T11:36:39Z\n'Damn that's some ninja-level Python' by a COLLABORATOR of type User on 2021-01-19T11:38:20Z\n'Wow, awesome. That's really good to have!' by a COLLABORATOR of type User on 2021-01-19T11:39:43Z\n'One potential extension would also be to map a single logical axis to multiple mesh axes. So for example in the example output below you could also have:\r\n```\r\n([('j', ('x', 'y'))], [('x', 2), ('y', 2)])\r\n```\r\nIt's an edge case that would be good to test too!' by a COLLABORATOR of type User on 2021-01-19T11:41:37Z\n'I think that every `contextlib.contextmanager` is already both a manager _and_ a decorator, so please let's do! You should be able to just replace all uses of the old one with this (but maybe keep the previous name?)' by a COLLABORATOR of type User on 2021-01-19T11:42:48Z\n'Ugh this part was really confusing to me, because I kept wondering what does `PdotSpec.items()` do. I've only realized that you've shadowed the name a while later 🤦  Can you please change that?' by a COLLABORATOR of type User on 2021-01-19T12:08:25Z\n'lol, another ninja-level feature. Let-bindings in comprehensions!' by a COLLABORATOR of type User on 2021-01-19T12:09:52Z\n'This part is a little confusing, because it depends on some unsaid invariants of `PdotSpec` (in this case that `lhs_mapped` returns `(*map_contract, *map_batch)`. It would be nicer if we could put this spec generation in the `PdotSpec` itself, so that one doesn't have to remember about those properties to understand this bit of code.' by a COLLABORATOR of type User on 2021-01-19T12:13:22Z\n'Besides the testing slowdown, I had run into another problem with such tests. If this pattern generates 1000+ test cases from which we sample 10 by default or 25 in CI (and we pick always the same ones), it is quite possible that some important use cases are never tested until somebody runs the test manually with a very large NUM_GENERATED_CASES. That is in part why I asked how many tests are generated here. It looks likely to be in the thousands, and if it takes more than 1sec to generate them then it is possibly many thousands. ' by a COLLABORATOR of type User on 2021-01-20T07:05:32Z\n'One of my parametrizations generates ~250k cases and finishes in ~1.5s, so I'd expect the number to be O(100k) here too' by a COLLABORATOR of type User on 2021-01-20T12:28:09Z\n'> it is quite possible that some important use cases are never tested until somebody runs the test manually with a very large NUM_GENERATED_CASES\r\n\r\nThat seems like a pretty good process to me. It's worked well for us in the past, e.g. for the lazy sublanguage and for our early development of the more complex lax primitives (like conv_general_dilated, lax vmap tests, etc). During development (or revision) we manually run lots of them, then we rely on a subset for CI for regressions. (Regressions seem less likely in these cases.)\r\n\r\nWe took an alternative route for einsum testing, where we just added cases that we thought of. It didn't work well: the result was we missed lots of edge cases that had to be cleaned up as users ran into them! The space of einsum configurations was just too tricky to cover thoroughly with a priori thinking. I concluded systematic tests can be really useful.\r\n\r\nLuckily we don't have to choose between systematic coverage and tests for specific edge cases: we can do both! Perhaps a useful pattern (maybe even one that can be automated?) is to add explicit cases whenever we see a failure in the manually-run exhaustive tests. That way, we could have the exhaustive tests to run manually when we want to (during development or revision to the code in question), but perhaps by default their generation is skipped for speed (i.e. CI could skip generating them entirely, or we could figure out a more efficient way to generate a subsample). At the same time we can also have good explicit edge case coverage, both from whatever hand-crafting we come up with and from saving past failures of our systematic search.\r\n\r\nWDYT?' by a COLLABORATOR of type User on 2021-01-20T15:18:01Z\n'Yes, it's about mapped axes and uses positions. All the info in PdotSpec uses positional axes, in this case to identify which positional axes xmap should map. That is, the fields `map_contract` and `map_batch` are accessed (indirectly, via `lhs_mapped` and rhs_mapped` when setting up the `xmap` call itself, rather than the `pdot` call.\r\n\r\nThe properties `pos_contract_after_mapping` and `pos_batch_after_mapping` compute the positions _after_ mapping has eliminated some.\r\n\r\nDoes that make sense?' by a COLLABORATOR of type User on 2021-01-20T15:58:47Z\n'I think that makes the position computations more complicated. That is, when the recursive call returns axis positions, then the caller would need to increment them depending on whether it had eliminated an axis.\r\n\r\nI think it's better to keep things 'flat' so as to avoid as much axis incrementing bookkeeping as possible.\r\n\r\nWDYT? LMK if that's unclear and I can explain better.' by a COLLABORATOR of type User on 2021-01-20T16:00:06Z\n'~It avoids redundant test cases. That's necessary to, for example, use named parameterized tests rather than numbered ones.~ Sorry, that was wrong, maybe from an earlier version of the code...' by a COLLABORATOR of type User on 2021-01-20T16:00:33Z\n'Done.' by a COLLABORATOR of type User on 2021-01-20T20:43:52Z\n'Oh great idea, I forgot about that! Will do.' by a COLLABORATOR of type User on 2021-01-20T20:44:57Z\n'Oops. That was an oversight. Thanks for the catch!' by a COLLABORATOR of type User on 2021-01-20T20:47:04Z\n'I can't help myself. I love Python golf.' by a COLLABORATOR of type User on 2021-01-20T20:47:28Z\n'Hmm, well right now `PdotSpec` doesn't include any name information. But we could factor the names into it, which might help simplify this calling code as well as other things. I'll give that a shot.' by a COLLABORATOR of type User on 2021-01-20T20:48:46Z\n'By the way, as per my earlier comment in this thread, I think the best option here is to keep figuring out a faster approach to (subsampled) systematic testing while also not blocking this particular PR on it, since this PR doesn't itself introduce any problematic test generation times. That is, we should solve the test generation timing problem before it becomes an issue with these xmap tests, but it doesn't need to be solved in this PR.' by a COLLABORATOR of type User on 2021-01-20T20:59:07Z\n'Ah, a bug! This code was generating duplicate cases. Will fix...' by a COLLABORATOR of type User on 2021-01-20T21:08:43Z\n'On the other hand, doing it this way this was a source of a bug that caused duplicate cases to be generated :)' by a COLLABORATOR of type User on 2021-01-20T21:22:00Z\n'It seems there are 15295 tests generated in my current file, though I've been changing things so it may not reflect the same number as an earlier version. I did a more accurate timing measure and found it required took less than 0.09 seconds to generate them, and 0.96 seconds to generate and subsample them. So we're okay on timing for now I think!' by a COLLABORATOR of type User on 2021-01-21T06:45:10Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 30,
        "reason_for_closure": "Inconsistent with numpy; PR closed as it would break existing users."
    },
    {
        "summary": "Pull Request '5434' titled 'Make jax.random.categorical compatible with jax_numpy_rank_promotion='raise'' was authored by a User, who is associated as a NONE. \nIt was created at 2021-01-15T11:16:35Z, and was closed at 2022-02-17T19:40:24Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Without this, jax.random.categorical raises an exception when using a non-empty sample_shape and jax_numpy_rank_promotion is set to 'raise'.'\nPR has comments:\n'Can you add a unit test?' by a MEMBER of type User on 2021-01-15T22:30:12Z\n'Looks like this was fixed in https://github.com/google/jax/pull/7147' by a COLLABORATOR of type User on 2022-02-17T19:40:24Z\n\nPR has review comments:\n'Slight nit: I'd prefer `lax.expand_dims` here, since you are inserting size-1 dimensions. `reshape` is a big hammer and we'd prefer to avoid it where its power is not needed.' by a COLLABORATOR of type User on 2021-01-20T14:54:02Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 1,
        "reason_for_closure": "Incorrect :- PR fixed incompatibility with numpy, but jax features are a superset of numpy, hence reverted."
    },
    {
        "summary": "Pull Request '5305' titled 'Add custom JVP for scipy.special.gammaln' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2021-01-04T07:45:18Z, and was closed at 2021-01-05T00:38:39Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'This gives a 50x speedup on my machine for `grad(jac.scipy.special.gammaln)(4.0)`' by a CONTRIBUTOR of type User on 2021-01-04T07:45:49Z\n\nPR has review comments:\n'It looks like this line is already written here?\r\nhttps://github.com/google/jax/blob/afe612217b6d956ee02d924ec286ebdb6fabdbb7/jax/_src/lax/lax.py#L2315' by a COLLABORATOR of type User on 2021-01-04T21:25:36Z\n'@shoyer Ah, I didn't see that!  Sorry for the noise.  I just ran it with %timeit and got different results, but I should have just tried it with `make_jaxpr` to begin with.' by a CONTRIBUTOR of type User on 2021-01-05T00:38:34Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "PR closed as unnecessary: implemented functionality is valid and shouldn't be deprecated."
    },
    {
        "summary": "Pull Request '5108' titled '[jax2tf] Fix bug in conversion of bfloat16 to tensor' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-12-05T00:24:59Z, and was closed at 2020-12-07T17:45:30Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'Fixes #5106'\nPR has comments:\n'OK - given review and offline conversations, I'm going to close this. Here's the plan as I understand it:\r\n\r\n- disable the problematic `bfloat16_constant` test in #5085 to unblock it\r\n- wait for the TF/JAX bfloat16 unification work to allow this special path for bfloat16 conversion to be removed\r\n- at that point re-enable the test disabled in #5085' by a COLLABORATOR of type User on 2020-12-07T17:45:30Z\n\nPR has review comments:\n'Do we really need this abbreviation? It only saves a few characters.' by a COLLABORATOR of type User on 2020-12-06T13:25:24Z\n'Since the JAX and TF `bfloat16` actually have the same representation, even though they look like different types, is it possible to do the conversion without any array copy? ' by a COLLABORATOR of type User on 2020-12-06T13:27:31Z\n'@bchetioui PTAL' by a COLLABORATOR of type User on 2020-12-06T13:27:51Z\n'Saving characters wasn't the goal: this is what we use in `lax_numpy.py` and I thought it makes the intent clearer at the call-site. Happy to use `np.result_type` directly if you'd prefer.' by a COLLABORATOR of type User on 2020-12-06T15:34:32Z\n'This is minor, but I do prefer `np.result_type`. This is one aspect of Google style guide that I like to use: identifiers from other modules use `module.id` style. ' by a COLLABORATOR of type User on 2020-12-07T08:16:01Z\n'I am actually not able to pass this test on my machine. @jakevdp what versions of TF and NumPy are you running this with?' by a COLLABORATOR of type User on 2020-12-07T09:50:13Z\n'I can not think of a way to do it from inside jax2tf. However, with a little more careful of handling of dtypes (i.e. adding calls to `to_jax_dtype` and `to_tf_dtype` where it is necessary to get the expected bfloat16 representation), we should be able to make a call like `jax2tf.convert(func_jax)(jnp_bfloat16_array.view(tf.bfloat16.as_numpy_dtype))`.\r\n\r\nThis is not ideal – as it puts a burden on the user, and adds some clutter to the code (which seems like it might be annoying to test thoroughly). The ideal solution would be for the TF and JAX bfloat16 to be merged together, and then all this logic can go away.' by a COLLABORATOR of type User on 2020-12-07T11:22:26Z\n'This merging of bfloat16 is now in progress, perhaps we can wait with this PR until that merging lands in TF.' by a COLLABORATOR of type User on 2020-12-07T12:01:29Z\n'The bug this PR addresses is blocking work on weak types, so I’d prefer not to delay the fix.' by a COLLABORATOR of type User on 2020-12-07T13:41:39Z\n'Another option would be to wait, but disable the problematic tests, with the understanding that changes to `lax.convert_element_type` will break some uses of `bfloat16` in jax2tf by virtue of converting inputs to DeviceArrays more predictably.' by a COLLABORATOR of type User on 2020-12-07T13:44:39Z\n'Yes, it fails on my machine too, but no longer fails with an infinite recursion error.\r\n\r\nI pushed it because ~half the jax2tf tests fail on my machine, so I was hoping it was one of the expected failures :grin:' by a COLLABORATOR of type User on 2020-12-07T13:45:55Z\n'Oh got it! I think tests fail because you are not using a recent enough version of `tf-nightly` :-) I think the tests should run properly if you simply use the latest one. Otherwise, for future reference, the [jax2tf README](https://github.com/google/jax/tree/master/jax/experimental/jax2tf) should in principle document a version that works (actually it doesn't right now, because a breaking bfloat16 change happened in October, so I will update it!).\r\n\r\nBTW, one reason we didn't catch the error you encountered sooner is that we perform all our jax2tf tests in the TF world, i.e. with the JAX arguments converted to TF tensors with the proper dtype. The failure you encounter is a similar kind of bfloat16 bug as the one you fixed, which I think we can work around. But to be honest, if that logic has to be removed very soon (when the bfloat16 dtypes in TF and JAX are unified), it might not be worth fixing here – because verifying that all these edge cases are handled properly is probably not effortless.\r\n\r\nI would be in favor of disabling whatever problematic tests there may be in the meantime (I actually do not expect that any of our tests will fail because of your change to `lax.convert_element_type` btw, since as I mentioned, all the calls they make to jax2tf conversions use `tf.Tensor`s as parameters).' by a COLLABORATOR of type User on 2020-12-07T14:16:45Z\n'OK, sounds good. For reference, this is the PR in question: https://github.com/google/jax/pull/5085\r\n\r\nThe jax2tf failures that are blocking the merge only show up on internal tests. I'll disable the relevant tests in order to get that change in.' by a COLLABORATOR of type User on 2020-12-07T14:38:08Z\n'SGTM! My bad, I can not see internal failures! :)' by a COLLABORATOR of type User on 2020-12-07T14:41:50Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 14,
        "reason_for_closure": "Unnecessary: PR closed as the proposed change was deemed not beneficial."
    },
    {
        "summary": "Pull Request '4977' titled '[jax2tf] Fix and test conversion of iota' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-11-20T11:53:00Z, and was closed at 2020-11-20T16:42:11Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'Closed in favor of #4984 ' by a COLLABORATOR of type User on 2020-11-20T16:42:11Z\n\nPR has review comments:\n'This is needed for `dtypes.bfloat16` to be properly converted to `tf.bfloat16`, and makes it clear that our dtype is in the TF world :)' by a COLLABORATOR of type User on 2020-11-20T11:53:55Z\n'Good catch!' by a COLLABORATOR of type User on 2020-11-20T15:36:05Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as superseded by another PR/fix."
    },
    {
        "summary": "Pull Request '4817' titled 'Investigations into batch polymorphism.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-11-06T16:41:18Z, and was closed at 2023-03-04T18:11:18Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Creating this draft to investigate how well JAX and JAX2TF support batch polymorphism. So far, this includes:\r\n\r\n- [x] JAX tests based on the Haiku modules tests (1)\r\n- [ ] JAX tests based on the Flax modules (maybe models?) (2)\r\n- [ ] (1) with JAX2TF\r\n- [ ] (2) with JAX2TF\r\n\r\n'\nPR has comments:\n'So far, the following (22/41) tests are failing:\r\n```\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_BatchNorm\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Embed\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(Conv1DLSTM, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(Conv1DLSTM, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(Conv2DLSTM, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(Conv2DLSTM, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(Conv3DLSTM, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(Conv3DLSTM, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(GRU, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(GRU, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(IdentityCore, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(IdentityCore, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(LSTM, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(LSTM, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(ResetCore, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(ResetCore, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(VanillaRNN, dynamic_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_Recurrent(VanillaRNN, static_unroll)\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_nets.MobileNetV1\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_nets.ResNet\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_nets.VectorQuantizer\r\nFAILED tests/batch_polymorphism_test.py::BatchPolymorphismTest::test_shapecheck_nets.VectorQuantizerEMA\r\n```\r\nand an argument has been added to `shapecheck` to optionally specify `dtypes`, as in some cases one cannot trace with `np.float32` parameters. Investigation is ongoing!' by a COLLABORATOR of type User on 2020-11-06T16:44:22Z\n'Closing as stale.' by a COLLABORATOR of type User on 2023-03-04T18:11:18Z\n\nPR has review comments:\n'`map` works with functions with multiple arguments, so you should be able to write the above as:\r\n```\r\nmap(lambda in_shape, in_dtype: ShapedArray(in_shape, dtype=in_dtype), in_shapes, in_dtypes)\r\n```' by a COLLABORATOR of type User on 2020-11-07T12:20:26Z\n'Perhaps we should name this as `in_dtypes`?' by a COLLABORATOR of type User on 2020-11-07T12:21:20Z\n'This is fine for an experiment (which it is), but we won't be able to submit this because we should not take a dependency on haiku.' by a COLLABORATOR of type User on 2020-11-07T12:22:23Z\n'I think that this kind of addition to `shapecheck` is important to have, but the actual mechanism does not seem very ergonomic because one has to specify the `in_shapes` and `in_dtypes` separately. Perhaps a better solution is to use a ShapeDtypeStruct where the shape can be a string?' by a COLLABORATOR of type User on 2020-11-08T09:02:30Z\n'There is a way to avoid this problem, by using jax2tf with shape-polymorphic conversion. That will exercise the same code as shapecheck, but will pickup dtype from the actual arguments (like the masking transform). This alternative will exercise more code, e.g., the jax2tf shape-polymorphic conversion, which is why I suggested earlier to just focus on shapecheck. But if shapecheck is blocking us, it is better to go directly to jax2tf.  ' by a COLLABORATOR of type User on 2020-11-09T11:49:36Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 5,
        "reason_for_closure": "Closed as obsolete/redundant: PR addressed an inconsistency that was deemed acceptable/desirable behavior."
    },
    {
        "summary": "Pull Request '4682' titled '[x64] Add initial weak_type logic to abstract eval rules' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-10-22T21:56:54Z, and was closed at 2020-12-08T13:52:57Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This is a small part of the exporation from #3903. The main effect of this change is to make unary lax operations propagate the `weak_type` from input to output. This solves some of the jit invariance issues outlined in #3903, while punting on the more difficult questions of how to handle the full dtype promotion table in the presence of weak types.\r\n\r\nExample repro:\r\n```python\r\nimport jax.numpy as jnp\r\nfrom jax import jit\r\n\r\nx = jnp.arange(4, dtype='float16')\r\n\r\ndef add(x, y):\r\n  return x + abs(y)\r\n\r\nprint(add(x, 1.0).dtype)\r\nprint(jit(add)(x, 1.0).dtype)\r\n```\r\nOutput before this change:\r\n```\r\nfloat16\r\nfloat32\r\n```\r\nOutput after this change\r\n```\r\nfloat16\r\nfloat16\r\n```'\nPR has comments:\n'Replaced by #4850' by a COLLABORATOR of type User on 2020-12-08T13:52:57Z\n\nPR has review comments:\n'This test is not strictly necessary for this PR, but it is a good TDD case for the next step in implementing weak types, so I thought it would be useful to include.' by a COLLABORATOR of type User on 2020-10-22T22:00:43Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Superseded :- PR closed in favor of maintaining broader JAX functionality."
    },
    {
        "summary": "Pull Request '4539' titled 'Add jax.numpy.polyint' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-10-11T09:23:05Z, and was closed at 2021-02-27T01:16:06Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Though not mentioned in the numpy [documentation](https://numpy.org/doc/stable/reference/generated/numpy.polyint.html), this implementation matches `np.polyint` in the cases `m > 1 and len(k) == 1` as well as `len(k) > m`.\r\n\r\nI've also added some `np.` (i.e., base `numpy`) qualifiers in the `polyder` implementation as my impression is that this improves constant-folding for `jax.make_jaxpr` and `jax.jit`.'\nPR has comments:\n'Just as an FYI: we moved some of these files into `jax/_src/numpy`, so you'll need to rebase your PR on top of head.' by a COLLABORATOR of type User on 2020-10-17T16:43:22Z\n'`polyint` was added in #5731' by a COLLABORATOR of type User on 2021-02-27T01:16:06Z\n\nPR has review comments:\n'If we're requiring `m` to be concrete (as implied by the change from `arange` to `np.arange` below), then we should call\r\n```python\r\nm = core.concrete_or_error(int, m, \"The error arose in jnp.polyder argument `m`\")\r\n```\r\nSame in `polyint` below.' by a COLLABORATOR of type User on 2020-10-12T16:20:50Z\n'We're moving toward requiring inputs to be arrays to start with (though proceeding carefully for existing functions for backward compatibility). For a new function like this, we should call `_check_arraylike('polyint', p)` before `p = asarray(p)`.\r\n\r\nSimilar for `k` when it is not `None`.' by a COLLABORATOR of type User on 2020-10-12T16:22:40Z\n'I can never remember the precedence order of commas and ternary operators. For readability, could you add some parentheses to make this clearer? e.g.\r\n```python\r\nlambda: [rng(a_shape, dtype), (None if k_shape is None else rng(k_shape, dtype))]\r\n```' by a COLLABORATOR of type User on 2020-10-12T16:28:00Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 3,
        "reason_for_closure": "Inconsistent behavior: PR attempts to match numpy but JAX generally extends numpy functionality."
    },
    {
        "summary": "Pull Request '4496' titled '[jax2tf] Ensure that in tests TF does not constant-fold in eager mode…' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-10-08T09:30:49Z, and was closed at 2020-10-08T17:47:27Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of '… before compiling\r\n\r\nI had noticed that `tf.function(tf.math.maximum, experimental_compile=True)(1., 2.)` would actually just constant fold the operation in eager mode and compile a constant. This can be prevented by using `input_signature` (which this PR does). Alternatively it seems that it would be prevented by ensuring that the arguments are arrays.'\nPR has comments:\n'Closing in favor of #4501.' by a COLLABORATOR of type User on 2020-10-08T17:47:24Z\n\nPR has review comments:\n'Is `tf.nest.map_structure` equivalent to using `jax.tree_util.tree_map` in this case? Also, does that mean we should also replace the simple `map` at line 129 with a similar operation?' by a COLLABORATOR of type User on 2020-10-08T10:10:42Z\n'Indeed, `tf.nest.map_structure` is like `tree_map` (except the latter has some fancy custom nodes, which are not relevant for our tests). Maybe we could use the latter, but I felt that since we are in TF world here, perhaps it is best to stick to TF mechanisms. \r\n\r\nYou are right that the map in line 129 should also use `tf.nest.map_structure`. Looking at `convert_if_bfloat16`, should it not use `tf.convert_to_tensor` always? \r\n ' by a COLLABORATOR of type User on 2020-10-08T10:37:00Z\n'The case when `tf.convert_to_tensor` is not called is typically gonna occur when the arg in question is a scalar; not sure if it is necessary to explicitly convert there?\r\n\r\nAlso, I had a similar reasoning as yours, but in the opposite direction: we are in the \"TF world\" wrt the leaf objects, but the structure is still a JAX one? (If that makes sense?)' by a COLLABORATOR of type User on 2020-10-08T10:48:38Z\n'I have changed the PR to apply tf.nest.map_structure for convert_if_bfloat16' by a COLLABORATOR of type User on 2020-10-08T14:43:14Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Closed as Won't Fix: PR functionality deemed unnecessary and potentially harmful to existing users."
    },
    {
        "summary": "Pull Request '4375' titled '[jax2tf] Implement proper tracking of which primitives have dedicated tests (WIP)' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-09-22T11:03:01Z, and was closed at 2023-03-04T17:58:18Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'We want to know exactly which primitive conversions are missing dedicated tests.\r\nThe method we used before was just showing that some primitives were never called throughout the tests, which didn't give much insight into whether they were actually properly tested or not.'\nPR has comments:\n'I am not yet convinced that we want this code change. I do think that we will need a way to organize the harnesses, but I feel that we have not yet thought through the use cases. E.g., an alternative way would be to add to the harness a tuple of primitives that are covered. We would also need a way to list all the harnesses. \r\n\r\n' by a COLLABORATOR of type User on 2020-09-22T12:13:39Z\n'Closing as stale.' by a COLLABORATOR of type User on 2023-03-04T17:58:18Z\n\nPR has review comments:\n'If we associate all the tests to all of the primitives, aren't we going to run some tests multiple times?' by a COLLABORATOR of type User on 2020-09-22T12:08:32Z\n'I would not call this `tf_conversion_test`. These are harnesses that exercise JAX primitives. I am really hoping to be able to use them for other testing. ' by a COLLABORATOR of type User on 2020-09-22T12:09:51Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as stale."
    },
    {
        "summary": "Pull Request '4346' titled 'Disable CPP Jax jit tests.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-09-18T20:22:07Z, and was closed at 2020-09-21T23:39:08Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'it's because I will do a breaking change in jax_jit. As I need 2 Cls (one on google3 for TF, one on github for JAX), and that they are incompatible, I must disable this (otherwise google3/ tests will fail).\r\n\r\nWe do need to move jax_jit and co to JAX.\r\n\r\nThe follow-up will be cr/332509389.'\nPR has comments:\n'This went in as https://github.com/google/jax/commit/9f53d2a8d883bc6eefc1f701cdc6639bdc774f84; seems to have been a fluke in the new copybara flow.' by a CONTRIBUTOR of type User on 2020-09-21T23:39:08Z\n\nPR has review comments:\n'Nit: shouldn't we just skip the test? Won't `PythonJitTest` also run the Python tests?' by a COLLABORATOR of type User on 2020-09-21T16:14:01Z\n'it's not just the test we need to skip, it's all the tests of this class.\r\n\r\nIt's a base class used by PythonJitTest, so if disable tests, it will also be disabled there.\r\n\r\nBy doing this, it will run CPPJitTest, but as the PythonTests' by a CONTRIBUTOR of type User on 2020-09-21T16:23:08Z\n'Doesn't PythonJitTest override this method?' by a COLLABORATOR of type User on 2020-09-21T16:31:15Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 3,
        "reason_for_closure": "Implemented :- PR closed as the fix is included in newer dependency version."
    },
    {
        "summary": "Pull Request '4175' titled 'Gmres qr' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-08-28T21:45:20Z, and was closed at 2020-11-24T20:19:34Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'A synthesis of @gehring's and @shoyer's GMRES implementation #4025 with the one in TensorNetwork. \r\n\r\nPer those authors, this implementation supports both preconditioning and pytrees, but I've modified it to also use a more efficient QR implementation of the main Arnoldi loop that builds the QR decomposition of the Krylov matrix instead of the matrix itself. This greatly reduces the overhead of the final linear solve, since the relevant system is now triangular. It also allows the iteration to terminate early if convergence is reached midway through the Arnoldi iterations.\r\n\r\nThe behaviour of the maxiter and restart arguments has also been modified (I think) to mirror that in SciPy. Finally, a few bugs have been fixed such that the implementation passes some simple tests, but I haven't tested very extensively. \r\n\r\n@gehring @shoyer, it would be great if you had a look!\r\n\r\nRelevant issues: #1531 '\nPR has comments:\n'All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F4175) for more info**.\n\n<!-- need_author_consent -->' by a NONE of type Bot on 2020-08-28T21:45:27Z\n'> I've modified it to also use a more efficient QR implementation of the main Arnoldi loop that builds the QR decomposition of the Krylov matrix instead of the matrix itself. This greatly reduces the overhead of the final linear solve, since the relevant system is now triangular. It also allows the iteration to terminate early if convergence is reached midway through the Arnoldi iterations.\r\n\r\nThis makes complete sense! I think it may also improve numerical stability.\r\n\r\nOn the other hand, `apply_givens_rotation` is another un-vectorized loop, so this _might_ significantly slow things down on accelerators. If that is the case, then we might want to have two implementations: the Given rotation version allowing for early termination, and the version that runs for some fixed number of iterations.\r\n\r\nTo inform this decision, it would be helpful to do have microbenchmarks results, similar to those I had in https://gist.github.com/shoyer/dc33a5850337b6a87d48ed97b4727d29' by a COLLABORATOR of type User on 2020-08-28T22:40:23Z\n'@googlebot I consent.' by a CONTRIBUTOR of type User on 2020-08-29T20:27:58Z\n'You are right the extra loop could conceivably lead to a large speedup on accelerators in certain use cases - those in which the matvec operation does not entirely dominate the expense. \r\n\r\nHowever, a GPU user may also encounter a case where this operation does dominate, but the ability to terminate the Arnoldi iterations early is important (for example, because GMRES is to be run on many different problems in sequence over the course of an iteration, and the appropriate incoming Krylov dimension is therefore not known.\r\n\r\nI would therefore, in the case that the original implementation were found to be sometimes favourable, recommend that the user be provided with both options when switching to the GPU.' by a CONTRIBUTOR of type User on 2020-09-02T05:22:43Z\n'Just letting you all know I'm back from vacation (which is where I've been) and am working on this again. Updates to follow once I've made them.' by a CONTRIBUTOR of type User on 2020-09-14T14:55:10Z\n'@mganahl Your comments don't seem to have attached themselves to the relevant lines of code, so I'm not sure exactly what you're referring to, but in sequence:\r\n\r\n- All print commands will indeed likely be removed;\r\n- This implementation of GMRES allows b and x0 to be a data structure called a pytree that I honestly don't understand very well (it might be something to look into for symmetric tensors however). Along with a few other quirks, the tree_map statements are there to support this structure.\r\n- Early stopping of Arnoldi should I think be redundant with early stopping of GMRES. But we do have a tolerance threshold that stops the normalizations from blowing up just in case.' by a CONTRIBUTOR of type User on 2020-09-14T15:06:49Z\n'I've added better tests and done a bit of fiddling. The present implementation appears to give comparable (actually slightly better) error as SciPy when it works, but is also much more unstable than SciPy, failing completely in many cases that SciPy handles easily (for example, in most cases when maxiter > 1, and even for maxiter == 1 in some cases). The reason for this is not yet clear.' by a CONTRIBUTOR of type User on 2020-09-14T20:41:45Z\n'It might be make sense to look over SciPy's implementation to see if it handles any edge cases that we don't handle here. The source code for SciPy's iterative solvers can be found here:\r\nhttps://github.com/scipy/scipy/tree/master/scipy/sparse/linalg/isolve\r\n\r\nThe GMRES implementation uses a mix of Python and Fortran:\r\nhttps://github.com/scipy/scipy/blob/914523af3bc03fe7bf61f621363fca27e97ca1d6/scipy/sparse/linalg/isolve/iterative.py#L432\r\nhttps://github.com/scipy/scipy/blob/914523af3bc03fe7bf61f621363fca27e97ca1d6/scipy/sparse/linalg/isolve/iterative/GMRESREVCOM.f.src\r\n\r\n(You might also or alternatively consider implementing L-GMRES, which I've found to be generally a superior alternative to GMRES with restarts.)' by a COLLABORATOR of type User on 2020-09-14T21:10:23Z\n'The SciPy code isn't the easiest on Earth to read, is it? Still, I'm hopeful because the TensorNetwork version does not suffer these instabilities. Lemme poke a bit more. ' by a CONTRIBUTOR of type User on 2020-09-15T16:38:18Z\n'Current, I've identified 2 failure modes in the previous implementation. The 1st was the normalization causing nans and is already handled, but the second I only recently identified and is caused by trying to solve a zero/singular matrix once the basis was constructed (a problem that is made more common when thresholding the normalization). It might be worth checking if that is what is happening here. The fix should be straightforward if that is the case.' by a CONTRIBUTOR of type User on 2020-09-15T17:03:18Z\n'Excellent, this is very likely the problem. This may have something to do\nwith the input matrix having large condition number. That, however, is not\nthe full picture because I find the algorithm consistently breaks down when\nA is the identity matrix, which is obviously invertible.\n\n\n\nOn Tue, Sep 15, 2020 at 1:03 PM gehring <notifications@github.com> wrote:\n\n> Current, I've identified 2 failure modes in the previous. The 1st was the\n> normalization causing nans and is already handled, but the second I only\n> recently identified and is caused by trying to solve a zero/singular matrix\n> once the basis was constructed (a problem that is made more common when\n> thresholding the normalization). It might be worth checking if that is what\n> is happening here. The fix should be straightforward if that is the case.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/4175#issuecomment-692847830>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAINHBSRP2MONL65LD6QVHTSF6M6NANCNFSM4QORQ26Q>\n> .\n>\n' by a CONTRIBUTOR of type User on 2020-09-15T17:21:21Z\n'I've also encountered it when the residual is very small right from the start, e.g. the b vector is close to zero or the initial guess is very good. This was obviously an issue for the batched version but maybe this also affects this implementation if it always runs at least one iteration.' by a CONTRIBUTOR of type User on 2020-09-15T17:36:43Z\n'I think I know the problem: in the TensorNetwork version, the final solve\nis sliced to include only the submatrix formed during the Arnoldi process.\nIn this code this is not (easily) possible since everything is traced, so\nthe final solve involves the full Arnoldi matrix, which is mostly zeroes in\nthe case of early termination. The resulting problem can thus very easily\nbe singular.\n\nTo solve it we need to find a way to involve only the relevant submatrix in\nthe solve, a tricky proposition given that the number of iterations is a\ntraced value.\n\nOn Tue, Sep 15, 2020 at 1:36 PM gehring <notifications@github.com> wrote:\n\n> I've also encountered it when the residual is very small right from the\n> start, e.g. the b vector is close to zero or the initial guess is very\n> good. This was obviously an issue for the batched version but maybe this\n> also affects this implementation if it always runs at least one iteration.\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/4175#issuecomment-692866096>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAINHBXZ5NW3YYUQGPZECJTSF6Q3VANCNFSM4QORQ26Q>\n> .\n>\n' by a CONTRIBUTOR of type User on 2020-09-15T17:51:38Z\n'Ah, that makes sense! Would initializing the triangular matrix with the identity instead of zeros then simply throwing/masking out the dummy variables after the fact be a reasonable workaround?' by a CONTRIBUTOR of type User on 2020-09-15T17:57:21Z\n'Yes, that might well work. I will try when I get a second.\n\nOn Tue, Sep 15, 2020 at 1:57 PM gehring <notifications@github.com> wrote:\n\n> Ah, that makes sense! Would initializing the triangular matrix with the\n> identity instead of zeros then simply throwing/masking out the dummy\n> variables after the fact be a reasonable workaround?\n>\n> —\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/4175#issuecomment-692878820>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAINHBRDMVJAKD3C2HQJP5DSF6TJDANCNFSM4QORQ26Q>\n> .\n>\n' by a CONTRIBUTOR of type User on 2020-09-15T18:06:59Z\n'It works! You're a genius.' by a CONTRIBUTOR of type User on 2020-09-15T19:07:17Z\n'Awesome!' by a CONTRIBUTOR of type User on 2020-09-15T19:21:28Z\n'What's the certified way to do `assert a <= b` in a test? Right now I've had to resort to `assert` and exception handling.' by a CONTRIBUTOR of type User on 2020-09-15T20:34:27Z\n'> What's the certified way to do `assert a <= b` in a test? Right now I've had to resort to `assert` and exception handling.\r\n\r\nhttps://docs.python.org/3/library/unittest.html#unittest.TestCase.assertLess' by a COLLABORATOR of type User on 2020-09-15T21:27:32Z\n'I did some benchmarking. CPU performance seems to be comparable to SciPy, but the mystery Colab GPU performs much (~ 10X) more slowly. The GPU runtimes appear to be almost independent of the matrix size, which strongly suggests something about the GMRES overhead (rather than the matvec itself) is the culprit. I haven't checked the QR version against the fixed-iteration one yet.\r\n\r\nhttps://colab.research.google.com/drive/1DZBFNQeXiDuiyPxXRCJFnuSknaUzB1HU?usp=sharing' by a CONTRIBUTOR of type User on 2020-09-16T15:07:57Z\n'The difference between the fixed and QR implementations seems to be minor; in addition CPU performance is actually a bit slower than SciPy. Perhaps there's some sort of vectorization trickery I'm missing?' by a CONTRIBUTOR of type User on 2020-09-16T18:22:15Z\n'> The difference between the fixed and QR implementations seems to be minor; in addition CPU performance is actually a bit slower than SciPy. Perhaps there's some sort of vectorization trickery I'm missing?\r\n\r\nYou want to be careful to always `jit` the outer-most function when you are benchmarking in JAX, and to use `block_until_ready()`. I did a few tweaks to make that work in your notebook:\r\nhttps://gist.github.com/shoyer/2f86513877fcbf044cea964060dd8de3\r\n\r\nOtherwise you are mostly measuring JIT compilation time, not computation time.' by a COLLABORATOR of type User on 2020-09-16T19:04:33Z\n'Alright, well I don't quite understand why I'm unable to reproduce your profiling here, but based on your notebook we seem to have the following results at the given parameters:\r\n\r\n- On the CPU, the QR implementation is significantly (~30%) faster than the fixed one, even when we don't terminate early.\r\n- On the GPU, the QR implementation is about 10x slower than the fixed one.\r\n- The final linear solve consumes about 40% of the overall runtime.\r\n\r\nThe latter result would seem to indicate that SciPy's ability to dynamically resize the Arnoldi matrix potentially confers a significant advantage, and thus our workaround is perhaps not ideal (although it may be unavoidable). In addition, it seems that maintaining both implementations is desirable.\r\n\r\nIt is worth pointing out that the case where the matvec operator multiplies by a known dense matrix is kind of a weird edge case - unless the matrix were known to be well approximated by a low-dimensional Krylov space, a dense solver would probably be preferred. So the large impact of the GMRES overhead compared to the matvec itself may be misleading. Certainly this is the case in TensorNetwork applications.' by a CONTRIBUTOR of type User on 2020-09-16T20:43:45Z\n'The \"fixed\" implementation could be made to end the Arnoldi process once an invariant subspace was found, without necessarily involving the QR loop. This would probably have similar properties re: early termination as the QR one without breaking the GPU so badly. It would still be worth keeping both I think, since the QR has other advantages.' by a CONTRIBUTOR of type User on 2020-09-16T20:50:41Z\n'I've noticed that both CG and gmres currently store the Arnoldi vectors as the columns of V rather than the rows. Since by default we presumably use row-major linear ordering this seems potentially problematic, especially on the GPU. Would it be worth transposing the program?' by a CONTRIBUTOR of type User on 2020-09-18T15:20:07Z\n'I found a bug I need to fix before this is ready; but does anyone know why the Read the Docs build is failing?' by a CONTRIBUTOR of type User on 2020-09-22T17:58:51Z\n'Ok, this one seems to work. However I'm still not sure what's going on with the documentation.' by a CONTRIBUTOR of type User on 2020-09-22T19:06:16Z\n'Achievement unlocked: @rmlarsen lending his expertise to JAX linear algebra 🥇 Woohoo!' by a COLLABORATOR of type User on 2020-09-25T22:24:37Z\n'The main thing holding me back here is that I don't understand why Read the Docs is failing - I've never used it before and the error messages in this case are not very helpful, at least to me. Does anyone have an idea?' by a CONTRIBUTOR of type User on 2020-09-28T14:16:02Z\n'This might be ready to merge!' by a CONTRIBUTOR of type User on 2020-10-16T15:36:41Z\n\nPR has review comments:\n'I don't love the `n_kry` name, which took me a while to figure out. Could you stick with the name `maxiter`, or maybe something more verbose like `n_krylov` or `krylov_dim`?' by a COLLABORATOR of type User on 2020-08-28T22:04:34Z\n'It looks like this scan is scanning over the columns of `V`?\r\n\r\nOne key implementation detail from https://github.com/google/jax/pull/4025 / https://github.com/google/jax/issues/1531#issuecomment-657164860 is the use of a matrix/vector product rather than a loop. As I noted in that comment, \"This is perhaps 10-20% faster on CPU and ~10x faster on GPUs\". So I suspect this is missing that aspect.\r\n\r\nMy version also used two iterations of classical gram Schmidt, which apparently can be important for ill-conditioned matrices per https://slepc.upv.es/documentation/reports/str1.pdf' by a COLLABORATOR of type User on 2020-08-28T22:17:17Z\n'This is a bit of a \"gotcha\" currently but if you're doing linear algebra on TPUs it's important to set `precision=lax.Precision.HIGHEST` whenever doing dot-product. Otherwise it defaults to bfloat16, which is usually not enough.' by a COLLABORATOR of type User on 2020-08-28T22:24:47Z\n'This test file should go in `tests/`, for consistency with JAX's other tests.' by a COLLABORATOR of type User on 2020-08-28T22:25:31Z\n'Generally you should not be adding these assertion wrappers :)\r\n\r\nInstead, use parameterized tests (see the existing test suite for examples).' by a COLLABORATOR of type User on 2020-08-28T22:26:24Z\n'One other unit test that would be nice to have is a consistency check with SciPy's GMRES after a small fixed number of iterations (before convergence). I have a version of that in my gist: https://gist.github.com/shoyer/dc33a5850337b6a87d48ed97b4727d29' by a COLLABORATOR of type User on 2020-08-28T22:27:46Z\n'I think the relevant parameter is actually very confusingly named `restart`, which is why I favour mentioning the Krylov dimension explicitly. However, I do support either of your two options - perhaps `krylov_dim` slightly more.' by a CONTRIBUTOR of type User on 2020-09-02T05:17:30Z\n'I had not realized this was so carefully considered, and will restore the old code.' by a CONTRIBUTOR of type User on 2020-09-02T05:17:52Z\n'Sounds good, thank you!' by a CONTRIBUTOR of type User on 2020-09-02T05:18:18Z\n'Ok, I'll write parameterized tests there.' by a CONTRIBUTOR of type User on 2020-09-02T05:18:40Z\n'Looks great' by a CONTRIBUTOR of type User on 2020-09-02T05:19:01Z\n'`0.*y` will fail if `y` is a pytree.\r\n```suggestion\r\n    lambda y: (tree_map(jnp.zeros_like, y), 0.),  # To get the dtype right\r\n```' by a CONTRIBUTOR of type User on 2020-09-03T15:45:20Z\n'This should work and is a bit more direct\r\n```suggestion\r\n               jnp.zeros((), dtype=norm.dtype)),  # To get the dtype right\r\n```' by a CONTRIBUTOR of type User on 2020-09-15T19:23:24Z\n'I would expect `_vdot_tree` to always return a non-negative number. Is this not the case?' by a CONTRIBUTOR of type User on 2020-09-15T19:24:30Z\n'See other comment about `_vdot_tree`' by a CONTRIBUTOR of type User on 2020-09-15T19:32:34Z\n'It's to cast to real' by a CONTRIBUTOR of type User on 2020-09-15T19:34:15Z\n'I think this should use `assume_real=True` then? `assume_real` appears to refer to the output rather than the input variables.' by a COLLABORATOR of type User on 2020-09-15T19:40:12Z\n'This likely will never have any noticeable impact but I'll mention it just for the sake of keeping things as simple as possible. Using `_dot` here might be unnecessary. Is there a reason why it was changed from `jnp.dot`, e.g., `tree_map(lambda X: jnp.dot(X[..., :-1], y), V)`?' by a CONTRIBUTOR of type User on 2020-09-15T19:40:28Z\n'I've never used complex numbers in practice so I'm far from familiar with the conventions surrounding how they are handled numerically. I'm the one who added the `assume_real` argument here since I wasn't quite sure what the impact of the assumptions of the original implementation were. I opted to play it safe and check with @shoyer later but I never got that far with that first PR.\r\n\r\nAll that to say that it's very possible that we could just revert `_vdot_tree` to the original one without the `assume_real` argument. Someone would just have to validate that it makes sense.' by a CONTRIBUTOR of type User on 2020-09-15T19:51:08Z\n'Getting rid of assume_real makes sense to me, yeah (in normal array world, I would usually write x = np.abs(np.vdot(a, b)) if I wanted a real number).' by a CONTRIBUTOR of type User on 2020-09-15T20:14:50Z\n'_dot has the precision explicitly specified, and I didn't want to keep typing it out' by a CONTRIBUTOR of type User on 2020-09-15T20:16:45Z\n'Ah, right, I forgot about that. Good catch!' by a CONTRIBUTOR of type User on 2020-09-15T20:57:23Z\n'Does this change affect the CG implementation? I believe it used the `assume_real=True` branch. Would the CG calls to `_vdot_tree` need to be wrapped with `jnp.abs` as well?' by a CONTRIBUTOR of type User on 2020-09-15T21:03:43Z\n'Well, the tests seem to still pass, but I'll take a look' by a CONTRIBUTOR of type User on 2020-09-15T21:06:12Z\n'Yes, we definitely want to still use `_vdot_real_part` for CG. Otherwise we are doing a bunch of redundant computation for the imaginary part of the solution, even though we know a priori that it is zero.' by a COLLABORATOR of type User on 2020-09-15T21:30:26Z\n'OK, fair enough. I think the assume_real is a little confusing though, so I made this two different functions.' by a CONTRIBUTOR of type User on 2020-09-15T22:27:10Z\n'This is the same as z^H M z = conj(z)^T M z, which is the natural inner product for complex-valued vector spaces.' by a CONTRIBUTOR of type User on 2020-09-25T19:16:51Z\n'You only really need the second iteration if the vectors are nearly linearly dependent. A robust test is to only do the second iteration if ||r||  < 1/sqrt(2) ||x||.' by a CONTRIBUTOR of type User on 2020-09-25T19:18:56Z\n'Why is the highest precision needed here? The preconditioner is often an approximation of the inverse of A anyway.' by a CONTRIBUTOR of type User on 2020-09-25T19:44:08Z\n'Ah, I see this is just for testing. Fair enough.' by a CONTRIBUTOR of type User on 2020-09-25T19:46:58Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 30,
        "num_review_comments": 30,
        "reason_for_closure": "Implemented superseded by NumPy :- PR implemented feature now superseded by dependency update."
    },
    {
        "summary": "Pull Request '4117' titled 'Support overriding implementations of JAX functions within a scope' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-08-21T06:48:48Z, and was closed at 2021-02-26T17:39:25Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'The idea here is to support overriding the implementation of higher order JAX\r\nfunctions within a limited scope, for libraries such as Haiku and Flax that\r\nimplement their own versions of this functions that support mutation.\r\n\r\nUsage example:\r\n\r\n    with jax.override_context({'lax.scan': my_scan}):\r\n      # all calls to lax.scan() are replaced by my_scan()\r\n      ...\r\n\r\nUltimately, it would be great to replace this with a unified interface for\r\nmutable state in JAX, but this could be a convenient temporary measure.'\nPR has comments:\n'@shoyer This feature could potentially allow a large speedup of `jnp` functions running on the NumPy backend: `numpy_eval()(jnp.<some_fun>)` could be mapped directly to `np.<some_fun>`, instead of tracing `jnp.<some_fun>` and calling the NumPy implementations of the encountered lax primitives. It might make https://github.com/google/jax/issues/3893 fast out-of-the-box, removing the need to keep optimized NumPy implementations of shape rules.' by a CONTRIBUTOR of type User on 2020-08-21T11:16:05Z\n'@JuliusKunze I would be a little reluctant to support directly overriding `jax.numpy` functions. JAX works so well in part because transformations are written at the LAX primitive level, which is a much more uniform and rationalized interface than `jax.numpy`. That's the main reason why I suggested this interface as a temporary measure -- it would be much better (if possible) to figure out a uniform interface for threading variables holding mutable state through transformations.' by a COLLABORATOR of type User on 2020-08-21T21:25:16Z\n'Whoa, this is awesome!!  This would resolve my problem in Haiku (linked above) and it seems very versatile too.  What's the future of this pull request?' by a CONTRIBUTOR of type User on 2021-02-05T11:18:10Z\n'I think I support this. We haven't made any progress on a unified state mechanism since the last time this was proposed, and it's better than external libraries directly monkeypatching JAX. Note that most libraries that provide state-aware versions of functions like `jit`, `scan`, etc. add additional knobs/functionality to those functions (and not necessarily in a mutually compatible way), so this feature doesn't solve all aspects of the problem.' by a CONTRIBUTOR of type User on 2021-02-07T07:12:46Z\n'Since drafting this, we actually have use-case for this in our own application code where `haiku.scan` wasn't quite right and we wanted our own version.\r\n\r\nMy one thought is that we should probably make it clearer that it's experimental. If we don't think it's worth making a new experimental module, perhaps we should just include \"experimental\" in the name, e.g., `jax.experimental_override_context`? Or perhaps it can just live in the top-level `jax.experimental` namespace along with the `enable_x64`/`disable_x64` context managers.' by a COLLABORATOR of type User on 2021-02-07T19:31:25Z\n'This is ready for review now.' by a COLLABORATOR of type User on 2021-02-07T22:43:03Z\n'Wow, I appreciate how fast the Jax team responds to issues!\r\n\r\nI'm just curious, would it be worth adding the `eval_shape` override to the list?  Haiku seems to provide an override.' by a CONTRIBUTOR of type User on 2021-02-08T01:53:58Z\n'Placing `override_context` in `experimental` sounds good to me.' by a MEMBER of type User on 2021-02-08T03:50:35Z\n'> I'm just curious, would it be worth adding the `eval_shape` override to the list? Haiku seems to provide an override.\r\n\r\nYes, absolutely!\r\n\r\nI think I got them all, but let me know if you notice any others that are missing. (It's also easy to extend this in the future, of course.)' by a COLLABORATOR of type User on 2021-02-08T05:50:00Z\n'Tests are passing -- is anyone from the JAX team up for a review?' by a COLLABORATOR of type User on 2021-02-10T00:38:03Z\n'After discussion with the whole JAX team, we decided _not_ to go further with this PR at this time. It solves some real problems (writing library code that uses higher order functions that wants to be compatible with stateful transformations), but:\r\n\r\n1. Clearly isn't the ultimate solution (which would be some universal system for managing state _inside_ JAX)\r\n2. It is still something that is going to require application specific choices. For example, if you `scan` over a neural net layer, should the weights be scanned over or not?\r\n\r\nFor now, the recommendation is to write your own, library specific implementations of higher order functions like `scan` and your own scope for overrides, if necessary.' by a COLLABORATOR of type User on 2021-02-26T17:39:25Z\n\nPR has review comments:\n'type: this->these' by a COLLABORATOR of type User on 2020-08-31T07:45:30Z\n'Is this nested `override_context` need for the case when the override wants to call the original function? ' by a COLLABORATOR of type User on 2020-08-31T07:51:49Z\n'Maybe it is worth documenting here that all cals to `lax.scan` inside `my_scan` will use the version of `lax.scan` before the override.  ' by a COLLABORATOR of type User on 2020-08-31T07:52:36Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 11,
        "num_review_comments": 3,
        "reason_for_closure": "After discussion with the JAX team, the PR was deemed not the right approach at this time."
    },
    {
        "summary": "Pull Request '4008' titled 'fix custom_jvp/vjp closure issues, and nondiff_argnums too!' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-08-10T16:19:03Z, and was closed at 2020-10-21T00:55:55Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing, JEP - JAX enhancement proposal. \nIt has a body of 'fixes #4566, fixes #4521, fixes #2912, fixes #3822, fixes #4173, fixes #2520, fixes #3808 \r\n\r\nIf you survey those issues, you'll come to the conclusion that `jax.custom_jvp` and `jax.custom_vjp` don't work with closing over JAX Tracers, and moreover that `nondiff_argnums` can be just as problematic. Indeed, `nondiff_argnums` was effectively implemented in terms of lexical closure. The original plan was not to make `custom_jvp`/`custom_vjp` work with lexical closure, but I never documented that, and the error messages were terrible!\r\n\r\nThis PR makes `custom_jvp` and `custom_vjp` work with closed-over Tracers. Woo! That is, now `custom_jvp` and `custom_vjp` functions and rules can close over Tracers to our hearts' content. For all non-autodiff transformations, things will Just Work. For autodiff transformations, we'll get a clear error message about why we can't differentiate with respect to values over which a custom_jvp or custom_vjp closes:\r\n> Detected differentiation of a custom_jvp function with respect to a closed-over value. That isn't supported because the custom JVP rule only specifies how to differentiate the custom_jvp function with respect to explicit input parameters. Try passing the closed-over value into the custom_jvp function as an argument, and adapting the custom_jvp rule.\r\n\r\nThis PR accomplishes that goal by following through on the original _ansatz_ of `custom_jvp` and `custom_vjp`, namely to make them work like `core.call`. That is, we do all the `core.process_env_traces` stuff properly.\r\n\r\nTODO:\r\n- [x] make pre-omnistaging version work again\r\n- [x] fix jax2tf sublevels interaction\r\n- [x] fix all google internal users\r\n- [x] update custom derivatives notebook with new custom_vjp nondiff_argnums advice\r\n- [x] write a pr message'\nPR has comments:\n'Making the fixes from @apaszke in #4664.' by a COLLABORATOR of type User on 2020-10-21T00:52:42Z\n'For posterity: this was merged, as 4a20eea, not sure why copybara got confused.' by a COLLABORATOR of type User on 2020-10-21T00:58:07Z\n'Woohoo, this is huge!' by a CONTRIBUTOR of type User on 2020-10-21T19:01:39Z\n\nPR has review comments:\n'This is missing the value of `nondiff_argnums`?' by a COLLABORATOR of type User on 2020-10-19T09:28:32Z\n'There's interesting point here: when an argument can sometimes be a tracer, but sometimes will just be a constant, and you're not actually tracing your code, you might want to query whether returning `None` is ok. The alternative is to always put in the work to compute the value, even though it might get thrown away later. But idk how important eager mode is for JAX, so it might not matter too much (it definitely did in PyTorch) 🤷 ' by a COLLABORATOR of type User on 2020-10-19T09:34:23Z\n'Isn't that quadratic in the number of arguments? The previous implementation is linear. Maybe make `nondiff_argnums` a set before this line?' by a COLLABORATOR of type User on 2020-10-19T09:37:05Z\n'Thanks! I will fix.' by a COLLABORATOR of type User on 2020-10-20T23:21:10Z\n'Hmm, I may not be following: returning None is always okay here, as it just is shorthand for \"zeros like the input\". That is, it works regardless of whether the corresponding input was a tracer, and works efficiently both in eager mode and in jit mode. Can you say more?' by a COLLABORATOR of type User on 2020-10-20T23:22:46Z\n'It is but I thought it wouldn't matter :) I guess some functions have lots of arguments! I'll fix this.' by a COLLABORATOR of type User on 2020-10-20T23:23:20Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 6,
        "reason_for_closure": "Inconsistent behavior fixed: PR aligns JAX with NumPy by disallowing complex dtypes in `jnp.arctan2`."
    },
    {
        "summary": "Pull Request '3868' titled 'Make PyTreeDef pickleable for Multiprocessing' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-07-27T03:39:23Z, and was closed at 2023-03-30T21:11:12Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'To enable Jax Multiprocessing, it's necessary to pickle the treedefs (ex: Haiku params) That's supported by pybind (link below), but not implemented on the PyTreeDef class.\r\n\r\nCan you take a look? It's the first C++ I've written in my life\r\n\r\nDoes traversal_ fully encode the tree, or is there other state? \r\n\r\n2 new public methods on PyTreeDef to get/set traversal_ (not sure about type casts)\r\n```\r\n        py::tuple GetTraversal() {return py::make_tuple(traversal_); } \\\\ <-- do we need to convert each Node?\r\n        void SetTraversal(py::tuple t) { traversal_ = t.cast<std::vector<Node>>(); }\r\n```\r\nThen at the bottom in PYBIND11_MODULE, add pickling functions `__getstate__` and `__setstate__` \r\n```\r\n            // make PyTreeDef pickleable to enable use with multiprocessing\r\n            // https://pybind11.readthedocs.io/en/stable/advanced/classes.html#pickling-support\r\n            // function 1: __getstate__ - return a tuple which fully encodes the state of the object\r\n            // for PyTreeDef, the traversal_ vector of node structs fully encodes the tree (I think?)\r\n            // function 2: __setstate__ - build a new PyTreeDef from a tuple with a traversal_\r\n            .def(py::pickle(\r\n                     [](const PyTreeDef &p) { return p.GetTraversal(); },\r\n                     [](py::tuple t) {\r\n                         PyTreeDef p;\r\n                         p.SetTraversal(t);\r\n                         return p;\r\n                     }););\r\n```'\nPR has comments:\n'```\r\nimport pickle\r\n\r\n  @parameterized.parameters(*TREES)\r\n  def test_pickles(self, inputs):\r\n    xs, tree = tree_util.tree_flatten(inputs)\r\n    pickled_tree = pickle.dumps(tree)\r\n    unpickled_tree = pickle.loads(pickled_tree)\r\n    result = tree_util.tree_unflatten(unpickled_tree, xs)\r\n    self.assertEqual(result, inputs)\r\n```' by a CONTRIBUTOR of type User on 2020-07-27T06:50:02Z\n'Looks like this was fixed a while back. Test: https://github.com/google/jax/blob/31eeaed913a778525a51117a36be8daba4bacf47/tests/tree_util_test.py#L410\r\n\r\nThanks for the PR!' by a COLLABORATOR of type User on 2023-03-30T21:11:12Z\n\nPR has review comments:\n'I don't think this is going to work as is, because Python won't know how to serialize a `Node` object; it's not something exposed via pybind11.\r\n\r\nIn addition I'm not sure that I necessarily want to rely on the representation of the PyTreeDef object being backwards and forwards compatible. e.g., right the representation is a post-order traversal of the tree, but we might want to change that representation in the future.\r\n\r\nI'm wondering if we could find another alternative. For example, another encoding of a PyTreeDef would *be* an instance of the original pytree: you can recover the PyTreeDef from the pytree by calling `flatten`. Perhaps we could use that idea as the basis of a serialization (e.g., put some sentinel value instead of the leaves and serialize that?)' by a COLLABORATOR of type User on 2020-08-03T16:16:31Z\n'yes i like that idea, how would you add that... just unflatten / flatten to pickle / unpickle? \r\n\r\none issue is, the pytreedef would need data in order to be unflattened into a pytree to pickle. potentially it would be possible to just put shapedarrays or placeholders there? ' by a CONTRIBUTOR of type User on 2020-08-05T16:19:15Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Inconclusive :- PR closed as JAX supersedes NumPy functionality and shouldn't deprecate features just because NumPy does."
    },
    {
        "summary": "Pull Request '3856' titled 'Add erfcx' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-07-24T20:28:40Z, and was closed at 2020-07-24T22:02:51Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Closes https://github.com/google/jax/issues/1987.'\nPR has comments:\n'So this is a reasonable approach, but I think the reason that `scipy.special.erfcx` exists is because the straightforward way to compute it leads to overflow issues in the square exponential:\r\n```python\r\nimport numpy as np\r\nfrom scipy import special\r\n\r\ndef erfcx(x):\r\n  return np.exp(x ** 2) * special.erfc(x)\r\n\r\nprint(erfcx(30), special.erfcx(30))\r\n# nan 0.018795888861416754\r\n```\r\nI think that a jax implementation should use an approach that prevents these kinds of overflows. #1987 has some discussion of some possible approaches.' by a COLLABORATOR of type User on 2020-07-24T21:43:36Z\n'> So this is a reasonable approach, but I think the reason that `scipy.special.erfcx` exists is because the straightforward way to compute it leads to overflow issues in the square exponential:\r\n> \r\n> ```python\r\n> import numpy as np\r\n> from scipy import special\r\n> \r\n> def erfcx(x):\r\n>   return np.exp(x ** 2) * special.erfc(x)\r\n> \r\n> print(erfcx(30), special.erfcx(30))\r\n> # nan 0.018795888861416754\r\n> ```\r\n> \r\n> I think that a jax implementation should use an approach that prevents these kinds of overflows. #1987 has some discussion of some possible approaches.\r\n\r\nI just miss the discussion there. Thanks for reminder! I'll update it soon.' by a CONTRIBUTOR of type User on 2020-07-24T21:51:37Z\n'@jakevdp Other than the link in the issue, the scipy's implementation seems more robust, but requires a hardcoded expansion table. Do you think it's better in order for numerical stability?\r\n\r\nhttps://github.com/scipy/scipy/blob/master/scipy/special/Faddeeva.cc#L872-L1297' by a CONTRIBUTOR of type User on 2020-07-24T22:16:35Z\n\nPR has review comments:\n'For small numbers, this will be *very* close to erfc. I'd try with `jtu.rand_positive` or even `jtu.rand_default` (though for larger negative numbers, the output diverges pretty quickly)' by a COLLABORATOR of type User on 2020-07-24T20:43:47Z\n'Sure, let me try it first. Thanks!' by a CONTRIBUTOR of type User on 2020-07-24T21:02:02Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 2,
        "reason_for_closure": "Here are some concise reasons for closure based on the provided examples:\n\n* **Inconsistent with NumPy:** Breaks JAX's superset functionality of NumPy.\n* **Functionality removed:** Deprecates existing, working behavior.\n* **Bug fixed upstream:** CUDA update resolved the underlying issue.\n* **Issue resolved differently:** Deviceless AOT works; PR redundant.\n* **Misunderstanding:** Author misinterpreted reviewer feedback.\n* **Exposes internals:** Error message revealed private APIs.\n* **No clear benefit:** Insufficient justification for change. \n* **Alternative solution exists:** Problem addressed by refactoring.\n* **Low impact/priority:**  Documented in changelog; sufficient for now."
    },
    {
        "summary": "Pull Request '3837' titled 'add make_hlo to api for printing XLA HLO' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-07-23T17:20:25Z, and was closed at 2021-11-29T15:28:53Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This adds a function much like make_jaxpr that makes it easy to print the unoptimized JAX-emitted HLO for a jitted function, as well as the XLA-compiled and optimized HLO for the function.'\nPR has comments:\n'It would be great to have an API to return the compiled/optimized hlo module as well (I think the computation object rather than the text is more useful since usually I want to plot this with graphviz), a few times I've found myself wondering \"what will xla do if i do x\" and have to dig out a snippet that (ab)uses currently private APIs to look at the optimized version.' by a COLLABORATOR of type User on 2020-07-23T17:32:45Z\n'@tomhennigan \"API to return the compiled/optimized hlo module as well ... plot this with graphviz\".  For this to work some more things underneath need to get plumbed through to JAX.  \r\n\r\nAs far as I could tell you can't get anything but the hlo_text from the compiled function object, unlike the uncompiled case where you can get the dot graph.  If I'm mistaken and you know the incantation to actually do that, I'd love to know what it is!' by a COLLABORATOR of type User on 2020-07-23T17:37:04Z\n'It'd be nice to future-proof the API a little. If it weren't for the `print_opts` that might have to be passed to `module.to_string`, I'd say just return `module` and let the caller turn it into a string. That way we can extend the functionality in the future, e.g. plumbing more functionality onto the `module` object, without changing the type of this `make_hlo` function. WDYT? Ideas?\r\n\r\n(If we kept this function returning a string, I'd suggest renaming it to be more descriptive in that regard, like `hlo_string` rather than `make_hlo` perhaps.)' by a COLLABORATOR of type User on 2020-07-24T02:25:59Z\n'\r\n> @tomhennigan \"API to return the compiled/optimized hlo module as well ... plot this with graphviz\". For this to work some more things underneath need to get plumbed through to JAX.\r\n> \r\n> As far as I could tell you can't get anything but the hlo_text from the compiled function object, unlike the uncompiled case where you can get the dot graph. If I'm mistaken and you know the incantation to actually do that, I'd love to know what it is!\r\n\r\nI didn't plumb through many methods on the `HloModule` object. We could add more if you have a use case.\r\n\r\nFor dot graphs specifically, try the following free function:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8e9f3196fd8841de83bd6a622df696ea191d1d78/tensorflow/compiler/xla/python/xla.cc#L814' by a COLLABORATOR of type User on 2020-08-03T16:49:05Z\n'Closing in favor of the newer AOT compilation APIs.' by a COLLABORATOR of type User on 2021-11-29T15:28:53Z\n\nPR has review comments:\n'Why not extend `xla_computation` with more arguments instead of wrapping it?' by a CONTRIBUTOR of type User on 2020-07-23T17:32:07Z\n'well at the moment because people just asked I submit this one ;)  - the other issue is that the properties available on the compiled modules last I checked differed quite a lot from the uncompiled ones, so it's not so easy to offer a unified top-level api to both.  I'm not sure what's missing in the bindings underneath to get e.g. the dot-graph for the compiled version, but last time I checked you couldn't fetch such things...' by a COLLABORATOR of type User on 2020-07-23T17:39:00Z\n'> the other issue is that the properties available on the compiled modules last I checked differed quite a lot from the uncompiled ones\r\n\r\nSorry I'm a bit confused, don't they both return strings?\r\n\r\nE.g. why not add a bool `optimize` argument to `xla_computation`?' by a CONTRIBUTOR of type User on 2020-07-23T17:43:29Z\n'No, `xla_computation` returns a Python-wrapped `XlaComputation` which has the following API:\r\nhttps://github.com/tensorflow/tensorflow/blob/1bc83bae4f28f7576465f0d23a2ad9a700796198/tensorflow/compiler/xla/python/xla.cc#L736\r\n\r\nThe compiled and uncompiled objects are (naturally) different because one is an `Executable`, the other is an `XlaComputation`.\r\n\r\nNote there are more things you can do with an `XlaComputation`, e.g. compile it, serialize it to a protobuf, build a dot graph, etc.' by a COLLABORATOR of type User on 2020-07-23T17:55:05Z\n'It would definitely be nice to be able to get a dot graph from the compiled HLO the way the profiler does!' by a CONTRIBUTOR of type User on 2020-07-24T01:45:52Z\n'Should this be named `make_hlo_text` instead? ' by a COLLABORATOR of type User on 2020-07-24T09:00:11Z\n'Right now we also have `jax.xla_computation(fun)(args).as_hlo_text()`. I understand that the new function returns the same thing if `optimize = False`?  ' by a COLLABORATOR of type User on 2020-07-24T09:03:44Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 7,
        "reason_for_closure": "Closed as infeasible/incorrect: PR attempted to remove functionality that was deemed valuable."
    },
    {
        "summary": "Pull Request '3821' titled 'DeviceArray.__iter__ returns DeviceArrays, without host sync' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-07-22T19:11:09Z, and was closed at 2021-10-16T00:47:57Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'cross-ref #1583 #3330 \r\n\r\nThe current (before this PR) implementation of `DeviceArray.__iter__` fom #965 pulls the array back to the host and does the slicing on the resulting NumPy array, effectively:\r\n\r\n```python\r\n# current implementation\r\nclass DeviceArray:\r\n  def __iter__(self):\r\n    return iter(self._value)  # self._value is the cached ndarray version of the array\r\n```\r\n\r\nThere are three issues we want to address here:\r\n1. `list(device_array)` shouldn't return a list of `numpy.ndaray`s, and instead should return a list of `DeviceArray`s, regardless of how those are computed (this issue came up in #1583 and in some JAX user chats recently);\r\n2. `key, subkey = random.split(key)` in op-by-op mode shouldn't incur device synchronization, but it does curently because the we effectively call `random.split(key).__iter__()` (this issue came up in #3330 and in JAX chats);\r\n3. from #965, we don't want expressions like `list(device_array)` or `list(jnp.arange(10000))` to be slow to evaluate, where before #965 there could be a several-second compilation time and relatively slow (~100ms) execution time.\r\n\r\nTo address (1) we could either keep the bounce via NumPy and add some device_puts, or keep all the operations on the device. To address (2) we want to keep all the operations on the device. Currently that means we need to compile and execute XLA computations (rather than just doing something in the runtime).\r\n\r\nTo address (3), i.e. to keep things performant, most importantly we don't want to incur big compilation times, which basically means we don't want to compile computations with large output arity. We also don't want to dispatch lots of XLA computations, to keep execution time reasonable. We can balance the two by chunking things, so that for chunk size C and array size N we end up compiling O(1) computations, none having output arity more than C, and dispatching about 2 N / C computations (for each chunk, one to slice it out of the original array and one to explode it).\r\n\r\nI timed the current and new implementation this way:\r\n\r\n```python\r\nimport time\r\nimport jax.numpy as jnp\r\nfrom jax.interpreters import xla\r\n\r\ndef clear_caches():\r\n  xla.xla_primitive_callable.cache_clear()\r\n  xla._xla_callable.cache_clear()\r\n\r\ndef time_things(x):\r\n  # time to grab the first element, nothing compiled\r\n  times = []\r\n  for _ in range(100):\r\n    clear_caches()\r\n    tic = time.time()\r\n    next(iter(x))\r\n    times.append(time.time() - tic)\r\n  print(f\"time to grab first element, uncompiled: {sum(times) / len(times) * 1000} ms\")\r\n\r\n  # time to grab first element, compiled\r\n  times = []\r\n  for _ in range(100):\r\n    tic = time.time()\r\n    next(iter(x))\r\n    times.append(time.time() - tic)\r\n  print(f\"time to grab first element, compiled: {sum(times) / len(times) * 1000} ms\")\r\n\r\n  # time to grab the whole thing, nothing compiled\r\n  times = []\r\n  for _ in range(100):\r\n    clear_caches()\r\n    tic = time.time()\r\n    list(x)\r\n    times.append(time.time() - tic)\r\n  print(f\"total time to grab all elements, uncompiled: {sum(times) / len(times) * 1000} ms\")\r\n  print(f\"average time to grab each element, uncompiled: {sum(times) / len(times) / x.shape[0] * 1000} ms\")\r\n\r\n  # time to grab the whole thing, compiled\r\n  times = []\r\n  for _ in range(100):\r\n    tic = time.time()\r\n    list(x)\r\n    times.append(time.time() - tic)\r\n  print(f\"total time to grab all elements, compiled: {sum(times) / len(times) * 1000} ms\")\r\n  print(f\"average time to grab each element, compiled: {sum(times) / len(times) / x.shape[0] * 1000} ms\")\r\n\r\nsmall = jnp.arange(10) + 1\r\nbig = jnp.arange(10000) + 1\r\n```\r\n\r\nNotice we are *not* calling `block_until_ready` and instead just looking at dispatch times. The numbers that come out are pretty noisy.\r\n\r\nFor the *current* implementation, on a TPUv3 internal colab we see this:\r\n\r\n```\r\ntime_things(small)\r\ntime to grab first element, uncompiled: 0.001392364501953125 ms\r\ntime to grab first element, compiled: 0.001277923583984375 ms\r\ntotal time to grab all elements, uncompiled: 0.003528594970703125 ms\r\naverage time to grab each element, uncompiled: 0.0003528594970703125 ms\r\ntotal time to grab all elements, compiled: 0.0027894973754882812 ms\r\naverage time to grab each element, compiled: 0.0002789497375488281 ms\r\n\r\ntime_things(big)\r\ntime to grab first element, uncompiled: 0.0012826919555664062 ms\r\ntime to grab first element, compiled: 0.0011134147644042969 ms\r\ntotal time to grab all elements, uncompiled: 0.7117271423339844 ms\r\naverage time to grab each element, uncompiled: 7.117271423339843e-05 ms\r\ntotal time to grab all elements, compiled: 0.7156133651733398 ms\r\naverage time to grab each element, compiled: 7.156133651733398e-05 ms\r\n```\r\n\r\nFor the *new* implementation with chunk_size=100, on a TPUv3 internal colab we see this:\r\n\r\n```\r\ntime_things(small)\r\ntime to grab first element, uncompiled: 12.47842788696289 ms\r\ntime to grab first element, compiled: 0.1662755012512207 ms\r\ntotal time to grab all elements, uncompiled: 12.541697025299072 ms\r\naverage time to grab each element, uncompiled: 1.2541697025299074 ms\r\ntotal time to grab all elements, compiled: 0.16188859939575195 ms\r\naverage time to grab each element, compiled: 0.016188859939575195 ms\r\n\r\ntime_things(big)\r\ntime to grab first element, uncompiled: 88.96037578582764 ms\r\ntime to grab first element, compiled: 0.809943675994873 ms\r\ntotal time to grab all elements, uncompiled: 182.55109786987305 ms\r\naverage time to grab each element, uncompiled: 0.018255109786987307 ms\r\ntotal time to grab all elements, compiled: 80.00826835632324 ms\r\naverage time to grab each element, compiled: 0.008000826835632323 ms\r\n```\r\n\r\nI _think_ the ballparks here seem acceptable, and so meet desideratum (3) while avoiding the host sync per (2). Comparing to #965, we avoid any multi-second compile times even though we end up paying 80ms to compute `list(big)` rather than <1ms. I suspect these times will get better when we improve `jit` dispatch time, since a significant fraction of the trace is spent on Python overhead (anything that's not an Execute bar):\r\n\r\n![image](https://user-images.githubusercontent.com/1458824/88247615-38789580-cc53-11ea-90cc-bbdb285624bb.png)\r\n\r\nHere's one other benchmark:\r\n\r\n```python\r\nfrom jax import random \r\nimport time\r\n\r\ndef time_things2():\r\n  key = random.PRNGKey(0)\r\n  key, _ = random.split(key)\r\n  \r\n  tic = time.time()\r\n  for _ in range(1000):\r\n    key, subkey = random.split(key)\r\n  toc = time.time()\r\n  print(f\"{(toc - tic)} ms\")\r\n```\r\n\r\nWith the *current* implementation, on TPU we get:\r\n\r\n```\r\ntime_things2()\r\n0.27765631675720215 ms\r\n```\r\n\r\nWith the *new* implementation, on TPU we get:\r\n\r\n```\r\ntime_things2()\r\n0.20100140571594238 ms\r\n```\r\n\r\nSo perhaps we're saving something from avoiding the sync here, even though there's no other work going on here.\r\n\r\nfixes #1583'\nPR has comments:\n'I think this is worth merging?' by a CONTRIBUTOR of type User on 2020-09-17T07:04:56Z\n'I'm trying to revive this... one issue is that the conditional implementation of `__hash__` here falls afoul of the change in #7347\r\n\r\nI don't think it's possible to have 0-dimensional arrays be hashable without re-breaking the issue fixed in #7347.' by a COLLABORATOR of type User on 2021-08-25T21:53:11Z\n'Closing in favor of #8043 ' by a COLLABORATOR of type User on 2021-10-16T00:47:57Z\n\nPR has review comments:\n'Under what conditions is device `None`?' by a COLLABORATOR of type User on 2020-07-22T20:37:11Z\n'Good question! It's when the DeviceArray isn't backed by a device buffer at all but instead is a [lazy constant](https://github.com/google/jax/pull/1668), particularly [one of these](https://github.com/google/jax/blob/fddb28d8202dd1f1fc4cc78b44bccc1c82812746/jax/interpreters/xla.py#L1126-L1130), which also isn't sticky on a particular device (see #1916 for a description of what \"sticky\" means).\r\n\r\nLuckily we shouldn't have to worry about that complexity much longer: after landing #3370 I hope to remove the lazy sublanguage completely.' by a COLLABORATOR of type User on 2020-07-22T21:09:47Z\n'If this still returns NumPy arrays, I suspect that might be undesirable because of, e.g., different promotion semantics.\r\n\r\nHowever, it might make sense in the short term until some of the overheads improve.' by a COLLABORATOR of type User on 2020-07-29T20:33:24Z\n'Yeah, good point. We should revise this case so that it returns CPU DeviceArrays, maybe.' by a COLLABORATOR of type User on 2020-08-10T23:43:14Z\n'On CPU, with `return iter(self._value)`, running the big timing script in the OP using the `big` array, we get:\r\n\r\n```\r\ntime to grab first element, uncompiled: 0.002505779266357422 ms\r\ntime to grab first element, compiled: 0.0018811225891113281 ms\r\ntotal time to grab all elements, uncompiled: 0.696418285369873 ms\r\naverage time to grab each element, uncompiled: 6.96418285369873e-05 ms\r\ntotal time to grab all elements, compiled: 0.6600356101989746 ms\r\naverage time to grab each element, compiled: 6.600356101989746e-05 ms\r\n```\r\n\r\nIf we instead keep doing the slicing in NumPy but device_put each result, i.e. we do something like this:\r\n\r\n```python\r\n      device = self.device_buffer.device()\r\n      if device is None or device.platform == 'cpu':\r\n        # do the slicing in NumPy for better performance\r\n        device = device or jax.devices('cpu')\r\n        aval = ShapedArray(self.aval.shape[1:], self.aval.dtype,\r\n                           self.aval.weak_type)\r\n        lexpr = lazy.array(aval.shape)\r\n        return (make_device_array(aval, device, lexpr, *device_put(x, device))\r\n                for x in self._value)\r\n```\r\n\r\nThen we get this timing:\r\n```\r\ntime to grab first element, uncompiled: 0.01703500747680664 ms\r\ntime to grab first element, compiled: 0.015168190002441406 ms\r\ntotal time to grab all elements, uncompiled: 79.03305768966675 ms\r\naverage time to grab each element, uncompiled: 0.007903305768966674 ms\r\ntotal time to grab all elements, compiled: 79.05462265014648 ms\r\naverage time to grab each element, compiled: 0.007905462265014648 ms\r\n```\r\n\r\nSo a 100x slowdown to return DeviceArrays rather than ndarrays on CPU. Perhaps we'll be able to speed that up soon, but I wanted to bounce this off you anyway. WDYT?' by a COLLABORATOR of type User on 2020-11-17T00:05:38Z\n'Updated timings, being slightly less dumb:\r\n\r\n```\r\ntime to grab first element, uncompiled: 0.01026153564453125 ms\r\ntime to grab first element, compiled: 0.009059906005859375 ms\r\ntotal time to grab all elements, uncompiled: 42.66105651855469 ms\r\naverage time to grab each element, uncompiled: 0.004266105651855468 ms\r\ntotal time to grab all elements, compiled: 42.526516914367676 ms\r\naverage time to grab each element, compiled: 0.004252651691436768 ms\r\n```' by a COLLABORATOR of type User on 2020-12-02T01:28:28Z\n'New times, after jit dispatch time upgrades in jaxlib==0.1.65 (remember these are noisy!).\r\n\r\nWith special treatment of CPU (which is part of what this comment thread was about):\r\n\r\n```\r\ntime to grab first element, uncompiled: 0.013675689697265625 ms\r\ntime to grab first element, compiled: 0.012180805206298828 ms\r\ntotal time to grab all elements, uncompiled: 38.03548336029053 ms\r\naverage time to grab each element, uncompiled: 0.003803548336029053 ms\r\ntotal time to grab all elements, compiled: 37.87814140319824 ms\r\naverage time to grab each element, compiled: 0.0037878141403198244 ms\r\n```\r\n\r\nWithout special treatment of CPU:\r\n\r\n```\r\ntime to grab first element, uncompiled: 0.013167858123779297 ms\r\ntime to grab first element, compiled: 0.012054443359375 ms\r\ntotal time to grab all elements, uncompiled: 37.4117112159729 ms\r\naverage time to grab each element, uncompiled: 0.00374117112159729 ms\r\ntotal time to grab all elements, compiled: 37.27412462234497 ms\r\naverage time to grab each element, compiled: 0.0037274124622344975 ms\r\n```\r\n\r\nSo, no need for special treatment of CPU now!' by a COLLABORATOR of type User on 2021-04-10T01:58:49Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 7,
        "reason_for_closure": "Closed as infeasible: the PR deprecates valid functionality and breaks existing use cases."
    },
    {
        "summary": "Pull Request '3798' titled 'Add sort_complex' was authored by a User, who is associated as a NONE. \nIt was created at 2020-07-19T23:19:26Z, and was closed at 2021-05-19T17:49:44Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Hi, \r\n\r\nThis is an attempt to implement a `sort_complex` routine from `good_first_issue` category.'\nPR has comments:\n'@SIben FYI' by a COLLABORATOR of type User on 2020-07-21T07:45:14Z\n'`sort_complex` was merged in #3870' by a COLLABORATOR of type User on 2020-07-27T21:40:53Z\n\nPR has review comments:\n'Actually, on reflection, as far as I can tell NumPy's API doesn't have any of the additional arguments here, only the array:\r\nhttps://numpy.org/doc/stable/reference/generated/numpy.sort_complex.html\r\n\r\nWe should not deviate from NumPy without a good reason.\r\n' by a COLLABORATOR of type User on 2020-07-20T12:57:19Z\n'Thank you for your comments, I appreciate it. I will fix the formatting, and I will contact the author of the other PR, hopefully we can join efforts on this. Thanks again!' by a NONE of type User on 2020-07-27T01:28:54Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Superseded :- Another PR (#3870) addressed the issue."
    },
    {
        "summary": "Pull Request '3796' titled 'WIP: Add bicgstab implementation' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-07-19T21:00:11Z, and was closed at 2021-02-22T04:47:17Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'I am working on the `bicgstab` implementation (using the current pytree convention in `cg`); I am most of the way there as the implementation was very similar to cg and the tests generally seem to work. This addresses one of the goals of #1531.\r\n\r\nBefore finalizing this PR, there are some points of confusion on the tests that have to do with numerical issues in iterative solvers. I would like some advice on this before proceeding.\r\n\r\n1. If you use a iterative solver, you only get the correct answer up to some tolerance. This makes gradient checking harder and different from a lot of the other more predictable behaviors in `jax`. I noticed the biggest shape in the gradient tests `*_solve` is `(2, 2)`, and when I checked larger shapes, there was an error (depending on the type, either array not equal or grad check failure)! What is the general consensus on testing with respect to gradient checks, especially for these approximate solvers?\r\n2. Mid-iteration convergence and breakdown checks may require some special implementation of loops. To truly match `scipy` for `cg` and `bicgstab` more generally, I think this will need to be implemented somehow. I have a TODO comment somewhere in this PR explaining how it would be implemented if `jit` were not a requirement.\r\n\r\nI also want to make sure that the changes I made to `cg` to avoid code duplication are kosher.'\nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F3796) for more info**.\n\n<!-- need_sender_cla -->' by a COLLABORATOR of type User on 2020-07-19T21:00:16Z\n'@googlebot I signed it!' by a CONTRIBUTOR of type User on 2020-07-19T21:47:48Z\n'CLAs look good, thanks!\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F3796) for more info**.\n\n<!-- ok -->' by a COLLABORATOR of type User on 2020-07-19T21:47:51Z\n'> * What is the general consensus on testing with respect to gradient checks, especially for these approximate solvers?\r\n\r\nIt's fine to relax error tolerances in gradient checks as long as we are still quite confident that the result is correct (up to numerical precision).\r\n\r\nFor example, I just tried switching the cg gradient test to a random 4x4 matrix. This turns up the following error:\r\n```\r\nE     AssertionError:\r\nE     Not equal to tolerance rtol=0.04, atol=0.008\r\nE\r\nE     Mismatch: 25%\r\nE     Max absolute difference: 1.0214233\r\nE     Max relative difference: 0.09353104\r\nE      x: array([-37.997494,   3.304602,  70.99634 , 318.19443 ], dtype=float32)\r\nE      y: array([-37.312508,   3.021955,  70.98675 , 317.173   ], dtype=float32)\r\n```\r\nBut really, this is close enough. So if we used these tests, I would be fine relaxing the error tolerance as needed.' by a COLLABORATOR of type User on 2020-07-20T01:04:21Z\n'Hi @shoyer I'm still fighting my code to get scipy and bicgstab to agree; they agree for many cases (especially higher numerical precision) but begin to deviate in some special cases. I've done a number of things to check what the differences are and I'm beginning to suspect that it has something to do with numerical precision becoming increasingly important after several steps (generally not after the first step). I checked scipy's implementation (and even hacked into their `bicgstab` function to print out intermediate values). Wondering what your testing procedure was so I can resolve these issues more efficiently.\r\n\r\nThe test I have started to run now is to write a numpy version of `bicgstab` and then to compare that version with the jax version to make sure things roughly agree given essentially the same code. But even this starts to disagree (with both `scipy` and `jax`); my current hypothesis is because of slight differences in implementation in methods such as `vdot` (order of addition seems to matter at times for numerical precision). \r\n\r\nHere is some example failure case output tracking `bicgstab` iterations for the `scipy` implementation and my `numpy` implementation for `float32[4, 4]`. It's worth noting that I've observed simulations in the past where `bicgstab` fails for `complex64` and not `complex128`, so precision appears to be a key factor in determining the success of `bicgstab`.\r\n```\r\nIteration 1: # this iteration looks fine\r\nr: numpy [-0.7498561 -2.9607344  5.9396653 -3.6000998] scipy [-0.74985594 -2.9607344   5.9396653  -3.6000998 ]\r\np: numpy [ 2.845481    0.93953496 -0.43305653 -1.9908385 ] scipy [ 2.845481    0.93953496 -0.43305653 -1.9908385 ]\r\nt: numpy [44.767788  -6.7145605 15.403215  21.610672 ] scipy [44.767788  -6.7145605 15.403215  21.610674 ]\r\nq: numpy [ 13.488404  15.618371 -25.598196   6.009314] scipy [ 13.488404   15.618371  -25.598196    6.0093145]\r\nshat: numpy [-0.5489147 -2.9908729  6.008803  -3.5030997] scipy [-0.5489147 -2.9908729  6.008803  -3.5030997]\r\n\r\nIteration 2: # things start to change more here but still fine\r\nr: numpy [-1.3085905 -1.9914868  4.1542206 -3.6563268] scipy [-1.3085957 -1.9914829  4.1542125 -3.656325 ]\r\np: numpy [-4.5599647 -4.1502147  6.374942  -0.8395064] scipy [-4.559961   -4.1502132   6.3749413  -0.83950925]\r\nt: numpy [ 28.715466 -17.281141  27.14217   29.973507] scipy [ 28.715366 -17.281208  27.142246  29.973562]\r\nq: numpy [ 24.778402 -29.029495  51.462784  14.110635] scipy [ 24.778421 -29.029476  51.462746  14.110645]\r\nshat: numpy [-1.8608497 -1.6591338  3.6322193 -4.232781 ] scipy [-1.8608582 -1.6591254  3.6322048 -4.2327857]\r\n\r\nIteration 3: # oh boy...\r\nr numpy [-0.00711237 -0.12015637  0.1177693  -0.10020799] scipy[-0.0025171  -0.11048866  0.0973189  -0.08428508]\r\np numpy [ 2.093349   1.931221  -1.9813652 -3.183013 ] scipy [ 2.0936446  1.9315835 -1.9819365 -3.1829684]\r\nt: numpy [ 1.0321703  -0.27053687  0.5031162   0.84241974] scipy [ 0.8828936  -0.21744165  0.4108755   0.73308885]\r\nq: numpy [  5.698114   8.345237 -17.993662  15.740916] scipy [  5.6956105   8.347048  -17.997128   15.740109 ]\r\nshat: numpy [-0.027246   -0.11487925  0.10795546 -0.11664033] scipy [-0.02184983 -0.10572734  0.08832195 -0.10033754]\r\n```\r\n\r\nIncidentally, after using more random cases than is currently recommended, I noticed `cg` failed a few scipy comparison tests, either due to similar reasons or changes I've made to the tests. I'll get back to you on exactly what those failure cases are.' by a CONTRIBUTOR of type User on 2020-07-24T22:35:20Z\n'@sunilkpai This looks close enough to me? How does the accuracy of the final solution compare?\r\n\r\nSciPy is probably always using float64 precision. It might be worth enabling x64 mode for JAX, just for the sake of comparison. This currently requires a special flags:\r\nhttps://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#Double-(64bit)-precision\r\n\r\nIf the error does not decrease when you turn on x64 mode, then perhaps we are missing something important and should be worried! In that case I would look into the early termination conditions.' by a COLLABORATOR of type User on 2020-07-24T23:47:22Z\n'@shoyer Yeah, you might be right for this case, but I think this becomes magnified as `n` gets higher (e.g. `n = 32` becomes a problem, this was just an unlucky case for `n = 4` that was easy to use as an example of what may go wrong). I will try your suggestion and see if anything changes!' by a CONTRIBUTOR of type User on 2020-07-25T00:16:51Z\n'Also, re: early termination, this case may actually be necessary when s = 0 for `bicgstab`, which is an error case generated by some of the pytree tests. If `s = 0` then `t = 0` as well, resulting in `nan` evaluation for `omega`!' by a CONTRIBUTOR of type User on 2020-07-25T00:31:41Z\n'Hi @shoyer this is still wip but I am getting closer I think thanks to your help! I have been adding your suggestions for the testing framework. Some checks between numpy and scipy are necessary (since there are differences and numerical failure cases) and therefore are now part of the tests. \r\n\r\nThe `exact` test for `cg` seems to be the culprit in the above CI failure (either due to some unintentional changes i made to the test or just the fact that it's a different matrix). There are also random failures for `bicgstab` in a more exhaustive test setting... all depends on the matrix (verified this by adding extra characters to the test name to generate new set of cases). Generally this occurs on roughly about 5% of the tests or so, sometimes the error is small (twice the rel tolerance) other times very large (orders of mag larger than tolerance).' by a CONTRIBUTOR of type User on 2020-07-25T23:33:00Z\n\nPR has review comments:\n'I thought bicgstab works on arbitrary matrices, without requiring symmetry? It would be good to clarify this :)' by a COLLABORATOR of type User on 2020-07-20T00:38:30Z\n'I think this condition is only true for `cg`, not `bicgstab`? I would expect arguments to `bicgstab` to essentially never be symmetric. As I understand it, if they were, there are better solvers to use (e.g., `cg`).' by a COLLABORATOR of type User on 2020-07-20T00:39:57Z\n'That's a copying error, you are correct!' by a CONTRIBUTOR of type User on 2020-07-20T00:39:59Z\n'We could probably do this with some cleverness (e.g., involving `lax.cond`), but I think it's actually totally fine to skip it -- at least for now. In the best case, this would save one extra function evaluation.\r\n\r\nOn GPUs, dynamic control flow is so expensive that I expect we could actually get better performance in many cases by only checking for stopping every `n` steps.' by a COLLABORATOR of type User on 2020-07-20T00:44:00Z\n'I would be in favor of testing the Poisson equation _as well_ as random matrices, but I would definitely test random matrices (with no structure) as well. This seems much more likely to turn up issues.' by a COLLABORATOR of type User on 2020-07-20T00:46:58Z\n'You have both `cg` and `bicgstab` using most of the same code. So I don't think it provides much value to add these repeated tests. I would either drop them or make the original tests parametric so they test both `cg` and `bicgstab` with the same method.' by a COLLABORATOR of type User on 2020-07-20T00:49:20Z\n'This is at least a little surprising. I wouldn't expect random 2x2 matrices to be a problem -- that suggests that maybe there's a bug in the algorithm?' by a COLLABORATOR of type User on 2020-07-20T00:52:19Z\n'I am concerned that this might be effectively dividing by zero if the algorithm converges in the first half of the step. I wonder if this could be related to your testing challenges?\r\n\r\nIf I'm understanding correctly, this might not be happening for `cg` only because demonator is also the termination condition on the loop.\r\n\r\nOne way to guard against this would be replace the denominator with something like `jnp.maximum(_vdot_tree_real(t, t), epsilon)`.' by a COLLABORATOR of type User on 2020-07-20T00:57:35Z\n'I agree, will do!' by a CONTRIBUTOR of type User on 2020-07-20T01:24:22Z\n'Yeah, I agree, I thought of this as well. I will implement that suggestion!' by a CONTRIBUTOR of type User on 2020-07-20T01:29:14Z\n'I am happy to add that in as I have seen it used in other contexts, but I haven't really seen this done in scipy's fortran revcom or any other pseudocodes. That said, I have noticed that my implementation and scipy tend to have similar breakdown scenarios (usually we add breakdown checks for whether rho or omega are zero and that is returned as part of `info`). Do you recommend we add this in anyway?' by a CONTRIBUTOR of type User on 2020-07-20T01:36:21Z\n'My general rule of thumb is to try not to innovate in the details of numerical methods. I'm not an expert on most of them, and it can be easy to get the details wrong.\r\n\r\nI would definitely suggest copying the safety/breakdown checks you notice in other implementation like SciPy rather than my ideas! Let me know if you have questions about how to implement them in JAX.' by a COLLABORATOR of type User on 2020-07-20T02:08:14Z\n'Yeah, I am surprised as well. I think having breakdown checks is perhaps important. If `scipy` returns `-10` or `-11` that means `rho = 0` or `omega = 0` respectively and maybe that is resulting in some issues. I will look into this; I do recall small matrices actually trips up even `scipy`'s `bicgstab`.' by a CONTRIBUTOR of type User on 2020-07-20T02:24:29Z\n'I think if you have a matrix that is symmetric but NOT positive definite (e.g. pos semidefinite), this would still be useful, no? Don't know how often that would be the case in practice but it's certainly a possibility.' by a CONTRIBUTOR of type User on 2020-07-21T00:38:09Z\n'OK, but either way it's definitely a different condition for `bicgstab` vs `cg`. All real values for the solution does not imply that it's symmetric.\r\n\r\nSo it seems like `isolve` should grow a `symmetric` argument, which could be set in a solver specific way. Potentially we could even expose a new `symmetric` argument on the public `bicgstab` function.' by a COLLABORATOR of type User on 2020-07-21T01:15:49Z\n'The safety/breakdown checks might require some deeper changes in jax (similar to why `info` is currently `None`). Might be a future thing to add after getting the basic `bicgstab` to work?' by a CONTRIBUTOR of type User on 2020-07-21T04:07:23Z\n'Alright, and I also wonder if there is some utility in treating `isolve` as a public function or no? I can give it an underscore in the name if not!' by a CONTRIBUTOR of type User on 2020-07-21T04:15:48Z\n'Yes, all the helper functions should definitely be marked as private.' by a COLLABORATOR of type User on 2020-07-21T05:44:56Z\n'You should definitely be able to add early breakdown without pervasive changes to JAX.\r\n\r\nThere are at least two ways to do it:\r\n\r\n1. You could nest a call to `jax.lax.cond`, either returning half-way through or doing the single GMRES step.\r\n2. You could keep track of whether early existing should happen in a boolean variable, and use `lax.select`/`jnp.where` (basically the same) to selectively update variables, e.g., compare how loopy logic for sampling from the Poisson distribution is implemented in [JAX](https://github.com/google/jax/blob/5e91965723ac16dda361435d1bfcd4dc99201537/jax/random.py#L1075-L1093) vs [NumPy](https://github.com/numpy/numpy/blob/b743bcc1c6d0ab8b26042faf5da517b69e4fbb63/numpy/random/src/distributions/distributions.c#L558-L575):\r\n\r\nThey have slightly different performance tradeoffs, but either solution should be fine here. (1) might be a little easier.' by a COLLABORATOR of type User on 2020-07-25T00:48:41Z\n'Thanks, this is helpful!' by a CONTRIBUTOR of type User on 2020-07-25T02:22:34Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 9,
        "num_review_comments": 20,
        "reason_for_closure": "Inconsistent behavior with NumPy; JAX implementation provides broader functionality."
    },
    {
        "summary": "Pull Request '3736' titled 'Curry jit, grad, value_and_grad and vmap.' was authored by a User, who is associated as a NONE. \nIt was created at 2020-07-13T18:23:45Z, and was closed at 2020-07-24T17:50:53Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Allows use as decorators with kwargs, e.g.:\r\n\r\n`@jax.jit(static_argnums=(1,))\r\n`\r\nas opposed to the current, less pythonic:\r\n\r\n`@functools.partial(jax.jit, static_argnums=(1,))\r\n`\r\nAlso updates all internal usages.'\nPR has comments:\n'The alternative to adding a toolz dependency would be to write our own helper function, e.g., a decorator that produces something like:\r\n```python\r\ndef wrapper(*args, **kwargs):\r\n  if args or 'fun' in kwargs:\r\n    return jit(*args, **kwargs)\r\n  else:\r\n    if args:\r\n      raise ValueError(\"can't use positional arguments when currying\")\r\n    return partial(jit, **kwargs)\r\n```\r\n\r\n\r\n\r\n\r\n' by a COLLABORATOR of type User on 2020-07-23T18:57:49Z\n'Thanks for proposing and prototyping this! Recording my thoughts here for posterity, though we discussed more in our chat rooms. Not everyone agrees with the reasoning or conclusion here!\r\n\r\nI don't think we should merge this. My primary concern is that there are unforeseen edge cases that will end up harming usability on net. Once we merge a new api that users start relying on, if edge cases later emerge it may be difficult or impossible to revise our decision and remove it. It's possible that there are _no edge cases at all_ to this convenience layer, but the `tools.curry` logic [looks tricky enough](https://github.com/pytoolz/toolz/blob/ea3ba0d60a33b256c8b2a7be43aff926992ffcdb/toolz/functoolz.py#L300-L335) that it's not easy for me to make that claim without deeper investigation. To be confident, I'd want to thoroughly understand the `toolz.curry` logic, and still probably brainstorm and write tests for all the evil edge cases I can think up. That would take time, and I don't think the potential upside of this convenience layer justifies that kind of time investment given our other priorities.\r\n\r\nA distant secondary concern is performance. `jit` dispatch time isn't good now, but because it's relatively simple we know how to make it fast, and we've already started working on that. I don't want to add another wrapper from an external library to that path now, since it's another potential blocker to fast dispatch.\r\n\r\nThe current solution to the issue at hand here, with `@partial(jit, ...)`, is simple and predictable. It may be unfamiliar at first, but it's easy to learn and once learned it's easy to understand and predict its behavior. Moreover, any user who wants a curried version can get it as simple as:\r\n\r\n```python\r\nimport toolz\r\nfrom jax import jit\r\n\r\njit = tools.curry(jit)\r\n```\r\n\r\nThat is, we don't need to bundle these things together for users. They can use composition instead.\r\n\r\nWhile I don't think we should merge this, I appreciate the time you put into making a concrete proposal we could debate. Without it, we wouldn't have been able to weigh alternatives! Investigating this carefully made JAX better.' by a COLLABORATOR of type User on 2020-07-24T17:50:52Z\n\nPR has review comments:\n'It seems odd that a toolz dependence would need to be added here, whereas no other jax Python dependencies need to be. Are you sure this is needed?' by a COLLABORATOR of type User on 2020-07-23T18:56:48Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 1,
        "reason_for_closure": "Inconsistent behavior :- PR tries to enforce numpy's limitations on jax."
    },
    {
        "summary": "Pull Request '3722' titled 'Adding np.sort_complex within lax_numpy.py' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-07-11T16:23:51Z, and was closed at 2020-07-27T16:11:21Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Hi @jakevdp - In accordance with our discussion on issue #2079 I have ported the np.sort_complex function into lax_numpy.py and added 2 test cases for the same over to lax_numpy_test.py\r\n\r\nPlease do let me know if this is alright or how this can be made better.'\nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F3722) for more info**.\n\n<!-- need_sender_cla -->' by a COLLABORATOR of type User on 2020-07-11T16:23:57Z\n'Sure! Let me work on this now and raise a PR over.' by a CONTRIBUTOR of type User on 2020-07-14T07:36:46Z\n'CLAs look good, thanks!\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F3722) for more info**.\n\n<!-- ok -->' by a COLLABORATOR of type User on 2020-07-18T16:26:12Z\n'Hi! I recently made a pr (#3798) with `sort_complex` and was informed that one already exists here. I was wondering if you would like to join efforts on the issue? let me know.' by a NONE of type User on 2020-07-27T01:32:59Z\n'@hawkinsp - Just realized that I forgot to pull before pushing into this branch and now, 2 commits (from lex_sort and complex_sort) have been merged here.\r\nDo you want me to close this PR and raise another one?' by a CONTRIBUTOR of type User on 2020-07-27T07:39:58Z\n'I opened another PR #3870 since this one had a lot of merge conflicts. If its alright then I'll close this Pull Request.\r\n@iliailmer - you can look into it too!' by a CONTRIBUTOR of type User on 2020-07-27T10:48:48Z\n'Thanks @Vaibhavs10 - I'll go ahead and close this one, and continue the review in #3870.' by a COLLABORATOR of type User on 2020-07-27T16:11:21Z\n\nPR has review comments:\n'`complexfloating` is already a module-level variable; no need for the additional import.' by a COLLABORATOR of type User on 2020-07-13T21:20:22Z\n'For GPU and TPU, Jax requires some special-casing of complex types. You can duplicate the logic found here: https://github.com/google/jax/blob/b187663a87cc2a46317e208b1810b4d65cb19916/jax/lax/lax_fft.py#L38-L44' by a COLLABORATOR of type User on 2020-07-13T21:23:16Z\n'This is a list of functions that also are `np.ndarray` methods; I don't think `sort_complex` falls in this category, so it can be removed from this list.\r\n' by a COLLABORATOR of type User on 2020-07-13T21:25:26Z\n'One other thing: JAX arrays are immutable, so they don't support in-place sorts.  This would need to be `a = sort(a)` for the algorithm to work correctly on device array inputs.' by a COLLABORATOR of type User on 2020-07-13T21:38:54Z\n'Revert the addition of an extra space.\r\n' by a COLLABORATOR of type User on 2020-07-20T12:53:33Z\n'Couldn't we write this more simply something like this:\r\n```\r\na = lax.sort(a, dimension=0)\r\nreturn lax.convert_element_type(a, result_type(a, complex64))\r\n```\r\n\r\n`lax.convert_element_type` is a no-op if the type conversion is redundant, and `jnp.result_type` has the \"promote to complex\" behavior you are looking for.\r\n?' by a COLLABORATOR of type User on 2020-07-20T12:55:20Z\n'JAX uses 2-space indentation. Please fix?' by a COLLABORATOR of type User on 2020-07-20T12:55:43Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 7,
        "reason_for_closure": "Superseded by another PR/Issue :- PR closed as the author realized another PR had already implemented the same functionality with merge conflicts resolved."
    },
    {
        "summary": "Pull Request '3639' titled 'Adding support for array split' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-07-02T11:58:06Z, and was closed at 2020-09-02T23:13:54Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Adapted the split code to reproduce array_split behaviour'\nPR has comments:\n'Done in #4197' by a COLLABORATOR of type User on 2020-09-02T23:13:54Z\n\nPR has review comments:\n'A lot of this function is identical to `split`. My understanding is that `array_split` only differs from `split` for integer `indices_or_sections` that are not divisors of the corresponding axis. Could we compute `split_indices` in that case and delegate the rest of this function to `split()`?' by a COLLABORATOR of type User on 2020-07-06T18:19:54Z\n'It looks like this test doesn't exercise the situation where the array size is not divisible by the number of sections.' by a COLLABORATOR of type User on 2020-07-06T18:20:55Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Superseded by another change/fix.  No longer necessary.  Duplicate/already addressed elsewhere.  Incorrect approach/implementation. Intended behavior differs from numpy."
    },
    {
        "summary": "Pull Request '3583' titled 'add lax.associative_scan to docs' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-06-27T17:10:47Z, and was closed at 2020-08-28T18:22:52Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Also tweaked the docstring to say \"arrays\" instead of \"Tensors\".'\nPR has comments:\n'The docs appear to have several rendering problems: https://jax--3583.org.readthedocs.build/en/3583/_autosummary/jax.lax.associative_scan.html#jax.lax.associative_scan\r\n\r\nPlease fix?' by a COLLABORATOR of type User on 2020-06-28T16:44:09Z\n'Thanks both for the fixes! Also thanks @hawkinsp for showing me how to check docs rendering on a branch.' by a COLLABORATOR of type User on 2020-06-28T17:09:43Z\n'Closing in favor of #4172' by a COLLABORATOR of type User on 2020-08-28T18:22:52Z\n\nPR has review comments:\n'nit: np -> lax' by a CONTRIBUTOR of type User on 2020-06-28T16:04:20Z\n'ditto: np -> lax' by a CONTRIBUTOR of type User on 2020-06-28T16:04:35Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as wontfix: PR deprecates useful functionality; JAX should supersede NumPy, not match it."
    },
    {
        "summary": "Pull Request '3575' titled 'Improve jnp.sum's error messages for tuple inputs' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-06-26T19:11:54Z, and was closed at 2021-06-17T13:21:00Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'For issue #3451 '\nPR has comments:\n'@mattjj this is an attempt to point out the issue, I'm sure there must be a better way to solve it. I suspect modifying `_can_call_numpy_array` function to prevent the creation of a `ndarray` on bad input might be enough. However, I don't know how to define this \"bad input\" properly.' by a CONTRIBUTOR of type User on 2020-06-26T19:17:07Z\n'Awesome, @IgorWilbert ! This seems like a great way to do it to me.\r\n\r\nIt looks like there are some easy lint errors to fix (trailing whitespace).' by a COLLABORATOR of type User on 2020-06-26T21:04:15Z\n'@mattjj I think I am all set with lint now. Thank you for the detailed explanation, I have added a new test for it. However, I was not sure about where in the file it should be placed, and if I should make a reference to the issue number on the name of the test. Also, I noticed the `# TODO(mattjj): maybe move these two functions into xla.py` message. Do you think now is a good time to move these functions? Thanks!' by a CONTRIBUTOR of type User on 2020-06-27T21:17:53Z\n\nPR has review comments:\n'Can we catch some specific exception classes here, rather than the generic `except`?' by a COLLABORATOR of type User on 2020-06-26T21:04:54Z\n'Instead of `pass` here, I think we could put the final line `raise TypeError(\"Unexpected input type for array: {}\".format(type(x))) from None` here, with the `from None` on the end to avoid the extra exception printing.' by a COLLABORATOR of type User on 2020-06-26T21:05:48Z\n'Sure! I have added `RuntimeError`, `TypeError` and `ValueError` which I thought would be relevant. Please let me know if you want me to add or remove any class.' by a CONTRIBUTOR of type User on 2020-06-27T21:12:06Z\n'Great suggestion, the code is much better now. Also thank you for the `from None` explanation, it saved me some time :)' by a CONTRIBUTOR of type User on 2020-06-27T21:13:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 4,
        "reason_for_closure": "Improved error messages for tuple inputs in `jnp.sum`."
    },
    {
        "summary": "Pull Request '3532' titled 'Static kwargs for jit' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-06-23T23:30:48Z, and was closed at 2021-04-09T13:47:56Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Fixes https://github.com/google/jax/issues/1936\r\n\r\nThis PR adds supports for static keyword arguments in `jit`:\r\n\r\n- The new `static_argnames` allows for explicitly indicating keyword arguments that should be treated as static. This eliminates the need for separate helper functions to support keyword arguments (https://github.com/google/jax/issues/1936)\r\n- If either `static_argnums` or `static_argnames` is not provided, the other argument is set by introspecting the wrapped function for positional-and-keyword arguments with `inspect.signature()`.\r\n- It is now allowed to not include all positional argument referenced in `static_argnums`, because keyword arguments are identified automatically. It's still _possible_ to mess this up (e.g., by not using `functools.wraps` or explicitly setting static_argnums/static_argnames inconsistently with the function signature), but it should be much less likely to happen inadvertently.\r\n\r\nExample usage:\r\n```python\r\n@partial(jax.jit, static_argnames=['precision'])\r\ndef dot(x, y, *, precision=None):\r\n  ...\r\n```\r\n\r\nThe overhead of `static_argnames` seems to be comparable to `static_argnums` (about 6 us each):\r\n```\r\nIn [1]: import jax\r\n\r\nIn [2]: def f(x, *args, **kwargs):\r\n   ...:     print(args, kwargs)\r\n   ...:     return x\r\n   ...:\r\n\r\nIn [3]: f_jit = jax.jit(f)\r\n\r\nIn [4]: f_args = jax.jit(f, static_argnums=(1,))\r\n\r\nIn [5]: f_kwargs = jax.jit(f, static_argnames=('y',))\r\n\r\nIn [6]: %timeit f_jit(0).block_until_ready()\r\n() {}\r\n82.2 µs ± 2.88 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [7]: %timeit f_args(0, 'something static').block_until_ready()\r\n('something static',) {}\r\n88.3 µs ± 1.44 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\r\n\r\nIn [8]: %timeit f_kwargs(0, y='something static').block_until_ready()\r\n() {'y': 'something static'}\r\n88 µs ± 2.19 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\r\n```\r\n\r\nTODOs:\r\n\r\n- [x] ~~combine `static_argnums` and `static_argnames` into `static_args`? This\r\n      can be done without any overhead in the jitted function by creating two\r\n      separate variables.~~ I think it's better to keep the arguments separate. That allows for avoiding `inspect` if you prefer to be fully explicit. \r\n- [x] allow omitting static args when calling f_jitted? It is already OK to\r\n      omit `static_kwargs`\r\n- [x] validation of static_argnames\r\n- [x] ~~update donated_argnums to allow specifying donated arguments as names?~~ Can save this for later.\r\n- [x] add `static_argnames` to other functions in `api.py` that accept `static_argnums`: `xla_computation` and `make_jaxpr`'\nPR has comments:\n'Any news on this?' by a CONTRIBUTOR of type User on 2020-07-23T08:55:42Z\n'@shoyer anything I could do to help push this along?  It would be very nice to have!' by a CONTRIBUTOR of type User on 2020-08-07T05:19:29Z\n'This needs review from @mattjj  ' by a COLLABORATOR of type User on 2020-08-07T06:37:51Z\n'Just poking this because it would be super useful' by a CONTRIBUTOR of type User on 2021-03-03T18:46:26Z\n'Marking @jakevdp as a reviewer since I know he's thought about this topic.\r\n\r\nThis definitely needs a rebase, but I'm curious about any concerns about the general approach.' by a COLLABORATOR of type User on 2021-03-03T18:51:53Z\n'So I cleaned this up, and synced it with the master branch. I think it works well and is a clear usability win, but we can't merge this PR in the current state because we would also need the feature in the C++ JIT, which is maintained as part of jaxlib.\r\n\r\nIf we're sure we want to do this, I suppose we should look into implementing it in C++ too, which is probably not too difficult (CC @jblespiau). I'm not entirely sure in what order that needs to happen with respect to merging this PR.\r\n\r\nI guess we need to reach consensus that this is what we want first. @jakevdp what do you think?' by a COLLABORATOR of type User on 2021-03-08T02:08:02Z\n'I only glanced rapidly (and did not get exactly how it was done), but I just wanted to say that it may be possible to do this in Python-land only, converting a call e.g. \r\n\r\nf(1, 2, kw1=3)\r\n\r\ninto f(1, 2, 3) so there would be no need for any C++ change (with some extra complexity for default keyword arguments).' by a CONTRIBUTOR of type User on 2021-03-08T09:44:56Z\n'> I only glanced rapidly (and did not get exactly how it was done), but I just wanted to say that it may be possible to do this in Python-land only, converting a call e.g.\r\n\r\nThis is technically possible using the `inspect` module (@jakevdp wrote a notebook exploring this), but I don't recommend it. The problem is that `inspect` does its argument parsing in Python, so even parsing trivial cases adds 20-30 µs of overhead at each function call.\r\n\r\nThe current approach does some manipulation of the `**kwargs` dict, which I know can be done much faster in C++.' by a COLLABORATOR of type User on 2021-03-08T16:56:46Z\n'> I guess we need to reach consensus that this is what we want first. @jakevdp what do you think?\r\n\r\nI like this approach for the most part. I think there was some discussion about whether static arguments should be declared in the function definition vs declared at the call site. Do you recall the resolution there?' by a COLLABORATOR of type User on 2021-03-08T17:32:27Z\n'> I like this approach for the most part. I think there was some discussion about whether static arguments should be declared in the function definition vs declared at the call site. Do you recall the resolution there?\r\n\r\nHere is that proposal from our internal chat:\r\n\r\n- The idea is that `jax.jit(f)(jax.static(x), y)` compiles to the same thing as `jax.jit(f, static_argnums=0)(x, y)` with the bonus that it documents staticness at the call site instead of far away at the jitting. It also works well with PyTrees.\r\n- static beats partial for two reasons:\r\n  1. it is much clearer to read function arguments in the natural order `f(w, static(x), y, static(z))` is much clearer than `partial(f, x=x, z=z)(w, y)`\r\n  2. it handles PyTrees more gracefully where you can have mixed static/dynamic pytrees instead of favoring the top-level arguments.\r\n\r\nMy take: `static` looks nice (particularly for inline uses of `jit`), but it's an orthogonal feature and we still need something like `static_argnames`. Library code written using JAX (including JAX itself) defines `jit` decorated functions, and those functions need some way to declare static arguments, unless we want to insist that users always box static arguments in `static` (which would be quite the change).' by a COLLABORATOR of type User on 2021-03-08T18:16:10Z\n'See https://github.com/google/jax/pull/6273 for the immediate replacement here. We'll need to update the C++ machinery before exposing this publicly.' by a COLLABORATOR of type User on 2021-03-29T21:55:27Z\n'https://github.com/google/jax/pull/6365 lands this support, including in the C++ `jit`.' by a COLLABORATOR of type User on 2021-04-09T13:47:56Z\n\nPR has review comments:\n'misleading function name? Maybe clearer to inline this' by a COLLABORATOR of type User on 2021-03-04T17:22:01Z\n'use `_ensure_tuple`?' by a COLLABORATOR of type User on 2021-03-04T17:28:30Z\n'I'm a bit confused at this point... my understanding from above was that argnums and argnames now duplicate each other. Do we need to apply argnames if they've already been applied via their argnum determined from the signature?' by a COLLABORATOR of type User on 2021-03-04T17:40:24Z\n'It may be worth here doing something like\r\n```python\r\nif not core.skip_checks:\r\n    assert all(isinstance(val, type_) for val in result)\r\n```' by a COLLABORATOR of type User on 2021-03-04T17:42:51Z\n'We need to handle both here, because we don't want to inspect function signatures each time a `jit` decorated function is called (`inspect` is slow). Thus `argnums` is applied to all parameters passed by position (`*args`), and ` argnames`  is applied to all parameters based by name (`**kwargs`).' by a COLLABORATOR of type User on 2021-03-04T18:02:53Z\n'Ah, I see that now, thanks.' by a COLLABORATOR of type User on 2021-03-04T18:26:55Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 12,
        "num_review_comments": 6,
        "reason_for_closure": "Implemented :- PR closed as the feature request was implemented in another PR."
    },
    {
        "summary": "Pull Request '3528' titled 'WIP: Print JAX arrays to full precision' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-06-23T17:19:48Z, and was closed at 2020-06-24T21:36:46Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Hi! I made an attempt to implement print of JAX arrays to full precision using `jax.numpy.set_printoptions` as described in #3438. Now I am trying to figure out how to test the function. I have seen [some options](https://stackoverflow.com/questions/33767627/python-write-unittest-for-console-print) but they don't seem very adequate;'\nPR has comments:\n'On closer inspection, I'm not sure there's a bug here. JAX does seem to be respecting the precision options when printing. I think the original issue report was incorrect.' by a COLLABORATOR of type User on 2020-06-24T21:36:46Z\n'@hawkinsp makes sense, thanks for the feedback! I wish to work on it if the issue happens to be reopened.' by a CONTRIBUTOR of type User on 2020-06-24T21:39:04Z\n\nPR has review comments:\n'It's not sensible to compare the arrays for equality. Don't you want to test a property about how the arrays print as strings? Note \"exact equality with NumPy\" isn't the right criterion either, but testing that particular arrays convert to particular strings might work.' by a COLLABORATOR of type User on 2020-06-24T13:32:27Z\n'This doesn't seem like it's doing something sensible to me. The code you are copying from in #3438 is printing a tree of arrays by first converting arrays to NumPy and then relying on NumPy to do the right thing with respect to precision. However that's not what you want here; instead we want to change how `DeviceArray` values print themselves. I don't see how the change you have made here could work.\r\n\r\nI suspect you want to do is to change the implementation of `DeviceArray.__repr__` to take more of the NumPy options into account. Its implementation is here:\r\nhttps://github.com/google/jax/blob/a6e3f992a70775ef16c285e024cd2ac62ea32077/jax/interpreters/xla.py#L1051\r\n\r\nI suggest reverting changes to this file and to try modifying that function.' by a COLLABORATOR of type User on 2020-06-24T13:32:28Z\n'@hawkinsp you are definitely right, thank you for the suggestion. Now we have a more meaningful test that fails as expected. I think it could be further improved later though.' by a CONTRIBUTOR of type User on 2020-06-24T21:02:08Z\n'@hawkinsp thank you for the suggestion and for indicating the location of the implementation. You are right, we should change how `DeviceArrays` are printed instead of using such workaround. It turns out that because there is an implementation of `DeviceArray.__str__`, the function `DeviceArray.__repr__` is [ignored](https://stackoverflow.com/questions/1436703/difference-between-str-and-repr). So I suppose the implementation of `__str__` should be modified. Now I am trying to figure you where and why the `floatmode=\"unique\"` option does not behave as expected.' by a CONTRIBUTOR of type User on 2020-06-24T21:10:22Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 4,
        "reason_for_closure": "Inconclusive: PR closed after discussion about correct approach and existing alternatives."
    },
    {
        "summary": "Pull Request '3499' titled 'Add np.trim_zeros numpy function' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-06-21T04:21:11Z, and was closed at 2022-06-29T19:41:45Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Addresses: #70 '\nPR has comments:\n'`np.trim_zeros` typically takes a string argument : https://github.com/numpy/numpy/blob/v1.18.4/numpy/lib/function_base.py#L1574 , but jax doesn't support strings as arguments, so I'm assuming `trim='fb'`' by a CONTRIBUTOR of type User on 2020-06-21T04:34:09Z\n'Thanks for the contribution!\r\n\r\nJax can accept string arguments, so long as the arguments are marked as static whenever the function is jit-compiled (see, for example [`jax.numpy.einsum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.einsum.html)). I'd suggest implementing the `trim` argument if you're willing!' by a COLLABORATOR of type User on 2020-06-21T14:16:10Z\n'@jakevdp Thanks for the info! Would love another review. (Apologies for the delay, have been traveling)' by a CONTRIBUTOR of type User on 2020-07-02T14:10:40Z\n'There is an error in our new test that checks that the signatures of the numpy and jax.numpy versions of the function match. In your implementation, you call the first argument `a`, where numpy calls the first argument `filt`. To pass the test, you can use `filt` for the first argument.' by a COLLABORATOR of type User on 2020-07-02T21:06:35Z\n'Ah, didn't realize that the name of the parameter also mattered, thanks for that! Would love a final review.' by a CONTRIBUTOR of type User on 2020-07-06T06:50:01Z\n'We merged https://github.com/google/jax/pull/4027 which added this function.' by a COLLABORATOR of type User on 2022-06-29T19:41:44Z\n\nPR has review comments:\n'When all arguments are static, there's nothing gained by jit-compiling the function. It would make more sense to directly define the implementation here.' by a COLLABORATOR of type User on 2020-07-06T17:21:38Z\n'It seems like it would be cleaner to continue using `enumerate` here, maybe something like this?\r\n```python\r\ndef trim_zeros(filt, trim='fb'):\r\n  start = 0\r\n  end = 0\r\n  if 'f' in trim.lower():\r\n    for start, v in enumerate(filt):\r\n      if v != 0:\r\n        break\r\n  if 'b' in trim.lower():\r\n    for end, v in enumerate(filt[::-1]):\r\n      if v != 0:\r\n        break\r\n  return filt[start: len(filt) - end]\r\n```' by a COLLABORATOR of type User on 2020-07-06T17:26:27Z\n'Generally our test frameworks uses lambdas to bind static parameters, and args_maker to generate random inputs. So it would look something like this:\r\n```python\r\n    np_fun = lambda filt: np.trim_zeros(filt, trim=trim)\r\n    jnp_fun = lambda filt: jnp.trim_zeros(filt, trim=trim)\r\n    args_maker = lambda: [rng(a_shape, dtype)]\r\n```' by a COLLABORATOR of type User on 2020-07-06T17:28:57Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 3,
        "reason_for_closure": "Inconsistent :- PR closed as the fix was deemed unnecessary."
    },
    {
        "summary": "Pull Request '3354' titled 'Refined the forcing of annotation processig in doc building' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-06-08T07:34:28Z, and was closed at 2020-06-23T13:35:09Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Re-implemented the insertion of annotations during doc building\r\n\r\n    * the line is inserted before the first import. Otherwise,\r\n      Sphinx will ignore the module-level docstring.\r\n    * the line is inserted only if not already present\r\n    * atexit, remove the line (we can run it on local development environment)\r\n    * since this code is complicated, move it to util.py and add a test.'\nPR has comments:\n'I have updated it to include a fix for #3452. However, I think that this is now too complicated and perhaps should just remove all this and wait for Python 3.7 (#3485)' by a COLLABORATOR of type User on 2020-06-18T12:00:59Z\n'Closing, since we will wait for Python 3.7' by a COLLABORATOR of type User on 2020-06-23T13:35:09Z\n\nPR has review comments:\n'I'm a bit unhappy that a doc build would touch source files in my git checkout, which is why I guarded this with `READTHEDOCS`. WDYT?' by a COLLABORATOR of type User on 2020-06-08T14:03:49Z\n'I feel the same, this whole thing is a big hack. But when I test the doc building locally, I want to see what will appear in RTD. So, I would run it with READTHEDOCS on. I guess if we leave the conditionals, others can skip it. \r\n\r\nHowever, I have tested it with a dirty checkout and it seems safe to use. \r\nAn alternative would be to make a copy of the sources and run the building there. \r\n\r\n\r\n\r\n ' by a COLLABORATOR of type User on 2020-06-08T16:11:07Z\n'I have added back the READTHEDOCS guard, although I think that it is not a useful guard. \r\nI have also changed the developer.rst to suggest setting this on before running sphinx locally. I assume you want to see the documentation as it will be rendered on readthedocs. ' by a COLLABORATOR of type User on 2020-06-18T09:41:21Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 3,
        "reason_for_closure": "The PR was closed because it was deemed unnecessary as the underlying issue was fixed by a dependency update (CUDA 12.6 U3)."
    },
    {
        "summary": "Pull Request '3338' titled 'Add segment_max operator.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-06-05T18:11:30Z, and was closed at 2021-05-12T19:10:47Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Segment_max is an important operation for GraphNets and is an open feature request at https://github.com/google/jax/issues/2255'\nPR has comments:\n'Thanks! Resolved by putting the test in lax_numpy_indexing_test.\r\n' by a CONTRIBUTOR of type User on 2020-06-18T11:14:17Z\n'This is still an important feature for softmax reducer in graph networks.' by a CONTRIBUTOR of type User on 2021-01-31T15:31:26Z\n'Looks like `jax.ops.segment_max` was added by another PR. Thanks for helping make this happen!' by a COLLABORATOR of type User on 2021-05-12T19:10:47Z\n'(https://github.com/google/jax/pull/6384 was the PR that added it)' by a COLLABORATOR of type User on 2021-05-12T19:12:00Z\n\nPR has review comments:\n'I'm wondering whether this is wise. Divisions can be expensive; I realize you are probably just copying `segment_sum` but I'm thinking users should not come to expect this \"wrapping\" out of domain behavior.\r\n\r\n(FWIW, the semantics of `index_update` is that updates with indices out of range are dropped.)' by a COLLABORATOR of type User on 2020-06-23T13:37:59Z\n'Given this is a mask, perhaps do the scatter with a `bool` type?' by a COLLABORATOR of type User on 2020-06-23T13:39:32Z\n'I'd be very happy to drop that and move to a more straightforward implementation - I did just include this for consistency with the existing segment_sum implementation.' by a CONTRIBUTOR of type User on 2020-06-24T22:40:47Z\n'Good idea' by a CONTRIBUTOR of type User on 2020-06-24T22:42:10Z\n'You may need a blank line before `Args`.\r\n\r\nI wanted to check the documentation rendering and then I realized this PR also needs to add `segment_max` to the docs. (`jax.ops.rst`)' by a COLLABORATOR of type User on 2020-07-07T13:06:19Z\n'There's actually no need to make these test parameters if there's only one of them. (The reason many test cases put these as parameters is historical.)' by a COLLABORATOR of type User on 2020-07-07T13:07:11Z\n'You should instantiate these outside `args_maker`.\r\n\r\nAnd I think you could instantiate both with `self.rng()`, which is a random state deterministically chosen based on the full test case name (including any parameters, etc.)' by a COLLABORATOR of type User on 2020-07-07T13:09:16Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 7,
        "reason_for_closure": "Inconsistent behavior with NumPy was deemed unnecessary to fix, closing the PR."
    },
    {
        "summary": "Pull Request '3333' titled 'Extract JAX primitive test harness from lax_test.py.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-06-05T13:23:03Z, and was closed at 2020-10-16T08:48:05Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'The goal is to have a collection of test harnesses for the JAX primitives\r\nto be able to test various implementation (JAX, NumPy, TensorFlow).\r\n\r\nPut the primitive_harness.py in the main source tree, under _test_utils,\r\nso that it can be imported from tests.\r\n\r\nDemonstrate the use of the harness for lax.pad and lax.squeeze, both in\r\ntf_ops_test and lax_test. The plan is to add support for more primitives\r\nas we make progress testing jax_to_tf.'\nPR has comments:\n'@tomhennigan PTAL' by a COLLABORATOR of type User on 2020-06-05T13:24:04Z\n\nPR has review comments:\n'This change seems very heavyweight for what it achieves and in my mind does significant harm to the readability of the tests. In particular, the test harness is no longer next to the test, and I would not want to maintain the result of doing this widely.\r\n\r\nCouldn't we achieve the same end goal (test both JAX and JAX-to-TF implementations of each primitive) by extending `_CompileAndCheck` (or adding a sibling to `_CompileAndCheck` that is also called from the lax tests?) The goal of `_CompileAndCheck` is essentially to compare two different implementations of a function (jitted and unjitted). Wouldn't it be simpler to extend it to compare three?\r\n\r\nAnother option might be to parameterize the test class as a whole (e.g., subclass it for each backend or something like that.)' by a COLLABORATOR of type User on 2020-06-05T13:28:39Z\n'We've also discussed that as part of the omnistaging change it may be possible to switch out which implementations of primitives you get under a scope (i.e., what the `impl` rule does). That may be helpful here, because then you can achieve much the same end by running exactly the same code in a scope (e.g., via a different test subclass that wraps test invocations or something like that.)' by a COLLABORATOR of type User on 2020-06-05T13:34:37Z\n'I agree that it is not great that this moves the harness away from the test code. But if you want to use the harness in more than one test, in at least one of them it is going to be away from test code. First I tried to keep lax_test as is (with harness + check together), and inherit the harness from it in jax_to_tf tests. This is not easy because lax_test.py is outside the jax package source tree, and not easily importable. Hence, the idea of primitive_harness inside the jax source tree.  \r\n\r\nI also tried to inherit the whole LaxTest class and then try to reuse it by hijacking various pieces of it. One of the attempts was to actually hijack the impl rule. That is quite messy, and couples the tests in complicated ways.\r\n\r\nWhat I want to achieve is to separate the harness (the set of inputs covering each primitive), from what you do with it. \r\n\r\n\r\n' by a COLLABORATOR of type User on 2020-06-05T15:52:29Z\n'My gut agrees with @hawkinsp that this will be hard to maintain.\r\n\r\nThe reason why I find this hard to read is that it separates the definition of the \"harness\" from the test itself. Now there are two separate files to keep in sync for each LAX function. This seems like it could be quite painful to maintain/upgrade.\r\n\r\nI would be curious to see a sketch of what the other approach with inheritance looks like. Maybe we'll be convinced after we see how messy the alternative is :)' by a COLLABORATOR of type User on 2020-06-05T16:45:11Z\n'This is a good idea, I can try to prepare another PR with an alternative. \r\n\r\nHowever, I do think that in this case there is a separation of concerns between the harness and the check. The harness defines a set of inputs to the primitive that we believe covers its semantics. This is separate from what you do with it: you compare compiled and uncompiled code, you compare with numpy, etc. Because of this separation I think it is not too bad to have them in separate places. \r\n\r\nConsider this code in lax_test.py\r\n```\r\n  @primitive_harness.parameterized(primitive_harness.lax_pad)\r\n  def testPadAgainstNumpy(self,  harness: primitive_harness.Harness):\r\n    self._CheckAgainstNumpy(lax_reference.pad,\r\n                            harness.fun,\r\n                            lambda: harness.args_maker(self.rng()))\r\n```\r\n\r\nI can understand this test without needing to know what sets of inputs are in `primitive_harness.lax_pad`. Similarly, I can read the `primitive_harness.lax_pad` without needing to know what checking I am going to do with it.\r\n\r\n' by a COLLABORATOR of type User on 2020-06-05T17:19:45Z\n'However in many of these tests (perhaps less so in `lax_test.py`) the contract is not a direct match to a primitive's API, but a contract with the test function itself. So separating the two harms readability.' by a COLLABORATOR of type User on 2020-06-05T17:39:36Z\n'I agree that this is separately understandable, but there are also a lot of repeated references to the specific function, which suggests that it someone specialized to the inputs.\r\n\r\nIs there a reason for writing separate test methods for each function? It seems like it might be possible to consolidate these, e.g., into something like the following:\r\n```\r\n  @primitive_harness.parameterized(\r\n      primitive_harness.lax_pad,\r\n      primitive_harness.lax_squeeze,\r\n      ...)\r\n  def test(self, harness: primitive_harness.Harness):\r\n    self._CheckAgainstNumpy(harness.reference_fun\r\n                            harness.fun,\r\n                            lambda: harness.args_maker(self.rng()))\r\n    if harness.differentiable:\r\n      jtu.check_grads(...)\r\n```\r\n\r\nHere's it's clear that nothing is particular to `pad` and all that needs to be kept in sync is the list of harnesses to check.' by a COLLABORATOR of type User on 2020-06-05T17:41:20Z\n'To @shoyer: Absolutely we can consolidate the CheckAgainstNumpy tests for multiple harnesses. Otherwise, I suspect that there will be a lot of identical test functions with a different harness each. I did not do that yet because I wanted to just try out this idea and discuss it before I put too much effort in it.   ' by a COLLABORATOR of type User on 2020-06-05T17:45:23Z\n'Re @hawkinsp: You are probably right, I do not intend to hoist all harnesses out. Only those that are about covering a primitive's semantics. ' by a COLLABORATOR of type User on 2020-06-05T17:46:41Z\n'Meanwhile I have discovered that lax.pad does not have test coverage for negative padding, and in fact lax_reference.pad is not correct for that case. I will definitely add a test case for that in jax_to_tf, but would be nice to have a shared harness. ' by a COLLABORATOR of type User on 2020-06-08T13:16:23Z\n'Implemented this idea separately for jax_to_tf tests (#3376). We can extend it later to lax_test.py as in this PR. ' by a COLLABORATOR of type User on 2020-06-09T06:16:51Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 11,
        "reason_for_closure": "Closed as the proposed change was deemed unnecessary and potentially harmful to existing users."
    },
    {
        "summary": "Pull Request '3263' titled 'WIP: tree vectorizing transformation' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-05-31T05:54:31Z, and was closed at 2021-11-10T23:08:42Z by a User.\nThe PR has labels: cla: yes - , pull ready - Ready for copybara import and testing. \nIt has a body of 'Fixes https://github.com/google/jax/issues/1012\r\n\r\nThe idea behind this transformation is that every argument to a function decorated by `@tree_vectorize` is virtually flattened and concatenated into 1D vector, e.g., `{'x': 1, 'y': array([[2, 3]])}` becomes `array([1, 2, 3])`.\r\n\r\nHowever, under the covers the original arrays are still preserved with their original shapes. This has several advantages:\r\n\r\n1. we don't actually need to copy the arrays into a giant vector (avoiding the unnecessary memory copies)\r\n2. all operations happen on arrays with the original shapes (useful for predictable performance on TPUs)\r\n3. we can restore the tree structure on the outputs of the function\r\n\r\nThe hope is that this should make it considerably easier to write efficient numerical algorithms that work on pytrees (e.g., for optimization, linear equation solving or ODE solving).\r\n\r\nHere are a few simple examples:\r\n```python\r\nfrom jax import tree_vectorize\r\nimport jax.numpy as jnp\r\n\r\nx = {'a': 1, 'b': jnp.array([2, 3])}\r\ny = 1\r\nadd = tree_vectorize(jnp.add)\r\nprint(add(x, y))\r\n# {'a': DeviceArray(2, dtype=int32), 'b': DeviceArray([3, 4], dtype=int32)}\r\n\r\n@tree_vectorize\r\ndef norm(x):\r\n  assert x.shape == (3,)\r\n  return jnp.sqrt(jnp.sum(x ** 2))\r\n\r\nprint(norm(x))\r\n# 3.7416575\r\n```\r\n\r\nEven though the inputs are always treated as vectors, outputs can have any number of dimensions, e.g., producing nested pytrees-of-pytrees, as shown in this example from the unit tests:\r\n```python\r\n@tree_vectorize\r\n@shapecheck(['n', 'm'], '(n, m)')  # just to show that it works!\r\ndef add_outer(x, y):\r\n  return jnp.expand_dims(x, 1) + jnp.expand_dims(y, 0)\r\n\r\ntree1 = {'a': 1, 'b': jnp.array([2, 3])}\r\ntree2 = {'c': 10, 'd': jnp.array([20, 30])}\r\npprint.pprint(add_outer(tree1, tree2))\r\n# {'a': {'c': DeviceArray(11, dtype=int32),\r\n#        'd': DeviceArray([21, 31], dtype=int32)},\r\n#  'b': {'c': DeviceArray([12, 13], dtype=int32),\r\n#        'd': DeviceArray([[22, 32],\r\n#              [23, 33]], dtype=int32)}}\r\n```\r\n\r\nTODO:\r\n\r\n- [ ] missing ops for `cg`:\r\n    - [x] `jit`\r\n    - [ ] `while_loop`\r\n- [ ] further missing ops for `odeint`:\r\n    - [x] `concatenate`\r\n    - [ ] `gather`\r\n    - [ ] `scatter`\r\n- [ ] measure performance improvement for `odeint`!\r\n- [ ] figure out the API, e.g.,\r\n     - [ ] explicit API for indicating functions, especially with multiple outputs?\r\n     - [ ] explicit API for indicating non-vectorized arguments?\r\n- [ ] make things more robust\r\n     - [ ] `tree_call` works with `vmap`, `jvp` and `vjp` (i.e., to implement `jacfwd` and `jacrev`)\r\n     - [x] missing `Trace` methods like `process_call`, `post_process_call`\r\n     - [x] support matching against explicit axes with trivial treedefs, e.g., so `x + jnp.zeros(...)` and explicit broadcasting work. \r\n- [ ] more comprehensive tests\r\n     - [ ] check exceptions\r\n     - [ ] make a reference implementation for a subset of `tree_vectorize` use cases that works by explicitly unraveling/concatenating\r\n'\nPR has comments:\n'A good test for composability with other JAX transformations are the various autodiff helper functions like `jacfwd` and `jacrev`.\r\n\r\nIn theory, the following should work, using `tree_vectorize` to handle all of the tree-mangling from `api.py` (`_std_basis`, `_unravel_array_into_pytree`, `tree_transpose`, etc):\r\n```python\r\n  def test_jacobian(self):\r\n\r\n    @tree_vectorize\r\n    def _jacfwd(f, x):\r\n      pushfwd = partial(jax.jvp, f, (x,))\r\n      basis = jnp.eye(x.size, dtype=x.dtype)\r\n      y, jac = jax.vmap(pushfwd, out_axes=(None, 1))((basis,))\r\n      return jac\r\n\r\n    @tree_vectorize\r\n    def _jacrev(f, x):\r\n      y, pullback = jax.vjp(f, x)\r\n      basis = jnp.eye(y.size, dtype=y.dtype)\r\n      jac = jax.vmap(pullback)(basis)\r\n      return jac\r\n\r\n    def _apply_argnums(transform, fun, argnums=0):\r\n      def jacfun(*args):\r\n        f_partial, dyn_args = api.argnums_partial(\r\n            lu.wrap_init(fun), argnums, args)\r\n        result = transform(f_partial.call_wrapped, dyn_args)\r\n        return result[0] if isinstance(argnums, int) else result\r\n      return jacfun\r\n\r\n    jacfwd = partial(_apply_argnums, _jacfwd)\r\n    jacrev = partial(_apply_argnums, _jacrev)\r\n\r\n    f = lambda x: {'c': x['a'] * (1 + x['b'] ** 2), 'd': x['a'] - x['b']}\r\n    tree = {'a': 1.0, 'b': 2.0}\r\n    expected = jax.jacfwd(f)(tree)\r\n\r\n    actual = jacrev(f)(tree)\r\n    self.assertTreeEqual(expected, actual)\r\n\r\n    actual = jacrev(f)(tree)\r\n    self.assertTreeEqual(expected, actual)\r\n```\r\n\r\nHowever, this test currently fails, because the way I handle function arguments with `tree_call` doesn't seem to work properly with autodiff. I think the problem is that `tree_call` needs to work more like an actual `call` primitive (re-invoking tracing), but there may be other ways to fix it.\r\n\r\nAttention from somebody who deeply understands JAX's transform rules (i.e., @mattjj ) would be appreciated :)' by a COLLABORATOR of type User on 2020-06-07T17:27:35Z\n'I simplified the issue from `jacobian` into `test_vmap_tree_call`. The underlying issue now is that transformations like `vmap` or `jvp` should be pushed down inside `tree_call`, but that doesn't currently work, i.e., we want `vmap(tree_call(...))` to be rewritten into `tree_call(vmap(...))`.' by a COLLABORATOR of type User on 2020-06-13T18:59:16Z\n'Closing this in favor of #8504' by a COLLABORATOR of type User on 2021-11-10T23:08:42Z\n\nPR has review comments:\n'I think this is very close to working, except we don't handle `basis` properly yet because it has a different tree structure. (This works in other cases, but not for transformed tree_call functions yet.)' by a COLLABORATOR of type User on 2021-03-05T23:49:50Z\n'Gradient support needs `JaxprTrace.process_tree_call` (which seems straightforward, but tedious).' by a COLLABORATOR of type User on 2021-03-05T23:50:52Z\n'@apaszke  This is starting to feel pretty repetitive, especially when we consider doing the same thing for every higher order fucntions/transformation in JAX (e.g., partial eval, scan, while_loop, etc). It feels like there must be a better way to leverage existing support for wrapping trees...' by a COLLABORATOR of type User on 2021-03-05T23:53:44Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 3,
        "reason_for_closure": "Closed as not useful; the change would deprecate existing functionality and break existing users."
    },
    {
        "summary": "Pull Request '3159' titled 'unrolled and block-unrolled scan' was authored by a User, who is associated as a MEMBER. \nIt was created at 2020-05-20T02:05:14Z, and was closed at 2021-04-08T18:47:38Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'Is this PR now stale, given `scan(..., unroll=...)` is merged already?' by a COLLABORATOR of type User on 2021-04-08T15:38:15Z\n\nPR has review comments:\n'I would explain here why would one use this. And I would also add an entry to Changelog and the FAQ.' by a COLLABORATOR of type User on 2020-05-20T15:50:37Z\n'Maybe you could concatenate pytree outputs with \r\n\r\n```python\r\nys = tree_multimap(lambda x, y: lax.concatenate((x, y)), ys, ys_rem)\r\n```\r\n\r\nand change the reshape at the end of `_block_unrolled_scan` accordingly?' by a CONTRIBUTOR of type User on 2020-05-23T13:46:44Z\n'Change the reshape to \r\n\r\n```python\r\ntree_map(lambda x: lax.reshape(x, (-1, *x.shape[2:])), ys_blocks)\r\n```\r\n\r\nfor pytree support?' by a CONTRIBUTOR of type User on 2020-05-23T13:49:26Z\n'Ah, sorry! I overlooked that `unrolled_scan` must have support for pytrees, such that `ys_block` can be a pytree in the first place. \r\n\r\nBut am I missing something, or is the actual \"unrolling\" functionality missing in `unrolled_scan`?' by a CONTRIBUTOR of type User on 2020-05-24T09:55:08Z\n'This seems like the wrong mechanism to achieve unrolling? It may work, but it is surprising and seems like it might have collateral damage.\r\n\r\nAlso, perhaps we should make `disable_jit` inside of a `jit` an error after we land omnistaging?' by a COLLABORATOR of type User on 2020-05-26T19:47:46Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 5,
        "reason_for_closure": "Inconsistent behavior :- PR tries to remove support for complex dtypes but JAX generally aims to extend NumPy's functionality, not restrict it.  The complex implementation, while imperfect, is not incorrect, thus removing it is not beneficial."
    },
    {
        "summary": "Pull Request '3118' titled 'Reductions won't accept lists anymore' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-05-16T13:22:25Z, and was closed at 2023-02-01T17:43:03Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Solves #3038 doing what was advised on #3050 by @jakevdp '\nPR has comments:\n'So, I am allowing tuples as input, otherwise we will lose `jnp.prod(shape)`, which would be a shame' by a CONTRIBUTOR of type User on 2020-05-16T14:12:29Z\n'So, right now it's failing on the masking test and parallel test, with the same error, can you guys help me out? @shoyer @hawkinsp \r\nSorry for bothering' by a CONTRIBUTOR of type User on 2020-06-08T14:58:15Z\n'From the test failures, it looks like the tests are passing a list to a jax function that no longer accepts lists. One of the issues comes from the fact that reductions attempt to convert their inputs into arrays: https://github.com/google/jax/blob/fd886b17a48f23637f3fcae810117721f155e16c/jax/numpy/lax_numpy.py#L1520\r\n\r\nBut the test of reductions is passing a list: https://github.com/google/jax/blob/e36c72b9839efe8a98a75d93f87661f55b585ddb/jax/experimental/jax_to_tf/tests/tf_ops_test.py#L283-L284' by a COLLABORATOR of type User on 2020-06-08T16:15:01Z\n'A general comment: I'm a bit wary of this pull request as written. The problem is, it removes support for codepaths that are likely employed by many users (passing lists to jax functions), and so will almost certainly introduce breakages into user code.\r\n\r\nFor any change like this, we should be displaying a deprecation warning for a few releases: that is, if a user is passing input that currently works and will not work after this change, it should not break immediately, but rather raise a `FutureWarning` with instructions about how to fix the issue. Then, once that warning has been present for a few release cycles, we can make the breaking change.' by a COLLABORATOR of type User on 2020-06-08T16:17:58Z\n'Great idea Jake, this would be quite disruptive to codebases as it stands indeed, I opened a PR with the warning' by a CONTRIBUTOR of type User on 2020-06-08T19:46:19Z\n'Hey folks. I just encountered the future warning about reductions on list and tuples. However, (and I only read quickly) - most of the chat in here seemed to be about lists and excluded tuples from this change.\r\n\r\nI just wanted to check in if this was intended. I've got a quite a few cute little tuple comprehensions that use jnp.sum and I'll rewrite them now (they are under review) if this is the direction JAX is taking. But if not, ..., well they are cute.' by a NONE of type User on 2020-06-29T20:47:34Z\n'Hi @btanner - we are indeed moving to a world where tuples & lists will not be silently converted to device arrays by jax functions. If this is really what you want, you can always do it explicitly with something like `jnp.sum(jnp.asarray(tup))`. But if your tuples are small, the overhead of converting them to device arrays before summing is probably not what you want, and if your tuples are large enough that it is what you want, the overhead of constructing them as Python tuples (versus, say, native jax array operations) is probably not what you want.' by a COLLABORATOR of type User on 2020-06-29T21:00:16Z\n'> Hi @btanner - we are indeed moving to a world where tuples & lists will not be silently converted to device arrays by jax functions. If this is really what you want, you can always do it explicitly with something like `jnp.sum(jnp.asarray(tup))`.\r\n\r\nTo be explicit, the other option (ideal for short tuples) looks like `functools.reduce(jnp.add, tup)`' by a COLLABORATOR of type User on 2020-06-29T21:05:33Z\n\nPR has review comments:\n'This change appears to be causing `masking_test.py` to fail.\r\n\r\n(If nothing else you could kick the problem down the road by duplicating the logic as it was before the change.)' by a COLLABORATOR of type User on 2020-05-26T19:54:24Z\n'Can we move this test into a helper function? We should be consistent about what we accept in most of these NumPy functions, not just reductions, and that is probably most easily achieved with a helper function that does the checking, e.g., `_check_arguments_are_arrays_or_scalars(...)` or similar?\r\n\r\nDo you think we should accept tuples only in reductions? I'd lean towards not accepting them anywhere, so I'm curious whether you thing the `jnp.prod(shape)` pattern is common.' by a COLLABORATOR of type User on 2020-05-26T19:55:21Z\n'Also note you are using an f-string here but aren't formatting anything.' by a COLLABORATOR of type User on 2020-05-26T19:57:37Z\n'It caused many errors, so it seems to be common in testing at least, but I need to check the logs to see where exactly they were being used. ' by a CONTRIBUTOR of type User on 2020-05-27T22:33:44Z\n'On the f-string, the idea was to print a's type, then I removed, but maybe I should put it back, what do you think?\r\nAlso, I'm not exactly understanding why does it make masking_test fail, could you help me understand the error?\r\nThank you very much for the reviews :D' by a CONTRIBUTOR of type User on 2020-05-27T22:35:44Z\n'I agree with @hawkinsp  that we should try to avoid both lists and tuples.\r\n\r\nIt might be cleaner to write a helper function like `_asarray_no_coercion()`, and just use that instead of `asarray()` internally.' by a COLLABORATOR of type User on 2020-05-27T22:41:19Z\n'`host_callback_test`, `linalg_test` and `random_test` use this pattern, I could open a PR removing it, should I use `onp.prod` instead or do you guys prefer another fix?\r\n' by a CONTRIBUTOR of type User on 2020-05-27T22:49:09Z\n'`onp.prod(x)` is a good alternative. So is `functools.reduce(operator, x, 1)`, if a little more verbose.' by a COLLABORATOR of type User on 2020-05-27T23:00:57Z\n'I added the `_asarray_no_coercion()`, was this what you had in mind?\r\nAlso stopped accepting tuples' by a CONTRIBUTOR of type User on 2020-05-30T13:11:12Z\n'> (If nothing else you could kick the problem down the road by duplicating the logic as it was before the change.)\r\n\r\nCould you elaborate a little bit on this please?' by a CONTRIBUTOR of type User on 2020-05-30T13:12:29Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 10,
        "reason_for_closure": "Inconsistent behavior :- jax implementation was more general than numpy, PR aimed to reduce functionality."
    },
    {
        "summary": "Pull Request '3114' titled 'Experimental thick-restart Lancozs method for sparse eigenvalue problems' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-05-15T23:56:23Z, and was closed at 2023-11-03T22:00:30Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'I'm not sure if actually makes sense to merge this code. It probably needs some additional work/testing to make it more robust, but maybe `jax.experimental` would be a suitable incubator? Either way, hopefully it's a useful point of reference.\r\n\r\nxref #3112'\nPR has comments:\n'Very nice. I am interested in automatic differentiation of sparse eigensolvers, but new to JAX -- did you already have something in mind about this? My only point of reference is this recent paper (https://arxiv.org/abs/2001.04121) where they reduce computing the adjoint to solving a low-rank linear problem. Maybe your new CG implementation could be used to that end?' by a NONE of type User on 2020-05-21T18:05:52Z\n'Indeed, if the eigenvalues are unique, you can calculate eigenvalue/eigenvector derivatives in a straightforward fashion. That paper appears to have rediscovered a very old method for efficiently calculated eigenvector derivatives when not all eigenvectors are known, e.g., see this 1976 paper: https://arc.aiaa.org/doi/abs/10.2514/3.7211\r\n\r\nThere's actually a rather large literature on this problem from the field of computational mechanics. There are some other formulations that are supposed to be more numerically stable, e.g., so you don't need to solve a non-full rank system of linear equations: https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.895. I suspect the resulting linear equations are not necessarily positive, so we would more sophisticated iterative solvers such as (L)GMRES.\r\n\r\nThe challenge is how to define derivatives in the case of degenerate eigenvalues. In that case the eigenvalue problem necessarily isn't a well defined function, so calculating derivatives is quite challenging (maybe even impossible in reverse mode). But we probably don't need to worry this for a first-pass solution.' by a COLLABORATOR of type User on 2020-05-21T20:43:00Z\n'This has become stale - I'm going to close. Feel free to re-open if you'd like to land this in `jax.scipy.sparse.linalg`' by a COLLABORATOR of type User on 2023-11-03T22:00:30Z\n\nPR has review comments:\n'nit: change to ``carry`` to match docs?' by a COLLABORATOR of type User on 2020-05-28T21:47:28Z\n'Maybe better/clearer to use ``Q_valid = Q.at[:, :i].set(0)``? Also, perhaps add a TODO to use masking when available.' by a COLLABORATOR of type User on 2020-05-28T21:53:11Z\n'A comment describing the meaning of each of these variables would be helpful.' by a COLLABORATOR of type User on 2020-05-28T22:02:16Z\n'My experience with lanczos solvers is that they perform poorly for finding the smallest eigenvalues except in shift-invert mode, in which the smallest eigenvalues are transformed to the largest.\r\n\r\nHave you tried modifying this to find the largest eigenvalues?' by a COLLABORATOR of type User on 2020-05-28T22:27:28Z\n'I think we have `self.assertArraysEqual` now' by a COLLABORATOR of type User on 2020-05-28T22:32:43Z\n'`self.assertArraysAllclose`, here and below' by a COLLABORATOR of type User on 2020-05-28T22:33:16Z\n'Hi! I noticed that Krylov vectors are stored in column major format here. Would it make sense to switch to row-major?  Wondering if for large-dimensional vectors, zero-padding on TPUs could lead to an unnecessarily large memory footprint.' by a NONE of type User on 2020-09-30T08:00:05Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 7,
        "reason_for_closure": "Stale :- PR closed due to inactivity and lack of author follow-up."
    },
    {
        "summary": "Pull Request '3034' titled 'Try to put all the requirements in a separate file' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-05-11T11:59:14Z, and was closed at 2021-07-19T08:11:24Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'BTW, there are a couple of other places where the same dependencies are listed in internal google3 code. I plan to refer to the new `requirements.txt` and `requirements_pip_only.txt`' by a COLLABORATOR of type User on 2020-05-11T12:13:36Z\n\nPR has review comments:\n'I'm trying to understand in what use case I need this. If it's to *run* jax, I don't need this. I just `pip install jax jaxlib`, and the dependencies of `jax` are already specified in `setup.py`.\r\n\r\nIf it's to *test* jax, then I usually want a few extra optional dependencies (e.g., `pytest-xdist`, but I'm not required to do so.)\r\n\r\nShould this be `requirements-for-testing.txt` or similar?' by a COLLABORATOR of type User on 2020-05-11T13:18:42Z\n'I have made a `requirements_testing.txt` for building and testing. This is used now by Travis, readthedocs, developer build-from-source instructions. Once this gets merge I will change also the Kokoro builds. ' by a COLLABORATOR of type User on 2020-05-26T07:00:17Z\n'@hawkinsp PTAL' by a COLLABORATOR of type User on 2020-05-26T07:00:37Z\n'I'm confused by this requirement. You shouldn't need `opt_einsum` to build JAX. And if you want it to test JAX, JAX itself depends on it so you don't need to specify it here.' by a COLLABORATOR of type User on 2020-05-26T20:05:08Z\n'I'm wondering if this should be split into a separate `build_requirements.txt` and `test_requirements.txt`, because the set of packages for each are disjoint, as far as I can tell.' by a COLLABORATOR of type User on 2020-05-26T20:06:09Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 5,
        "reason_for_closure": "Here are some reasons for PR closure based on the examples:\n\n* **Inconsistent behavior:** PR introduces discrepancies with established NumPy behavior.\n* **Unnecessary changes:** PR addresses an issue already resolved upstream or by other means.\n* **Misunderstanding:** Miscommunication between author and reviewer regarding desired changes.\n* **Internal API exposure:** PR risks exposing internal APIs, potentially creating future maintenance issues.\n* **Suboptimal implementation:** Proposed implementation inferior to existing solutions. \n* **Redundant change:** PR adds functionality that is not needed or duplicates existing features.\n* **Breaking change:** PR introduces a breaking change without sufficient justification.\n* **Incorrect fix:** PR attempts to fix a bug but introduces new issues or doesn't address the root cause.\n* **Incomplete implementation:** PR lacks crucial functionality or testing to be considered complete.\n* **Abandoned:** Author abandoned the PR due to changes in project direction or other reasons."
    },
    {
        "summary": "Pull Request '2943' titled 'Implemented numpy setxor1d' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-05-03T19:50:43Z, and was closed at 2020-07-14T18:35:35Z by a User.\nThe PR has labels: cla: no - . \nIt has a body of 'This is an implementation of [numpy.setxor1d](https://docs.scipy.org/doc/numpy/reference/generated/numpy.setxor1d.html) from issue #70 (and #2078). It follows the same structure as the original implementation with changes for Jax compatibility.\r\n\r\nnp.setxor1d is intrinsically hard to make jit-compatible since it depends on np.unique and its output shape depends on the values in the array. This problem will be resolved as soon as np.unique is jit-compaitible (#2760). \r\n\r\nThe test check all one dimensional arrays and number dtypes which are not complex (as by #2760, unique does not work with complex dtypes). The test does not compile with jit since unique is not compatible with it.'\nPR has comments:\n'Okay, thank you for your feedback! I’m currently working on it and will finish it until the weekend!' by a CONTRIBUTOR of type User on 2020-05-21T15:16:25Z\n'The tests are running smooth on my local machine but I am getting errors here which seem to be outside of my code. Can you help me with them?' by a CONTRIBUTOR of type User on 2020-05-23T21:22:18Z\n'As with #2941, you need to rebase on master, fix tests locally, and then force push to update this PR. Please let me know if you need help with that process.' by a COLLABORATOR of type User on 2020-05-24T14:37:46Z\n'``rng_factory`` now requires ``self.rng()`` as the first argument - take a look at how other test routines use it.' by a COLLABORATOR of type User on 2020-05-28T20:20:09Z\n'Okay, thank you!' by a CONTRIBUTOR of type User on 2020-05-28T20:45:15Z\n'Running the tests here gives me the error \"NotImplementedError: Numpy function <function setxor1d at 0x7fae7fd63ea0> not yet implemented\". Do you know how that can happen?' by a CONTRIBUTOR of type User on 2020-05-28T21:27:53Z\n'> Running the tests here gives me the error \"NotImplementedError: Numpy function <function setxor1d at 0x7fae7fd63ea0> not yet implemented\"\r\n\r\nYou'll need to add the function to the top-level imports here: https://github.com/google/jax/blob/e48a4e012bd253428e3bb0dd4ac5b163439a5270/jax/numpy/__init__.py#L20-L59' by a COLLABORATOR of type User on 2020-05-28T22:08:49Z\n'Okay, thank you! ' by a CONTRIBUTOR of type User on 2020-05-29T12:44:54Z\n'Looks like the remaining error is a dtype mismatch in x64 mode. I'd suggest passing ``check_dtypes=False`` in the test function.' by a COLLABORATOR of type User on 2020-05-29T16:47:36Z\n'Okay, thank you!' by a CONTRIBUTOR of type User on 2020-05-29T17:42:04Z\n'Looks good - please also remove the two ``.orig`` files that were added along the way.' by a COLLABORATOR of type User on 2020-05-29T17:43:47Z\n'Okay, this has happened now!' by a CONTRIBUTOR of type User on 2020-05-29T19:49:27Z\n'Okay, incorporated changes!' by a CONTRIBUTOR of type User on 2020-05-29T20:28:06Z\n'I just looked a bit closer at the numpy implementation: this seems to be essentially a line-for-line copy of the `setxor1d` code in numpy. That's fine, but for licensing purposes we'll need to move the implementation into ``jax/third_party/numpy`` before we can merge it.' by a COLLABORATOR of type User on 2020-05-29T21:06:30Z\n'Okay, that makes sense! Should I create a file `lax_numpy.py` similar to the structure in `jax/numpy` as there is only `linalg.py` in `jax/third_party/numpy`?' by a CONTRIBUTOR of type User on 2020-06-03T14:26:45Z\n'Sure, you can create `jax/third_party/numpy/lax_numpy.py`, reflecting the structure of the main package.' by a COLLABORATOR of type User on 2020-06-03T17:27:54Z\n'Looks great! The last thing we need is a rebase against master to fix the conflicts that have cropped up over the last few days. Please let me know if you need tips on how to do that.' by a COLLABORATOR of type User on 2020-06-10T04:08:55Z\n'All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F2943) for more info**.\n\n<!-- need_author_consent -->' by a COLLABORATOR of type User on 2020-07-04T23:49:05Z\n'Hi – looks like something is strange with the rebase. If you need help getting your branch back in order, please let me know what commands you ran.' by a COLLABORATOR of type User on 2020-07-06T18:03:38Z\n'Since there has been no action here for a while, I'm going to close this. Feel free to re-open if you'd like to re-visit this contribution! Thanks' by a COLLABORATOR of type User on 2020-07-14T18:35:35Z\n\nPR has review comments:\n'This will fail if an input is a Python scalar rather than a NumPy/JAX scalar.' by a CONTRIBUTOR of type User on 2020-05-04T22:17:45Z\n'This relies on the in-place behavior of NumPy `sort`; you want to use something like `aux = sort(aux)` for JAX compatibility.' by a CONTRIBUTOR of type User on 2020-05-04T22:18:32Z\n'We should not call `asarray`; valid inputs will be properly handled by `concatenate`.' by a COLLABORATOR of type User on 2020-05-29T20:18:57Z\n'Use two-space indent to match the rest of the file.' by a COLLABORATOR of type User on 2020-05-29T20:19:14Z\n'Since there's no complilation test, we can simplify to\r\n```\r\nnp_fun = np.setxor1d\r\njnp_fun = jnp.setxor1d\r\n```' by a COLLABORATOR of type User on 2020-05-29T20:20:17Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 20,
        "num_review_comments": 5,
        "reason_for_closure": "Stale :- No activity for a while, author unresponsive."
    },
    {
        "summary": "Pull Request '2941' titled 'Numpy union1d' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-05-03T19:40:44Z, and was closed at 2020-07-14T18:35:59Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This is an implementation of [numpy.union1d](https://docs.scipy.org/doc/numpy/reference/generated/numpy.union1d.html) from issue #70 (and #2078). It follows the same structure as the original implementation with changes for Jax compatibility.\r\n\r\nnp.union1d is intrinsically hard to make jit-compatible since it depends on np.unique and its output shape depends on the values in the array. This problem will be resolved as soon as np.unique is jit-compaitible (#2760). \r\n\r\nThe test check all one dimensional arrays and number dtypes which are not complex (as by #2760, unique does not work with complex dtypes). The test does not compile with jit since unique is not compatible with it.'\nPR has comments:\n'Okay, thank you for your feedback! I will incorporate them and finish it on the weekend!' by a CONTRIBUTOR of type User on 2020-05-21T15:17:44Z\n'Incorporated above changes!' by a CONTRIBUTOR of type User on 2020-05-23T20:50:08Z\n'The tests are running smooth on my local machine but I am getting errors here which seem to be outside of my code. Can you help me with them?' by a CONTRIBUTOR of type User on 2020-05-23T21:22:12Z\n'We are in the process of migrating from using ``import numpy as onp; import jax.numpy as np`` to using ``import numpy as np; import jax.numpy as jnp``, and this PR is a bit caught in the middle.\r\n\r\nThe best thing to do would be to rebase on master, update your use of ``onp`` so the tests pass locally, and then force push to this branch (let me know if you're not clear on how to do that).' by a COLLABORATOR of type User on 2020-05-24T14:30:23Z\n'Looks good - please also remove the two ``*.orig`` files that were added along the way.' by a COLLABORATOR of type User on 2020-05-29T17:44:10Z\n'Okay, this has happened now!' by a CONTRIBUTOR of type User on 2020-05-29T19:47:59Z\n'As with #2943, we'll need a rebase against master before we can merge this.' by a COLLABORATOR of type User on 2020-06-11T21:34:43Z\n'Hello - are you still interested in finishing this contribution?' by a COLLABORATOR of type User on 2020-07-08T22:54:24Z\n'Since there has been no action here for a while, I'm going to close this. Feel free to re-open if you'd like to re-visit this contribution! Thanks' by a COLLABORATOR of type User on 2020-07-14T18:35:59Z\n\nPR has review comments:\n'The numpy function accepts multi-dimensional inputs:\r\n```python\r\n>>> np.union1d(np.arange(9).reshape(3, 3), 10)                              \r\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10])\r\n```\r\nIt seems like the easiest way to handle this, as well as the zero-dim case, would be to call ``ravel()`` on both inputs' by a COLLABORATOR of type User on 2020-05-03T21:19:19Z\n'``float_dtypes or int_dtypes`` is probably not doing what is intended. You can use ``default_dtypes`` to get the union of `float_dtypes` and `int_dtypes`.' by a COLLABORATOR of type User on 2020-05-03T21:23:16Z\n'These lambdas are a no-op, so you can drop them and use ``onp.union1d`` and ``jnp.union1d`` directly.' by a COLLABORATOR of type User on 2020-05-03T21:24:17Z\n'Consider also testing multi-dimensional array shapes, as these are supported by numpy's version.' by a COLLABORATOR of type User on 2020-05-03T21:24:22Z\n'Remove `asarray` in both cases here; `ravel()` already handles all valid inputs.' by a COLLABORATOR of type User on 2020-05-29T20:21:50Z\n'Okay!' by a CONTRIBUTOR of type User on 2020-05-29T20:25:01Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 9,
        "num_review_comments": 6,
        "reason_for_closure": "Inconsistent behavior :- PR closed as it would break existing users and deviate from JAX's superset nature."
    },
    {
        "summary": "Pull Request '2835' titled 'Refactor xla_bridge.py to do all backend caching itself.' was authored by a User, who is associated as a MEMBER. \nIt was created at 2020-04-25T01:53:05Z, and was closed at 2021-05-26T21:08:52Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This is working towards removing backend caching from the XLA client, making jax responsible for caching instead. This will ultimately simplify the backend creation and caching logic.\r\n\r\nPrior to this change, each backend factory was expected to memoize its returned backends. `xla_bridge.get_backend()` was already memoized, but the extra caching layer was necessary to make `get_backend(None)` and `get_backend(<default backend string>)` return the same Backend object.\r\n\r\nThis change achieves the same effect in xla_bridge alone via the following changes:\r\n\r\n1. Creating specific backend factories for every platform in xla_bridge, instead of the rough groupings denoted by the `--jax_xla_backend` flag. This allows us to move the default backend logic to xla_bridge.\r\n\r\n2. Initializing all registered backends on the first get_backend() call. This allows xla_bridge to know which backends are available, to pick a default and warn if no GPU/TPU is available.'\nPR has comments:\n'Marking as draft until I can verify this will work inside google as well.' by a MEMBER of type User on 2020-04-25T01:53:43Z\n'Closing; https://github.com/google/jax/pull/6829 will replace this PR.' by a COLLABORATOR of type User on 2021-05-26T21:08:52Z\n\nPR has review comments:\n'Why is `_backends` an `OrderedDict`, if you are inserting the backends in the order of `.items()` on a regular dictionary (i.e., in no particular order)?\r\n\r\nI suspect you intended `_backend_factories` to be an ordered dict also?' by a COLLABORATOR of type User on 2020-06-01T19:24:52Z\n'This message string seems misleading given you call this function from multiple places.' by a COLLABORATOR of type User on 2020-06-01T22:03:49Z\n'Should this just be:\r\n```\r\n# TODO(skye): remove the name parameter\r\n```\r\nbecause there's no condition on it, it just needs doing now.' by a COLLABORATOR of type User on 2020-06-01T22:08:16Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 3,
        "reason_for_closure": "Superseded by other work :- another approach has been taken or the work is no longer needed."
    },
    {
        "summary": "Pull Request '2829' titled 'refactor how map and call primitives are tagged' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-04-24T19:47:47Z, and was closed at 2020-06-18T21:03:13Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This seems a little less arcane. Includes the commit from #2828, but I figured I'd factor out this cleanup.'\nPR has comments:\n'I'm not sure I like `staged_out` as a tag name here. I'm guessing it means \"outer `JaxprTrace`s will stage out instances of this primitive in their entirety, rather than partially-evaluating their bodies\" (right?)—but it took me a while to realize that was what this probably referred to. Maybe `always_staged_out` or `fully_stage_out`?' by a CONTRIBUTOR of type User on 2020-04-25T04:01:16Z\n'I think it’s a property of the primitive, whether its impl executes in Python or if it’s staged out of Python. Nothing on its own to do with tracing. But those are the primitives for which we would hoist as much as possible into them, hence the interaction with StagingJaxprTrace. ' by a COLLABORATOR of type User on 2020-04-25T04:10:09Z\n'OK, that makes sense! Thanks for explaining.' by a CONTRIBUTOR of type User on 2020-04-25T05:58:52Z\n'I had some follow-up changes in mind to try out, like either\r\n\r\n1. factoring out `xla_call_jaxpr`/`pmap_call_jaxpr` primitives that work like the other initial-style higher-order primitives so that there's no need for a special case analysis in jaxpr interpreters, or else\r\n2. cleaning up the representation / case analysis so that instead of relying on `extract_call_jaxpr` we have special `JaxprCallEqn` / `JaxprMapEqn` classes which have `call_jaxpr` and `call_jaxpr`/`mapped_invars` fields respectively.\r\n\r\nI implemented 2 already (in an un-pushed commit), but want to try 1 as it now seems more promising.\r\n\r\nDo you think we should merge this PR now-ish, and make more changes in follow-ups? Or fuse all such changes together? The latter might be slightly nicer for any jaxpr consumers, since they won't have to make multiple changes, but it also means making more changes at once (and increasing the chance I run out of fuel on this).\r\n\r\nIn any case, @gnecula if you have time I plan to wait for your review / LGTM, since I could really benefit from your advice here. Don't hold back: I'm happy to make changes!' by a COLLABORATOR of type User on 2020-04-28T00:36:41Z\n'I'm going to replace this with a fresh PR!' by a COLLABORATOR of type User on 2020-06-18T21:03:13Z\n\nPR has review comments:\n'Thoughts on whether rules for HOPs (like the transpose rule, but also process_call/process_map) should treat `call_jaxpr` as just another part of `params` now?' by a CONTRIBUTOR of type User on 2020-04-24T21:33:07Z\n'Oh hmm. An alternative would be to create a special Eqn for these which has a jaxpr slot, right? That seems better...' by a COLLABORATOR of type User on 2020-04-25T02:16:38Z\n'Or, I guess, a special non-params slot on the primitive. Maybe that's what you were thinking too?' by a COLLABORATOR of type User on 2020-04-25T02:22:09Z\n'Ah, that doesn't make sense: there's only one instance of the primitive. Params are really things we bundle with primitives in a jaxpr eqn. So I think we'd need special eqns.\r\n\r\nI'll give that a shot!' by a COLLABORATOR of type User on 2020-04-25T02:24:49Z\n'I just meant that George's vision was to make `call_jaxpr` just another param; that would work for transpose and translation (plus any other initial style transforms) but (of course!) not for final style transforms. So I think my comment was mostly off base.' by a CONTRIBUTOR of type User on 2020-04-25T03:48:19Z\n'Yeah but I think it’s not ideal as a param because then we have some invariants to enforce/check everywhere rather than having those invariants built into the data structure. ' by a COLLABORATOR of type User on 2020-04-25T04:11:49Z\n'The reason I thought that putting call_jaxpr in params is that it makes it easier to explain the Jaxpr language to users, since the initial style primitives already have Jaxprs in params. Perhaps we can take all the subjaxprs from params and keep them separate, with only `cond` having two of them?  ' by a COLLABORATOR of type User on 2020-04-25T07:33:19Z\n'This is a nice change' by a COLLABORATOR of type User on 2020-04-25T07:34:15Z\n'That's a great point.\r\n\r\nI talked with @gnecula a bit more this morning and he made the point that we could (and likely should) treat these higher-order primitives more uniformly. In particular, we can treat staged-out xla_call and xla_pmap like the other higher-order primitives and not need a special case in jaxpr interpreters for them. We had some more specific ideas but basically our conclusion was to try out some things and see how they looked.' by a COLLABORATOR of type User on 2020-04-28T00:28:37Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 9,
        "reason_for_closure": "Superseded :- PR closed in favor of maintaining superset functionality over NumPy."
    },
    {
        "summary": "Pull Request '2790' titled 'Add random.shuffle_independent() and deprecate axis argument to shuffle()' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-04-22T02:32:51Z, and was closed at 2020-04-29T13:46:23Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Addresses #2066'\nPR has comments:\n'Closing this in favor of a simpler approach: https://github.com/google/jax/issues/2066#issuecomment-621216589' by a COLLABORATOR of type User on 2020-04-29T13:46:23Z\n\nPR has review comments:\n'should this be?\r\n```suggestion\r\n      :func:`~jax.random.shuffle_independent`. If `None`, subarrays are shuffled as a group.\r\n```' by a COLLABORATOR of type User on 2020-04-28T20:16:18Z\n'Should this be `random.permutation`, for consistency with NumPy?' by a COLLABORATOR of type User on 2020-04-28T20:19:19Z\n'Long term, do we need support for shuffling in groups at all?\r\n\r\n- NumPy seems to get along without it OK, just creating explicit arrays to use with indexing\r\n- We have `vmap`, so arguably the idiomatic way to shuffle a bunch of arrays together is to `vmap` but not batch the key.' by a COLLABORATOR of type User on 2020-04-28T20:21:19Z\n'is there a reason why we should use the builtin DeprecationWarning or FutureWarning?' by a COLLABORATOR of type User on 2020-04-28T20:21:46Z\n'Done' by a COLLABORATOR of type User on 2020-04-29T03:59:52Z\n'Oops, wrong comment. I think permutation is different... numpy doesn't have any function to shuffle each subarray independently along an axis. I suppose an alternative would be to drop this functionality entirely.' by a COLLABORATOR of type User on 2020-04-29T04:01:07Z\n'I avoided DeprecationWarning because it is hidden by default, so not very useful to users. FutureWarning would be fine, though.' by a COLLABORATOR of type User on 2020-04-29T04:01:43Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 7,
        "reason_for_closure": "Closed as not useful: PR functionality considered unnecessary, redundant, or breaks existing behavior."
    },
    {
        "summary": "Pull Request '2782' titled 'Minor dedupe of code in `partial_eval.py`' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-04-21T10:48:42Z, and was closed at 2023-03-04T18:09:36Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'By the way, I think we could just make all primitives have multiple results, by making single-result primitives just return a singleton tuple. That would remove the `Primitive.multiple_results` tag and all the code that switches on its behavior. I'm not sure how that would play out, and it might be a significant amount of work, but I thought I'd mention it in case `multiple_results` is annoying to you too!' by a COLLABORATOR of type User on 2020-04-21T18:45:44Z\n'> By the way, I think we could just make all primitives have multiple results, by making single-result primitives just return a singleton tuple. That would remove the `Primitive.multiple_results` tag and all the code that switches on its behavior. I'm not sure how that would play out, and it might be a significant amount of work, but I thought I'd mention it in case `multiple_results` is annoying to you too!\r\n\r\nI suspect that would end up being a very big change!' by a COLLABORATOR of type User on 2020-04-28T16:48:08Z\n'Are we still hoping to merge this change? (It has conflicts against head, if nothing else.)' by a COLLABORATOR of type User on 2020-06-23T14:09:07Z\n'I don't know if anyone cares to merge it but I've fixed the conflict!' by a COLLABORATOR of type User on 2020-06-24T10:10:35Z\n'@mattjj should we merge this?' by a COLLABORATOR of type User on 2020-06-24T14:37:42Z\n'Closing as stale.' by a COLLABORATOR of type User on 2023-03-04T18:09:36Z\n\nPR has review comments:\n'How about we tweak it so that instead of defining `mak_output_tracer(out_avals : List[AbstractValue])` we define a `make_output_tracer(out_aval: AbstractValue)` and then write\r\n\r\n```python\r\nif primitive.multiple_results:\r\n  return map(make_output_tracer, out_aval)\r\nelse:\r\n  return make_output_tracer(out_aval)\r\n```\r\n\r\nWDYT? Then we can avoid the `f([x])[0]` pattern at the bottom.' by a COLLABORATOR of type User on 2020-04-21T18:42:00Z\n'I don't think that's possible, as all output tracers must share the same `eqn`.' by a COLLABORATOR of type User on 2020-04-28T16:44:21Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 2,
        "reason_for_closure": "Stale :- PR closed due to inactivity and loss of relevance over time."
    },
    {
        "summary": "Pull Request '2739' titled 'An implementation of id_print with CustomCall' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-04-16T14:50:55Z, and was closed at 2020-05-24T04:34:15Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'The implementation uses CustomCall and works only on CPU for now.\r\n\r\n Behaves like identify function for positional arguments, but prints all arguments.\r\n\r\n The return value is a tuple with the value of `args` or the value of the keyword\r\n  parameter `result` if present. If there is a single positional argument, returns\r\n  just that argument without packing it in a tuple.\r\n\r\n  The positional arguments must be JAX values. The keyword arguments are serialized\r\n  to a string and printed along with the positional arguments. The exception is the\r\n  `result` keyword argument, which must be a JAX value.\r\n\r\n     Usage:\r\n     >>> y = id_print(x * 2)  # prints and returns 2x\r\n     >>> y, z = id_print(x * 2, x * 3)  # prints and returns 2x and 3x\r\n     >>> y = id_print(x * 2, result=y)  # prints 2x and returns y\r\n     >>> y = id_print(x * 2, what='x')  # prints what=x followed by 2x\r\n\r\n  The order of execution is by data dependency: after all the arguments are computed\r\n  and before the result is used. At least one of the returned values must be used\r\n  in the rest of the computation, or else this operation has no effect.\r\n\r\nUpon JAX transformations, the transformed values are wrapped with `id_print`, \r\nand a special `transforms` tuple keyword argument is added with the sequence of\r\ntransformations applied:\r\n\r\n   - For `vmap` the arguments are batched, and transforms=('vmap')\r\n   - For `jvp` there will be an id_print for the primal values, and a separate\r\n      `id_print` for the tangents with `transforms=('jvp')`.\r\n   - For `grad` there will be an `id_print` for the primal values (if needed\r\n          in the computation of `grad` and an `id_print` with the adjoints of the\r\n          results, with transforms=('vjp').'\nPR has comments:\n'I assume this PR is in a draft state and the intent isn't to merge it as is? (If so, mark it as draft?)' by a COLLABORATOR of type User on 2020-04-28T15:57:37Z\n'Closing, we have now an implementation using outfeed. ' by a COLLABORATOR of type User on 2020-05-24T04:34:15Z\n\nPR has review comments:\n'This is a temporary flag to allow Travis to test the Python code, even before the C++ code made it into jaxlib.' by a COLLABORATOR of type User on 2020-04-20T08:44:53Z\n'In Python 3, you can use a real keyword argument between `*args` and `**kwargs`:\r\n```suggestion\r\ndef id_print(*args, result=None, **kwargs):\r\n```' by a COLLABORATOR of type User on 2020-04-21T23:01:36Z\n'Would it be feasible to support strings inline in `*args` instead? The rule might be that strings are dropped from `args` in the default return value, e.g,. so you can write something like `x = id_print('x =', x)` or `x, y = id_print('x =', x, 'y =', y)`.\r\n\r\nUsing the keyword argument for extra strings is a little more awkward to write, and it isn't 100% clear that only strings values are OK. I would sort of expect `id_print(what=x)` to print `what=` followed by the value of `x`.' by a COLLABORATOR of type User on 2020-04-21T23:19:07Z\n'Yes, I'll do this.' by a COLLABORATOR of type User on 2020-04-22T07:28:05Z\n'These are all good suggestions, I will play with them, I suspect that much of this can be added to the API.' by a COLLABORATOR of type User on 2020-04-22T07:29:01Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 5,
        "reason_for_closure": "Closed as superseded by another approach/implementation."
    },
    {
        "summary": "Pull Request '2608' titled 'Add boolean indexing function to jax.ops' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-04-05T12:47:32Z, and was closed at 2023-11-09T00:50:13Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'We sometimes need to perform boolean indexing inside of `jax.jit`, and found that it's useful to have a function which does that while padding the output to the same shape as the original input.\r\nI think that having this function will be useful, even if JAX gains better variable shape support in the future.\r\n\r\nSome things I'm not sure about in this implementation:\r\n - Is `jax.lax.sort_key_val` going to do a stable sort? XLA exposes a parameter for setting the sort to stable, but this is not exposed in `jax.lax.sort*`. I'd be happy to add an additional argument or to default the sort to be stable.\r\n - I can get the code to throw an error saying \"This is a bug in JAX's shape-checking rules; please report it!\". How can I perform proper shape checking, or should I perhaps add the checks to `jax.sort*`?'\nPR has comments:\n'I think the shape checking needs to happen in `lax.sort_key_val`, before calling the `sort_key_val_p`. I'm guessing that's also where you would add the `stable` parameter, which might be as simple as passing a boolean as a keyword (but not I'm not quite confident enough about XLA-Python to tell you for sure).' by a COLLABORATOR of type User on 2020-04-05T18:57:10Z\n'I recently found this pattern very helpful, but I only knew about it because I happened to see this PR. Would it be possible to get this \"official\" implementation checked in so the pattern can be more easily used? If there's anything I can do toward that, I'm happy to help.' by a NONE of type User on 2020-08-24T17:23:25Z\n'I'm cleaning up some old PRs – I think it's an interesting idea to add a function that's something like this, but the JAX package has evolved so much in the meantime that if we do it it probably makes sense to open a new PR. Thanks!' by a COLLABORATOR of type User on 2023-11-09T00:50:13Z\n\nPR has review comments:\n'This is restructured text, not markdown, and I think the backticks need to be doubled or written something like:\r\n```\r\n:code:`x[boolean_mask]`\r\n```' by a COLLABORATOR of type User on 2020-04-28T18:09:38Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 1,
        "reason_for_closure": "Stale :- JAX package has evolved significantly, making the PR outdated.  A new PR is suggested if the functionality is still desired."
    },
    {
        "summary": "Pull Request '2467' titled 'Fixed the documentation of api.linearize' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-03-20T08:59:44Z, and was closed at 2020-03-24T08:45:54Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'I am abandoning this PR.' by a COLLABORATOR of type User on 2020-03-24T08:45:54Z\n\nPR has review comments:\n'Actually, this implementation is much less memory efficient: it requires total memory cost that scales with the depth of the  computation, whereas `vmap`+`jvp` does not. We shouldn't make this change.' by a COLLABORATOR of type User on 2020-03-22T03:03:08Z\n'I don't understand the fix: isn't this just erasing the type information that was previously provided? I think the previous version was better.' by a COLLABORATOR of type User on 2020-03-22T03:03:39Z\n'I got confused by the context, because it talks about applying `f_jvp` multiple times, but the example code was not about `f_jvp`, nor about `linearize`:\r\n\r\n```\r\n  This function is mainly useful if you want to apply `f_jvp` multiple times,\r\n  i.e. to evaluate a pushforward for many different input tangent vectors at the\r\n  same linearization point. Moreover if all the input tangent vectors are known\r\n  at once, it can be more efficient to vectorize using `vmap`, as in::\r\n\r\n    pushfwd = partial(jvp, f, (x,))\r\n    y, out_tangents = vmap(pushfwd, out_axes=(None, 0))((in_tangents,))\r\n```\r\n\r\nI think that the confusing part is the transition at \"Moreover\", because it talks about \"more efficient\", but it refers to memory efficiency, while linearize uses more memory but saves computation cost. ' by a COLLABORATOR of type User on 2020-03-22T06:35:11Z\n'Indeed, I will revert this' by a COLLABORATOR of type User on 2020-03-22T06:38:54Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 4,
        "reason_for_closure": "Incorrect behavior change: PR abandons support for complex dtypes."
    },
    {
        "summary": "Pull Request '2357' titled 'Fix softplus grads at 0 v2' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-03-05T02:16:06Z, and was closed at 2020-03-10T16:35:35Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'https://github.com/google/jax/issues/2107\r\n\r\nI'm trying to see if XLA max implements:\r\n- IEEE max: like fmax/fmaxf/fmaxl/std::fmax/std::fmaxf/std::fmaxl/np.maximum\r\n- Python max: like std::max\r\n\r\nAlternative implementation: https://github.com/google/jax/pull/2356'\nPR has comments:\n'This fixes https://github.com/google/jax/issues/2107 but now complex numbers will return even worse results.\r\nIn addition, complex NaN handling (where real or imaginary could be NaN independently) looks broken. Is that okay? It looks like we don't care.' by a CONTRIBUTOR of type User on 2020-03-05T03:09:00Z\n'Hey, did you take a look at the discussion on #2195 ?' by a CONTRIBUTOR of type User on 2020-03-08T12:22:22Z\n'> Hey, did you take a look at the discussion on #2195 ?\r\n\r\nThanks, I'll comment there. For now, I've been initializing my softplus to 1e-4.' by a CONTRIBUTOR of type User on 2020-03-09T23:10:37Z\n'Thanks, @joaogui1, @jekbradbury, and @mattjj for explaining why this doesn't work. I'll take another look at this.' by a CONTRIBUTOR of type User on 2020-03-10T16:00:03Z\n'Actually, since the custom grad is coming, I'll just close this PR.' by a CONTRIBUTOR of type User on 2020-03-10T16:35:35Z\n\nPR has review comments:\n'If we don't care about NaN, we can just assume this is true.' by a CONTRIBUTOR of type User on 2020-03-05T03:18:32Z\n'Can you explain why the new value is more correct than the old one?' by a CONTRIBUTOR of type User on 2020-03-09T23:47:40Z\n'It should be the second derivative of max(x,0)**2, which is either 2 or 0.\r\nAlso, around x=0, the derivative of max(x,0) appears to be 1:\r\n```\r\nIn [1]: np.maximum(-0.0, 0.0)                                                                                                                       \r\nOut[1]: -0.0\r\n\r\nIn [2]: np.maximum(0.0, 0.0)                                                                                                                        \r\nOut[2]: 0.0\r\n```' by a CONTRIBUTOR of type User on 2020-03-10T01:55:51Z\n'Well, x \\mapsto max(x, y) is not differentiable at x = y. So we could error, which isn't usually what users expect, or choose some reasonable convention for it. From x < y the directional derivative is 0 and from x > y the directional derivative is 1, and so any value in [0 1] is reasonable by analogy to the subgradient (since max on its own is a convex function). For symmetry with y \\mapsto max(x, y) set the result to 0.5.\r\n\r\nThis numerical test captures the behavior we want, so we shouldn't change its numerical values.' by a COLLABORATOR of type User on 2020-03-10T13:34:02Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 4,
        "reason_for_closure": "Closed as unnecessary: PR deprecates existing functionality without clear benefit."
    },
    {
        "summary": "Pull Request '2307' titled 'Add ValueError to jax.numpy.squeeze function.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-02-25T22:04:42Z, and was closed at 2020-04-28T19:49:34Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Like numpy.squeeze function, if the value given to the axis is not equal to 1, it raises a ValueError.\r\nRelated issue: #2284'\nPR has comments:\n'Thanks for the PR! I think #2470 fixed this case; closing.\r\n\r\nPlease reopen if I've misunderstood!' by a COLLABORATOR of type User on 2020-04-28T19:49:34Z\n\nPR has review comments:\n'```\r\nif any(shape(a)[x] != 1 for x in axis):\r\n  raise ValueError(msg)\r\n```\r\n\r\nWe like to minimize vertical space in JAX, and you can save a line here.' by a COLLABORATOR of type User on 2020-02-27T20:31:58Z\n'Couldn't you avoid duplicating this check by simply waiting until after the `axis = (axis,)` call below and then making the test unconditional?\r\n\r\nThis would also allow you to move `msg` closer to its use.' by a COLLABORATOR of type User on 2020-02-27T20:33:07Z\n'How can I check int and tuple same time?' by a CONTRIBUTOR of type User on 2020-03-05T23:02:09Z\n'I mean you should write something like this:\r\n\r\n```\r\naxis = (axis,) if isinstance(axis, int) else axis\r\nif any(shape(a)[x] != 1 for x in axis):\r\n  raise ValueError(\"...\")\r\n```' by a COLLABORATOR of type User on 2020-03-06T16:20:57Z\n'Okey! We are already converting to tuple if integer so two different queries are meaningless. I'm sorry to miss this. Thank you!' by a CONTRIBUTOR of type User on 2020-03-17T02:13:42Z\n'this solution is a little incomplete: because of the \"if 1 not in shape(a):\" block, it will miss wrong axis specifications when there's no singleton dimension at all' by a CONTRIBUTOR of type User on 2020-03-19T18:44:36Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 6,
        "reason_for_closure": "Inconsistent behavior :- PR attempts to match numpy but JAX supersedes it.  Closing."
    },
    {
        "summary": "Pull Request '2298' titled 'Add GRU & LSTM layer to stax' was authored by a User, who is associated as a NONE. \nIt was created at 2020-02-24T14:40:25Z, and was closed at 2021-04-12T16:10:25Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Addresses issue #644. Adds a GRU & LSTM layer that is batch processing compatible and compiles rapidly. I first tried to work with pull request #1196 but found hick-ups regarding batch processing and compilation time (no lax.scan).'\nPR has comments:\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F2298) for more info**.\n\n<!-- need_author_cla -->' by a COLLABORATOR of type User on 2020-02-24T14:40:34Z\n'@googlebot I fixed it.' by a NONE of type User on 2020-02-24T14:44:41Z\n'CLAs look good, thanks!\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F2298) for more info**.\n\n<!-- ok -->' by a COLLABORATOR of type User on 2020-02-24T14:44:47Z\n'Interested in the GRU implementation. Will the merge occur soon?' by a NONE of type User on 2020-04-15T18:35:23Z\n'I was reviewing the proposed LSTM merge. It seems the output gate is not being used?\r\n![image](https://user-images.githubusercontent.com/4187522/114323555-bc312580-9af3-11eb-97ab-457d92936bc1.png)\r\n\r\nUnless... I'm looking at the wrong version of the code.\r\nhttps://github.com/google/jax/blob/c463fe09d26497ede02e04d629ab87e3120cc529/jax/experimental/stax.py' by a NONE of type User on 2021-04-11T22:30:59Z\n'As the [README for JAX experimental](https://github.com/google/jax/tree/master/jax/experimental) states: \"we generally will not merge PRs adding new features. Instead, please send your lovely pull requests and design ideas to more fully-featured libraries like Haiku, Flax, or Trax.\"\r\n\r\nBoth Haiku and Flax include GRU & LSTM modules -- I recommend giving them a try.' by a COLLABORATOR of type User on 2021-04-12T16:10:25Z\n\nPR has review comments:\n'I think it would be useful if you had the option to either learn the initial state (which is what this code does), zero it (which is the keras default for stateless) or use some sort of state initialisation function f(inputs)? Also perhaps support keras state-full where each batch uses the state from the previous.' by a NONE of type User on 2020-02-24T21:54:04Z\n'I raise this because I'm specifically researching state initialisation for RNN's (dynamical models specifically) - there are a number of options and the correct strategy has an impact on training - no doubt because the wrong initial state can results in unstable dynamics. [1] indicates that learning the initial state per batch can result in overfitting, they use a state estimator network instead.\r\n\r\n[1] https://dx.doi.org/10.1109/ijcnn.2017.7966138' by a NONE of type User on 2020-02-24T22:39:35Z\n'Should this not be output_shape = (input_shape[0], input_shape[1], out_dim)? If I understand correctly the input tensor is (batches,sequences,features) so the output tensor is (batches,sequences,out_dim)?' by a NONE of type User on 2020-02-25T00:51:26Z\n'Thanks David for taking the time! Yes, you are right - I will change that asap.' by a NONE of type User on 2020-02-25T09:55:30Z\n'Yes, that makes sense. I wanted to have a first working version. Adding flexibility sounds totally reasonable. Do you have any recommendations how one can keep the hidden state to be part of the params and not automatically optimize it when applying the opt update to the parameters? I have been searching for a clean way to fix/not optimize parameters for a while.' by a NONE of type User on 2020-02-25T09:58:45Z\n'First what I would change is you've created an initial state per batch. Problem with this is how do you `apply` a trained model to a test set with a different number of batches? I think what you could do is make `hidden = b_init(rng, (out_dim,))` i.e. learn a single (best average) initial state over all batches from the train data. You'll then have to repeat this in apply() by the number of batches.\r\n\r\nI've been thinking a lot about adding flexibility - I've not got a good solution yet. I feel like there should be two collections returned from `init_fun()` - `params` and `states` (and both passed to `apply_fun()`). But I'm still not 100% clear on the best design as there are three general cases in my mind. Learned State (your implementation), Explicit State (i.e. zero) and Estimated State (an additional function - perhaps another jax layer - which estimates the state, this gets complex though as for some (causal) models you want to estimate state based on the previous not future inputs/outputs). \r\n\r\nAlso looking at Keras, they have two RNN modes - stateless and statefull. The former uses constant (zero) initialisation per batch, the second holds the last state in memory (but doesn't learn it - and there's a reset method to zero it). To do this in stax I think you'd have to make states a parameter of `apply_fun` and also return the last state per batch. Then the caller can either pass in or reset as required. The problem is stax layers are single input / single output - you really want to return (outputs,state) and still be able to feed outputs into a Dense layer but have it pass through state.\r\n\r\nSo in short the \"best\" solution will probably require a significant change to the stax api i.e. something like\r\n\r\n```\r\ndef Layer():\r\n  def init_fun(rng, input_shape):\r\n    ...\r\n    return output_shape, params, states\r\n\r\n  def state_fun(params, inputs, **kwargs):\r\n    ...\r\n    return states\r\n\r\n  def apply_fun(params, states, inputs, **kwargs):\r\n    ...\r\n    return outputs, states\r\n\r\n  return init_fun, state_fun, apply_fun\r\n```\r\n\r\n' by a NONE of type User on 2020-02-25T21:40:27Z\n'Also I've figured a potential (hack?) way to either have learned or explicit state initialisation that's clean(ish). If in your `apply_fun` you ignore the state from params and use a value passed in explicitly via args or kwargs the grad wrt the params will be 0 i.e.\r\n\r\n\r\n```\r\n    def apply_fun(params, inputs, **kwargs):\r\n\r\n        states = kwargs.pop(\"states\", None)\r\n        if states is None:\r\n            _,_,state = params\r\n            # need to repeat initial state for each batch\r\n            states = np.repeat(state[np.newaxis,:], inputs.shape[0], axis=0)\r\n        #else:\r\n            # use explicit state initialisation if passed - this will force dL/dparams[-1] == 0 is this stable?\r\n```\r\n' by a NONE of type User on 2020-02-25T22:29:41Z\n'I noticed the other stax layers do this\r\n\r\noutput_shape = input_shape[:-1] + (out_dim,)\r\n\r\nAt first I assumed it just allowed you to type less - but I've realised it's actually allows you to pass an input shape of either (sequences,features) or (batches,sequences,features) - i.e. it gives you a bit more flexibility to train on one or multiple batches which is a nice feature (but then you need to check the input dimensions to infer if you need to swap the leading axis so I probably wouldn't change anything for now)' by a NONE of type User on 2020-02-26T01:49:31Z\n'An additional thought, based on some experimentation I've been doing this week. I think there's value in implementing the RNN layers as \"cells\" i.e. lift the handling of the scan to an outer function. That way the RNNCell `apply_fun` simply operates on a (batches_dim,embeddings_dim) array for a single timestep. The signature would be\r\n\r\n```\r\ndef LSTMCell(...)\r\n  # single RNN cell (states,intputs)->(states,outputs)\r\n  def init_fun(rng, input_shape):  # input_shape is (batches,embeddings)\r\n    ...\r\n    return output_shape, params\r\n\r\n  def apply_fun(params, states, inputs, **kwargs):    # note additional input / output arg\r\n    ...\r\n    return states, outputs\r\n\r\n  return init_fun, apply_fun\r\n```\r\n\r\nNow you can wrap this in a LSTM() layer and do the looping etc. I think this allows the LSTMCell() to be reused more easily - i.e. I want to handle variable sequence lengths, often LSTMCells are stacked etc.\r\n\r\nie. \r\n\r\n```\r\nLSTM(...):\r\n  # LSTM layer contains a single cell\r\n  cell_init, cell_apply = LSTMCell(...)\r\n\r\n  def init_fun(rng, input_shape):  # input_shape is (batches,sequences,embeddings)\r\n    # cell state is a learnt parameter\r\n     initial_cell_state = bias_init(...)\r\n     output_shape=...\r\n     return output_shape,(initial_cell_state,cell_init())\r\n\r\n  def apply_fun(params, inputs, **kwargs):\r\n     # need to swap batch and sequence axis to scan along sequences \r\n     initial_state, parms = params\r\n     inputs = np.swapaxes(inputs, 0, 1)\r\n     scan_fun = partial(cell_apply, params)\r\n     _,outputs = jax.lax.scan(scan_fun, states, inputs)\r\n     outputs = np.swapaxes(outputs, 0, 1)\r\n     return outputs\r\n```\r\n\r\nWhat are your thoughts?' by a NONE of type User on 2020-02-29T23:38:39Z\n'Sorry David for the late response. Yes, making the layer more modular sounds like a good idea. Feel free to add whatever you find adequate. I am currently a little time limited but will return once I find time again!' by a NONE of type User on 2020-03-02T17:58:01Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 10,
        "reason_for_closure": "Here are some concise reasons for closure, similar to the examples provided:\n\n* **Inconsistent:** Deviated from standard NumPy behavior unnecessarily.\n* **Compatibility:** Broke existing functionality for complex inputs.\n* **Disruptive:** Deprecation offered no clear benefit, risked user workflows.\n* **Unnecessary:** JAX supersedes NumPy; complex support desirable.\n* **Resolved:** Inaccurate result for complex number now fixed directly. \n* **Superseded:**  Alternative solution implemented instead of the PR."
    },
    {
        "summary": "Pull Request '2210' titled 'enable array literals (not just scalars)' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-02-11T05:12:25Z, and was closed at 2023-11-09T00:47:35Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Previously we only included scalars as literals in jaxprs. This PR removes that constraint, and allows arrays of any size to be included in jaxpr literals.\r\n\r\nThe reason for this change is to fix an old issue where array constants in pmapped computations were broadcasted to have a leading axis equal to the mapped axis size, e.g. from @hawkinsp:\r\n\r\n```python\r\nimport jax\r\nimport jax.random\r\nimport numpy as np\r\n\r\nx = jax.random.uniform(jax.random.PRNGKey(0), shape=(1000000, 10))\r\nf = jax.pmap(lambda i: x[i])\r\n\r\nprint(jax.xla_computation(f)(np.arange(12)).GetHloText())\r\n```\r\n\r\n```\r\n$ XLA_FLAGS=\"--xla_force_host_platform_device_count=12\" python\r\n\r\nENTRY xla_computation_pmap__lambda___axis_name__axis_0x7fd36de1a2f0__.56 {\r\n  ...\r\n  constant.1 = f32[12,1000000,10]{2,1,0} constant({...})\r\n  ...\r\n```\r\n\r\nThe issue was that [we got consts for a staged-out pmap as outputs from a pmap bind](https://github.com/google/jax/blob/28e802c6f15d7ed87fee9bee6270137423b9967c/jax/interpreters/partial_eval.py#L151-L153), and outputs from a pmap always have a leading axis size equal to the mapped axis size (i.e. constants are broadcast). This PR changes it so that array constants aren't produced as outputs but instead immediately hoisted into the jaxpr being formed as literals. (An alternative solution would be to allow non-mapped outputs from a pmapped function, which we might want anyway, but this seems simpler for handling array constants.)\r\n\r\nSince all JAX types are currently literalable types, I can't think of a way for the consts field to be populated now. I thought closing over outer tracers would do it, like this:\r\n\r\n```python\r\n@jit \r\ndef f(x):\r\n  @jit \r\n  def g():\r\n    return x\r\n  return g()\r\n```\r\n\r\nbut that instead populates `env`, which means it now gets closure-converted into the formal parameters.\r\n\r\nThis choice can affect serialization, since jaxprs themselves may contain large array constants (without further processing).\r\n\r\nTODO\r\n- [ ] attempt to de-duplicate large literals with subjaxprs'\nPR has comments:\n'Thanks for the feedback! I should've noted that this PR is a bit of a draft at the moment, and so I'm looking for early feedback.\r\n\r\n> Why were the constants hoisted out of the computation in the first place? Is that reason still applicable?\r\n\r\nThere wasn't a good reason; it was just an artifact of the first way we wrote things.\r\n\r\nOne benefit we realized after the fact is that it automatically had the effect of avoiding staging into XLA a large constant into both a subcomputation (like in a while_loop) and the computation that contains it. I may try to solve that more directly on this PR.\r\n\r\n> The motivation and the solution seem to not match.\r\n\r\nCan you say more? They seem to match in my head, but maybe I didn't explain it well!' by a COLLABORATOR of type User on 2020-02-11T16:44:28Z\n'> > The motivation and the solution seem to not match.\r\n> \r\n> Can you say more? They seem to match in my head, but maybe I didn't explain it well!\r\n\r\nWhat I meant was that the problem to solve is to better control how pmap broadcasts inputs and outputs. This suggests that the solution should be some control mechanism for pmap broadcasting. Incidentally, it seems that we now have the mechanism through the `mapped_invars`. \r\n\r\nInstead this solution is changing how we trace Python into Jaxpr and changes even how Jaxpr looks. This could have broader consequences, and also introduces a hidden dependency of pmap on how we trace constants. This reminds me very much of the freevars and how pmap was implicitly taking advantage of them, but it was not possible to see this unless you try to remove freevars. ' by a COLLABORATOR of type User on 2020-02-11T19:46:42Z\n'I agree that if we had both mapped_invars _and_ mapped_outvars that would give us another way of solving this. That’s what I pointed out when I wrote “An alternative solution would be...” Indeed that’s the first solution I thought of!\r\n\r\nBut that solution is heavyweight for this particular issue. Unless we have a good reason to plumb constants as outputs of a pmap, why not revise that instead? And indeed this is a revision to jaxprs, like we’ve been doing a lot of recently :) (I might try removing constants entirely now...)\r\n\r\nThis isn’t introducing a dependency of pmap on how we handle constants; you’ll notice I didn’t change the pmap code, but instead I just changed which things are hoisted as constants versus literals and everything kept working. We could flip a coin to decide whether to treat each value as a hoisted constant or a literal and it would still work. But it’s preferable to treat things as literals because it’s more efficient, hence the PR. \r\n\r\nCan you think of any reasons to prefer the constant-hoisting behavior?\r\n' by a COLLABORATOR of type User on 2020-02-11T20:03:11Z\n'(By the way, handling a pmap with mapped_outvars is tricky because, for example, when doing jvp or partial eval we don’t know until we run the function how many outputs there will be. It’s doable but much heavier than anything in this PR!)' by a COLLABORATOR of type User on 2020-02-11T20:06:10Z\n'To be clear, if you say there were no important reasons to hoist constants, I like the change. If we can remove hoisted consts, I think that Jaxpr will be even simpler :-) \r\n\r\nI see that you did not change the code for pmap, yet you fixed how it behaves. This suggests that the dependency is implicit. I would have thought that with or without hoisting the semantics should be the same. ' by a COLLABORATOR of type User on 2020-02-11T21:43:15Z\n'If you do agree to try to remove the constvars, then I'd be happy to do that. I assume that some work would be needed for the linearity-bitmaps for scan and while, and also maybe something for compilation (perhaps hoisting them out only for compilation)?' by a COLLABORATOR of type User on 2020-02-12T13:46:52Z\n'Closing, as non-scalar literals are now supported!' by a COLLABORATOR of type User on 2023-11-09T00:47:35Z\n\nPR has review comments:\n'Wouldn't this be replicated 10,1000,10, with the first 10 being the leading axis? \r\n\r\nThis test is hard to understand. I think it would be easier if we assert on more context around '1000,10'?' by a COLLABORATOR of type User on 2020-02-11T09:16:08Z\n'This is a typo on my part, and I'm confused why it isn't just crashing! I meant to give it `np.arange(1)`, so this test would run without multiple devices available.' by a COLLABORATOR of type User on 2020-02-11T16:41:30Z\n'I'll revise the checks to have more context.' by a COLLABORATOR of type User on 2020-02-11T16:43:27Z\n'I added more context to the test. As to why the pmap isn't failing when mapping over too large an axis, I believe it's because we've set up `jax.xla_computation` to work even when building computations for more devices than are available.' by a COLLABORATOR of type User on 2020-02-11T17:24:13Z\n'I am curious, what are non core.literalable_types. In other words, what things will still be lifted as consts? ' by a COLLABORATOR of type User on 2020-02-12T13:55:04Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 7,
        "num_review_comments": 5,
        "reason_for_closure": "Here are some 10-15 word summaries of PR closure reasons, similar to the examples provided:\n\n* **Inconsistent:** PR makes JAX diverge from NumPy behavior without sufficient benefit.\n* **Functionality Loss:**  Removing complex dtype support breaks existing JAX users.\n* **Fixed Upstream:** CUDA update addressed the underlying ptxas bug.\n* **Abandoned:** Deviceless AOT functionality achieved without these changes.\n* **No Benefit:** Implicit dependency made explicit; otherwise, no change. \n* **Broken Approach:** PR exposed internal APIs; refactor needed for custom partitioning.\n* **Unclear Goals:** Misunderstanding between reviewer and author on desired changes."
    },
    {
        "summary": "Pull Request '2209' titled 'Make JAX ODE solver work with arbitrary-dimensional functions' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-02-10T22:45:02Z, and was closed at 2020-06-25T23:56:35Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Currently it only accepts vector-valued functions.'\nPR has comments:\n'Thanks!\r\n\r\nI wonder: can a user make use of `vmap` to get this same behavior, without us having to change the odeint code itself?' by a COLLABORATOR of type User on 2020-02-11T01:15:40Z\n'Thanks Matt, yes, I can definitely flatten the function on my end, so feel free to ignore this PR. Added a test in case if you want to include it.' by a CONTRIBUTOR of type User on 2020-02-11T19:42:52Z\n'@mattjj should we close this PR?' by a COLLABORATOR of type User on 2020-06-24T14:39:57Z\n'@hawkinsp yes. I think we let it get stale, and odeint has grown this functionality now.\r\n\r\nThanks for pushing us in the right direction @romanngg!' by a COLLABORATOR of type User on 2020-06-25T23:56:35Z\n\nPR has review comments:\n'Instead of changing existing tests, would it be possible to add new tests?' by a COLLABORATOR of type User on 2020-02-11T01:13:22Z\n'Done! 95d20d24109d86f66f4465014fa040c422fcf05d' by a CONTRIBUTOR of type User on 2020-02-11T19:39:24Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 2,
        "reason_for_closure": "Stale :- PR abandoned as another approach is used."
    },
    {
        "summary": "Pull Request '2200' titled '[WIP ]Implementation of np.linalg.lstsq' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-02-10T08:36:13Z, and was closed at 2020-04-18T12:12:58Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Related to #1999 \r\n\r\n- [x] Make sure the implementation works when rank = m = n and when rank = m != n\r\n\r\n- [x] Write tests'\nPR has comments:\n'Hey @shoyer can you take a look at this? ' by a CONTRIBUTOR of type User on 2020-02-13T08:06:06Z\n'Right now the error is in typechecking, int64 vs int32 and the same for floats, can you point me towards a way to fix this please?' by a CONTRIBUTOR of type User on 2020-02-16T12:16:49Z\n'Ok, so after explicit casting and @IvanYashchuk help, tests are passing @shoyer ' by a CONTRIBUTOR of type User on 2020-03-20T00:03:19Z\n'Hey @mattjj Can you check this out please?' by a CONTRIBUTOR of type User on 2020-03-26T17:41:50Z\n\nPR has review comments:\n'It's better to explicitly pass `dtype` here, because if not given `jax.numpy.dtypes.float_` is used, making your tests against numpy implementation fail on dtype of residuals.\r\n```suggestion\r\n    residuals = np.array([], dtype=a.dtype)\r\n```' by a NONE of type User on 2020-03-19T19:54:02Z\n'NumPy always returns rank of `a` with `numpy.int32` type. Current JAX implementation returns `rank` with dtype `int32` or `int64` based on dtype of `a`. It makes sense not to check dtype for `rank`.' by a NONE of type User on 2020-03-19T20:24:06Z\n'It would be good to add the test case with `rhs_shape = (i, )` (it is different from `(i, 1)`).' by a NONE of type User on 2020-03-19T20:36:38Z\n'Didn't know that, thanks!' by a CONTRIBUTOR of type User on 2020-03-19T20:46:29Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 4,
        "reason_for_closure": "Implemented :- PR closed after successful implementation and testing."
    },
    {
        "summary": "Pull Request '2177' titled 'Add basic support for partitions within JAX.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-02-06T13:07:29Z, and was closed at 2023-03-04T17:56:50Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'Closing as stale.' by a COLLABORATOR of type User on 2023-03-04T17:56:50Z\n\nPR has review comments:\n'Copyright 2020' by a MEMBER of type User on 2020-02-06T22:02:01Z\n'I think this should be jax/interpreters/partition.py, instead of living in lax. Even though it's technically defining primitives, we put other jit-like primitives in interpreters (e.g. xla.py, pxla.py).' by a MEMBER of type User on 2020-02-06T22:06:34Z\n'This should be`@lu.cache`' by a MEMBER of type User on 2020-02-06T22:10:50Z\n'Add spaces around =, no need for parens' by a MEMBER of type User on 2020-02-06T22:12:11Z\n'Do you know if this works on non-TPU backends? You don't need to do it for this change, but consider adding a `backend` argument.' by a MEMBER of type User on 2020-02-06T22:18:29Z\n'When does this happen?' by a MEMBER of type User on 2020-02-06T23:32:24Z\n'How does this work? Don't you need outputs from all replicas?' by a MEMBER of type User on 2020-02-06T23:34:01Z\n'I'm a little worried about adding overheads to the non-partition path... what do you think about keeping the old _execute_compiled and _executed_replicated functions for now, with a TODO to benchmark if there's a noticeable difference?' by a MEMBER of type User on 2020-02-06T23:37:47Z\n'nit: make this a ValueError or something, since presumably we'll never implement this?' by a MEMBER of type User on 2020-02-06T23:39:47Z\n'2020' by a MEMBER of type User on 2020-02-06T23:44:39Z\n'Maybe print the equation or something, some hint as to where the mismatch happened' by a MEMBER of type User on 2020-02-06T23:49:56Z\n'Also inspect which device the result is on? (here and in the following tests)' by a MEMBER of type User on 2020-02-06T23:51:47Z\n'Done' by a COLLABORATOR of type User on 2020-02-07T11:34:52Z\n'Done' by a COLLABORATOR of type User on 2020-02-07T11:35:05Z\n'I haven't tried it on other backends.' by a COLLABORATOR of type User on 2020-02-07T11:35:39Z\n'Done' by a COLLABORATOR of type User on 2020-02-07T11:36:07Z\n'Done' by a COLLABORATOR of type User on 2020-02-07T11:36:52Z\n'Done' by a COLLABORATOR of type User on 2020-02-07T11:37:05Z\n'Done' by a COLLABORATOR of type User on 2020-02-07T11:37:15Z\n'I'm don't think it works with `@lu.cache`. We don't have a function here, it has already been traced. In terms of implementation, `partition` is more similar to the control flow ops than `jit`.' by a COLLABORATOR of type User on 2020-02-07T11:40:49Z\n'Done' by a COLLABORATOR of type User on 2020-02-07T11:43:01Z\n'This is now done in https://github.com/tensorflow/tensorflow/commit/1e5dc679bbc3f2ee1941218ab184dafabafacf95. If we bump the jaxlib version, I can remove this code.' by a COLLABORATOR of type User on 2020-02-07T13:39:35Z\n'I was confused by this too, but this matches the existing behaviour. See the removed `_execute_replicated` function. I imagine that when I get to tackling the `pmap` case, I will change this function to return the outputs from all replicas.' by a COLLABORATOR of type User on 2020-02-07T13:41:27Z\n'I added a comment. I've tried to make this bit of code general enough to support the multi-host case, though that probably doesn't actually make sense right now with all of the inputs on partition 0.' by a COLLABORATOR of type User on 2020-02-07T13:43:00Z\n'Done. Do we have any good benchmarks to try on this?' by a COLLABORATOR of type User on 2020-02-07T14:36:45Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 25,
        "reason_for_closure": "Stale :- PR closed due to inactivity and lack of recent progress."
    },
    {
        "summary": "Pull Request '2175' titled 'Support XLA replication and in/out axes in pmap' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-02-06T09:39:52Z, and was closed at 2023-03-04T17:54:31Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'I had not noticed that this was a Draft PR, perhaps you did not intend to send it for review yet. Still, some of the comments may be helpful though. ' by a COLLABORATOR of type User on 2020-02-06T13:36:43Z\n'Yes, thanks! I definitely want to wait until Skye’s PR (that this is stacked atop) goes in, and I also need to rebase it past your changes (including folding in mapped_invars), but your comments are still helpful.' by a CONTRIBUTOR of type User on 2020-02-06T16:35:25Z\n'Closing as stale.' by a COLLABORATOR of type User on 2023-03-04T17:54:31Z\n\nPR has review comments:\n'I would suggest moving reading the os.env (global state) as far out of the call chain as possible. In this case it seems easy to move it to pmap_shard_args_benchmark.' by a COLLABORATOR of type User on 2020-02-06T12:57:14Z\n'It makes sense for this function to be in a shared module.' by a COLLABORATOR of type User on 2020-02-06T13:00:42Z\n'Why not pass params as a list of dictionaries? Maybe add also a short docstring?' by a COLLABORATOR of type User on 2020-02-06T13:04:37Z\n'I had expected here that f would be invoked with the params. I think this would be clearer if benchmark would be passed both a function and the parameters. To enable the staging of the input data, the benchmark function would be more like the get_benchmark_fn below (signature params -> unit -> unit). WDYT?' by a COLLABORATOR of type User on 2020-02-06T13:08:26Z\n'I think we can also assert something about the indices of `r`. Perhaps instead of asserting the length of `r.device_buffers` assert the value of `d.logical_indices`. Then one can read from this test the sharding behavior. ' by a COLLABORATOR of type User on 2020-02-06T13:16:27Z\n'All this will need to be revisited in light of the fact that `consts` and `freevar_vals` are now part of `args` :-(' by a COLLABORATOR of type User on 2020-02-06T13:19:48Z\n'There is also params['mapped_invars'] that tells you which `args` are mapped over.' by a COLLABORATOR of type User on 2020-02-06T13:20:31Z\n'For functions like this I think that it is worth starting to add unit tests. We do most of our testing with integration tests and it is hard to know what kind of coverage we have. Also, unit tests can be helpful to understand in detail the behavior of this function. ' by a COLLABORATOR of type User on 2020-02-06T13:23:41Z\n'Maybe remove the _ from `_replace_nones`?' by a COLLABORATOR of type User on 2020-02-06T13:24:36Z\n'Please remember to change the CHANGELOG.md' by a COLLABORATOR of type User on 2020-02-06T13:26:38Z\n'Perhaps this can be clarified: \"...but each with an additional array axis at positions...\"' by a COLLABORATOR of type User on 2020-02-06T13:27:38Z\n'Suggest: ... but with each output having an extra array axis as positions indicated by ...' by a COLLABORATOR of type User on 2020-02-06T13:28:32Z\n'I think that is makes sense to merge this \"in_dims\" with the newly added \"mapped_invars\" param. That one is at the moment only added deeper in the code for staged jaxprs, but has overlapping semantics. ' by a COLLABORATOR of type User on 2020-02-06T13:30:10Z\n'And we just take the first element? ' by a COLLABORATOR of type User on 2020-02-06T13:34:12Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 14,
        "reason_for_closure": "Closed as not useful: PR abandoned because it offered little value and broke existing functionality."
    },
    {
        "summary": "Pull Request '2019' titled 'Proper error handling for scalar overflow when converted to 32-bit dtypes' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-01-17T01:01:02Z, and was closed at 2021-04-08T15:53:21Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Addresses #2006 . \r\n\r\nAlso attempting to replace `canonicalize_dtype ` with `coerce_to_array `. Edit: Removing `canonicalize_dtype ` inside `xla.py` introduces errors that i can't fix...giving that up for now. \r\ncc @mattjj \r\n\r\nTODO: Add tests to make sure error is properly caught. '\nPR has comments:\n'@jekbradbury i'm running into some build problems, but i don't know how to debug this one. Could you please take a look ? Thanks! ' by a CONTRIBUTOR of type User on 2020-01-23T05:47:44Z\n'@mattjj just wondering if you could take a look at my build problem! ' by a CONTRIBUTOR of type User on 2020-01-27T17:29:36Z\n'I think this is made obsolete by #6269 - sorry this was lost in the mix!' by a COLLABORATOR of type User on 2021-04-08T15:53:15Z\n\nPR has review comments:\n'This should only include `int32` and `uint32` (and should be specific to the ranges of those two dtypes, which you can get through the `min` and `max` properties of `onp.iinfo(onp.int32)` and `onp.iinfo(onp.uint32)`). `float32` has a much larger range.\r\n\r\nYou also only want to run this check on Python scalars, since numpy arrays are already guaranteed to fit in the limits imposed by their dtype.' by a CONTRIBUTOR of type User on 2020-01-17T02:16:51Z\n'Same here.' by a CONTRIBUTOR of type User on 2020-01-17T02:17:02Z\n'thanks @jekbradbury ! ' by a CONTRIBUTOR of type User on 2020-01-17T07:01:19Z\n'Thanks for the ping.\r\n\r\nI think the problem is a dependency problem, and it has to do with this line: this `dtypes` module needs to be upstream of `jax.numpy`, and so we can't import `jax.numpy` here. Perhaps just use `onp` instead?' by a COLLABORATOR of type User on 2020-01-27T17:42:01Z\n'We also don't want to import `jax.numpy` here. In general, be careful with adding new import statements, since dependencies can be tricky to get right!' by a COLLABORATOR of type User on 2020-01-27T17:42:40Z\n'worked like a charm, thanks @mattjj ! ' by a CONTRIBUTOR of type User on 2020-01-27T19:13:38Z\n'@mattjj I don't see a test for `xla.py`, do you know where I should put unit test for this PR ? ' by a CONTRIBUTOR of type User on 2020-01-27T19:28:50Z\n'@mattjj pinging you again in case you have any suggestions for writing tests in this PR :) ' by a CONTRIBUTOR of type User on 2020-02-05T04:31:39Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 8,
        "reason_for_closure": "Obsolete: PR made redundant by later dependency update."
    },
    {
        "summary": "Pull Request '1992' titled 'Pmap error' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2020-01-13T20:20:32Z, and was closed at 2020-07-14T19:14:25Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Fixes #1991. '\nPR has comments:\n'This is definitely an improvement, thanks! I would go even farther and plumb in some more debugging information to make this more actionable, e.g. which argument index is missing a leading axis? I'm happy to submit this as-is and iterate as well, lemme know what you wanna do.' by a MEMBER of type User on 2020-01-13T20:53:09Z\n'> ! I would go even farther and plumb in some more debugging information to make this more actionable, e.g. which argument index is missing a leading axis?\r\n\r\nThat's a good idea, though I'm not sure the cleanest way of doing that. Ideally, it would point at the exact `pmap` line that failed, but I'm not sure how.\r\n\r\nSomething like:\r\n```\r\njax.pmap(f)(jnp.ones((8, 100)), 10)\r\n                                ^\r\nValueError: Argument #2 is missing a leading axis\r\n```\r\n?\r\n\r\n' by a CONTRIBUTOR of type User on 2020-01-13T20:57:20Z\n'I think you need to plumb the argument indices through into `shard_args` here: https://github.com/google/jax/blob/master/jax/interpreters/pxla.py#L456 (i.e. map over `enumerate(avals)` instead of `avals`, and then plumb the index through to `shard_args` and the underlying handlers). That should let you specify the argument index in the error message, and the stack trace will contain the failing pmap line, which is pretty close!' by a MEMBER of type User on 2020-01-13T21:11:36Z\n'I believe the underlying issue is fixed (see the linked issue.)' by a COLLABORATOR of type User on 2020-07-14T19:14:25Z\n\nPR has review comments:\n'This is causing a syntax error, I think you need a comma on the above line.' by a MEMBER of type User on 2020-01-13T20:51:58Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 1,
        "reason_for_closure": "Underlying issue fixed by dependency update."
    },
    {
        "summary": "Pull Request '1941' titled 'lazy sublanguage part 1' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2020-01-03T23:46:51Z, and was closed at 2020-01-08T04:08:27Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'I was using this PR as a staging ground to pull stuff down into Google. I don't need it anymore though!' by a COLLABORATOR of type User on 2020-01-08T04:08:27Z\n\nPR has review comments:\n'But _removing_ singleton dimensions isn't supported yet? Can that be represented in a lexpr?' by a CONTRIBUTOR of type User on 2020-01-04T23:44:58Z\n'Yes, removing isn't supported. Adding a singleton dimension can also be written as a broadcast, but removing a singleton dimension can't be iiuc.\r\n\r\nI think perhaps we can make them work without changing lexprs, basically by ignoring/flattening them, but I left it as a \"follow-up work\" item in #1668.' by a COLLABORATOR of type User on 2020-01-07T23:29:37Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "No Longer Needed: Author decided to close the PR because it was no longer necessary."
    },
    {
        "summary": "Pull Request '1898' titled 'Combine `backend` and `device` arguments' was authored by a User, who is associated as a MEMBER. \nIt was created at 2019-12-19T23:35:38Z, and was closed at 2023-03-04T17:53:44Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'This looks promising! What's the status? Want to get it reviewed, or is there more work to be done?' by a COLLABORATOR of type User on 2020-01-07T07:05:01Z\n'Thanks for the ping, I forgot about this PR! @hawkinsp @shoyer we talked about this in December, WDYT of this change?' by a MEMBER of type User on 2020-01-07T21:36:15Z\n'Closing this PR. Whatever its merits were, it is quite stale.' by a COLLABORATOR of type User on 2023-03-04T17:53:44Z\n\nPR has review comments:\n'It does not seem nice to overload the device argument with a string denoting the platform. I assume this would be rarely needed, why not ask the user to use `devices=devices(\"gpu\")[0]`?  ' by a COLLABORATOR of type User on 2020-04-30T07:18:28Z\n'I think that we should get in the habit of deprecating arguments from public APIs before removing them. I would suggest keeping the `backend`, immediately turning it into device=devices(backend)[0], issuing in awarning, adding the note to Changelog, adding a reminder somewhere to remove it in a future release. This is a lot more expensive than just removing it, but I think that as we have more and more users it is a good thing to do. \r\n\r\nAlso, it may make sense to do codesearch/ for people who use the backend argument' by a COLLABORATOR of type User on 2020-04-30T07:20:45Z\n'If we do remove the string device arguments, we should fix this docstring too' by a COLLABORATOR of type User on 2020-04-30T07:22:15Z\n'Maybe check instead whether it is a supported platform.' by a COLLABORATOR of type User on 2020-04-30T07:24:22Z\n'Instead of doing this, I would rewrite (or eliminate) the test. It is not clear that in the absence of the backend argument this test adds value. ' by a COLLABORATOR of type User on 2020-04-30T07:25:49Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 5,
        "reason_for_closure": "Stale :- PR abandoned due to staleness."
    },
    {
        "summary": "Pull Request '1831' titled 'implement np.convolve method in lax_numpy' was authored by a User, who is associated as a NONE. \nIt was created at 2019-12-07T15:53:18Z, and was closed at 2020-04-11T00:19:11Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'implement np.convolve method as described in #1561 \r\n'\nPR has comments:\n'@hawkinsp I have updated the code, also I made some changes to lax.padtype_to_pads, where it cannot handle cases where the window shape is bigger than in_shape' by a NONE of type User on 2019-12-11T09:35:31Z\n'Did you push the changes? I don't see them in the PR yet.' by a COLLABORATOR of type User on 2019-12-11T17:23:23Z\n'@hawkinsp  sorry,  git add *.py didn't add new changes to the new commits. ' by a NONE of type User on 2019-12-12T05:57:51Z\n'@hawkinsp hi, finally the CI test passed, there is a off by one error compare to numpy's result when padding=same and lhr is even number. it would be great if you can review it ' by a NONE of type User on 2019-12-16T11:15:24Z\n'Hey @michiboo , sorry for the delay on our end (from NeurIPS followed by holidays).\r\n\r\nThis looks like it's in pretty good shape! I left a couple small comments just now about tweaks.\r\n\r\nJust to clarify from your last comment:\r\n\r\n> there is a off by one error compare to numpy's result when padding=same and lhr is even number. it would be great if you can review it\r\n\r\nDo you mean that the current implementation has a known bug in it, or that you have already fixed this problem?' by a COLLABORATOR of type User on 2020-01-08T03:45:48Z\n'\r\n\r\n\r\n\r\n> Hey @michiboo , sorry for the delay on our end (from NeurIPS followed by holidays).\r\n> \r\n> This looks like it's in pretty good shape! I left a couple small comments just now about tweaks.\r\n> \r\n> Just to clarify from your last comment:\r\n> \r\n> > there is a off by one error compare to numpy's result when padding=same and lhr is even number. it would be great if you can review it\r\n> \r\n> Do you mean that the current implementation has a known bug in it, or that you have already fixed this problem?\r\n\r\nHi, it is probably due to the fact that the Conv method in jax is implemented similar to tensorflow while the convolve implemented in numpy use its own conv method. \r\nI already fixed the problem by offset it by one.' by a NONE of type User on 2020-01-08T05:30:29Z\n'bump' by a NONE of type User on 2020-04-01T11:02:04Z\n'I think #2675 built on this work and finished off the underlying issue!' by a COLLABORATOR of type User on 2020-04-11T00:19:10Z\n\nPR has review comments:\n'Minor style nit: elsewhere in JAX, the style is to put a space after most commas, so here:\r\n` x_jax = lax.reshape(x, (1, 1, len(x)))`\r\nSame everywhere.' by a COLLABORATOR of type User on 2019-12-11T02:23:00Z\n'Is this the right padding? I don't think so: I would have assumed the padding would be related to the size of `y`, not the size of `x`. Certainly this doesn't seem to produce the right output when `x` and `y` don't have the same size.' by a COLLABORATOR of type User on 2019-12-11T02:29:18Z\n'Can you test multiple sizes of `x` and `y`? You would probably do well to make this a parametric test, as many others in this file are.\r\n\r\nYou could also avoid redundancy in the test by making the `mode` argument parametric as well.' by a COLLABORATOR of type User on 2019-12-11T02:30:13Z\n'I suspect you need some type promotion logic here. For example, does this implementation work similarly to:\r\n```\r\nonp.convolve(onp.arange(5), onp.arange(3, dtype=onp.float32))\r\n```\r\n\r\nMost likely what you need is the line:\r\n```\r\nx, y = _promote_types(x, y)\r\n```\r\n' by a COLLABORATOR of type User on 2019-12-11T02:32:58Z\n'Can we avoid the modification to `lax.py`? Perhaps you can just call `lax.conv_general_dilated` directly from the `np.convolve` implementation, and basically move these lines over there.' by a COLLABORATOR of type User on 2020-01-08T03:42:14Z\n'It might be a good idea to check that the inputs are 1D, and raise an error otherwise.' by a COLLABORATOR of type User on 2020-01-08T03:44:22Z\n'I suggest a couple of minor cleanups:\r\n```\r\nif ndim(x) != 1 or ndim(y) != 1:\r\n  msg = \"jax.numpy.convolve only accepts 1d arrays, got shapes {} and {}\"\r\n  raise ValueError(msg.format(shape(x), shape(y)))\r\n```' by a COLLABORATOR of type User on 2020-04-01T13:48:26Z\n'I'd put space around binary operators, e.g., `len(y) - 1, len(y) - 1)`.' by a COLLABORATOR of type User on 2020-04-01T13:50:15Z\n'Use spaces around binary `+` and `-` operators.' by a COLLABORATOR of type User on 2020-04-01T13:50:32Z\n'There's probably no need to pass `precision` here, unless you are intending to pass it to all of the `conv` variants. I'd probably just omit `precision=None` for now, to be honest; it only has any effect on TPU at the moment.' by a COLLABORATOR of type User on 2020-04-01T13:51:40Z\n'I think I'd probably just reuse `x` instead of introducing a new `x_jax` name, given you don't need to access the original `x` later. Same for `y`.' by a COLLABORATOR of type User on 2020-04-01T13:52:25Z\n'You should probably be consistent and use a single way to compute the shape everywhere. Currently you use `shape(x)[0]`, `len(x)` and `x.shape[0]` in various places.' by a COLLABORATOR of type User on 2020-04-01T13:56:52Z\n'Could we express this more directly? I think calling `padtype_to_pads` is actually just making the code more complicated, not less. Here I think you just want:\r\n```\r\nlow = (len(y) + 1) // 2\r\nhigh = len(y) - low\r\n```\r\n\r\n(although you'd need to double check that, I didn't test that.)' by a COLLABORATOR of type User on 2020-04-01T14:02:43Z\n'I'd use `float_dtypes` here rather than enumerating a list of types. Not all hardware platforms support all of these types (e.g., TPU doesn't support `float16`.)' by a COLLABORATOR of type User on 2020-04-01T14:04:13Z\n'Space after `shape2,`' by a COLLABORATOR of type User on 2020-04-01T14:04:31Z\n'This generates rather a lot of test cases (~98^2). That's too many. I think you can achieve what you want by just handwriting a few special cases, e.g.,\r\n`[2, 3, 5, 10]`\r\n\r\nDoes it make sense to test `0` and `1` for this function? Does numpy work for those cases? If so, I'd test them too.\r\n' by a COLLABORATOR of type User on 2020-04-01T14:06:11Z\n'Revert this change? It looks unintentional and would break this function.' by a COLLABORATOR of type User on 2020-04-01T14:07:09Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 17,
        "reason_for_closure": "Inconsistent behavior change: PR deprecates supported functionality, breaking existing users with little benefit."
    },
    {
        "summary": "Pull Request '1821' titled 'Implement scipy.sparse.linalg.cg ' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-12-05T18:11:00Z, and was closed at 2020-04-07T23:19:13Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'A minimal implementation of `scipy.sparse.linalg.cg` for #1531 . @shoyer I need your help with defining \"custom derivative rules\" as noted in the original issue thread! In this case am I taking derivative of `body_fun` w.r.t `x` ? '\nPR has comments:\n'For the gradient, I would suggest following the model of the iterative solver test for custom_linear_solve:\r\nhttps://github.com/google/jax/blob/0c0137d787830d8ebd584c4610f7932f3787cab6/tests/lax_control_flow_test.py#L1307-L1318\r\n\r\nConjugate gradients inverts positive symmetric matrices, so this would look something like:\r\n```python\r\n    def _cg_solve(matvec, b):\r\n      # your existing cg function, written with lax.while_loop\r\n\r\n    def cg(matvec, b):\r\n      # exposed as scipy.sparse.cg\r\n      return lax.custom_linear_solve(matvec, b, _cg_solve, symmetric=True)\r\n```\r\n\r\nMy other suggestion would be to write to write this in terms of a function that calculates matrix-vector products. For dense matrices, other methods like Cholesky factorization are usually more efficient. CG really shines for inverting very large but sparse systems of equations, e.g., where the dense matrix might have 1e6 x1e6 elements, which is too big to fit into memory.' by a COLLABORATOR of type User on 2019-12-05T20:48:12Z\n'@shoyer thanks for the suggestions! My vanilla implementation is also missing a preconditioner, which can be diag(A), approx Cholesky factorization of A or other complicated choices. scipy's implementation has one but I couldn't figure out what it is from the source code. Do I need this in order to take advantage of the sparsity, instead of doing full matrix multiplication like what I have right now ? re: matrix-vector products, I thought there's no matrix inversion involved in my code...but I could be wrong :D ' by a CONTRIBUTOR of type User on 2019-12-06T03:13:52Z\n'Preconditioning can be very useful, but it isn't necessary for basic usage. Let's save it for a follow up.\r\n\r\nI was being a little sloppy with my language. CG does a linear solve rather than inverting a matrix (but in practice, it's very common to do linear solves instead of matrix inverses). ' by a COLLABORATOR of type User on 2019-12-06T08:11:06Z\n'@shoyer I think i got my function into the right form that uses `matvec` instead of the explicit matrix `A`. However, with the function signature of `lax.custom_linear_solve` I won't be able to override the default of `x0`, `tol`, `maxiter` like `scipy.sparse.linalg.cg`. Does this matter ? Also, where should I put the test for this ? :D ' by a CONTRIBUTOR of type User on 2019-12-09T06:23:53Z\n'You can pass a lambda function or use functions.partial to handle the extra arguments.' by a COLLABORATOR of type User on 2019-12-09T08:09:49Z\n'> You can pass a lambda function or use functions.partial to handle the extra arguments.\r\n\r\nThanks, @shoyer ! what about the unit-test, should i add it to `lax_scipy_test.py` ? ' by a CONTRIBUTOR of type User on 2019-12-09T18:09:56Z\n'> what about the unit-test, should i add it to `lax_scipy_test.py` ?\r\n\r\nSure, this would be fine. You could also make a new test module for `scipy.sparse.linalg`, often that is a bit more obvious :).' by a COLLABORATOR of type User on 2019-12-09T18:25:11Z\n'> Sure, this would be fine. You could also make a new test module for `scipy.sparse.linalg`, often that is a bit more obvious :).\r\n\r\nThat also works! Do you envision that `scipy.sparse.linalg` will grow into a separate module ? :)' by a CONTRIBUTOR of type User on 2019-12-09T18:30:38Z\n'> That also works! Do you envision that `scipy.sparse.linalg` will grow into a separate module ? :)\r\n\r\nYes, it seems like a reasonable place to put JAX versions of all this SciPy stuff :)' by a COLLABORATOR of type User on 2019-12-09T18:38:20Z\n'@shoyer just wondering if the test looks right to you! Any suggestion for making the matrix sparse ?' by a CONTRIBUTOR of type User on 2019-12-10T04:01:39Z\n'@shoyer my `jax-numpy` arrays aren't valid as input for the original `scipy` function, is there a way i can cast them back as normal `numpy` arrays ?' by a CONTRIBUTOR of type User on 2019-12-11T01:17:16Z\n'> @shoyer my `jax-numpy` arrays aren't valid as input for the original `scipy` function, is there a way i can cast them back as normal `numpy` arrays ?\r\n\r\nYou can cast JAX NumPy arrays into plain NumPy arrays with `numpy.asarray()` or `jax.device_get()`.' by a COLLABORATOR of type User on 2019-12-17T22:44:43Z\n'> You can cast JAX NumPy arrays into plain NumPy arrays with `numpy.asarray()` or `jax.device_get()`.\r\n\r\nThanks! Now i'm getting a cryptic error: \r\n\r\n```\r\ntests/lax_scipy_sparse_test.py:50: in build_and_solve\r\n    return sparse.cg(matvec=matvec, b=b, maxiter=maxiter)\r\njax/experimental/sparse.py:63: in cg\r\n    x = lax.custom_linear_solve(matvec, b, cg_solve, symmetric=True)\r\njax/lax/lax_control_flow.py:1229: in custom_linear_solve\r\n    partial(solve, matvec), in_args_tree, b_avals)\r\njax/lax/lax_control_flow.py:60: in _initial_style_jaxpr\r\n    jaxpr, out_pvals, consts = pe.trace_to_jaxpr(fun, in_pvals, instantiate=True)\r\njax/interpreters/partial_eval.py:333: in trace_to_jaxpr\r\n    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\r\njax/linear_util.py:153: in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\njax/experimental/sparse.py:62: in <lambda>\r\n    cg_solve = lambda matvec, b: _cg_solve(matvec, b, x0, tol, maxiter)\r\njax/experimental/sparse.py:54: in _cg_solve\r\n    (r_k_norm < full_tol) & (k < maxiter), body_fun, (matvec, p_k, x_k, r_k, r_k_norm, k)\r\njax/lax/lax_control_flow.py:191: in while_loop\r\n    init_avals = tuple(_map(_abstractify, init_vals))\r\njax/util.py:39: in safe_map\r\n    return list(map(f, *args))\r\njax/lax/lax_control_flow.py:68: in _abstractify\r\n    return raise_to_shaped(core.get_aval(x))\r\njax/core.py:545: in get_aval\r\n    return concrete_aval(x)\r\n def concrete_aval(x):\r\n      try:\r\n        return pytype_aval_mappings[type(x)](x)\r\n      except KeyError:\r\n>       raise TypeError(\"{} is not a valid Jax type\".format(type(x)))\r\nE       TypeError: <class 'functools.partial'> is not a valid Jax type\r\n```' by a CONTRIBUTOR of type User on 2019-12-18T13:35:28Z\n'I think the problem is that you can't pass `matvec` (which is a function) as an argument into a while_loop. Basically body_fun (as seen by while_loop) should only take array arguments.\r\n\r\nThere are two ways you could solve this, I think either would work fine:\r\n1. Move the definition of `body_fun` inside `_cg_solve`, so you can omit the `matvec` argument and rely on the value from the outer scope.\r\n2. Partially apply `matvec` to `body_fun` and use that inside the argument to `while_loop`, e.g., use `functools.partial(body_fun, matvec)`' by a COLLABORATOR of type User on 2019-12-18T21:40:03Z\n'Please post the error message here\n\nOn Fri, Dec 20, 2019 at 10:07 AM Tuan Nguyen <notifications@github.com>\nwrote:\n\n> *@TuanNguyen27* commented on this pull request.\n> ------------------------------\n>\n> In jax/experimental/sparse.py\n> <https://github.com/google/jax/pull/1821#discussion_r360501612>:\n>\n> > +  N = np.shape(b)[0]\n> +  if x0 is None:\n> +    x_k = np.zeros_like(b)\n> +  else:\n> +    x_k = x0\n> +    if not np.shape(x0) == (N,):\n> +      raise ValueError('A and x have incompatible dimensions')\n> +    if maxiter is None:\n> +      maxiter = N*10\n> +  r_k = b - matvec(x_k)\n> +  r_k_norm = np.linalg.norm(r_k)\n> +  p_k = r_k\n> +  k = 0\n> +  full_tol = tol * np.linalg.norm(b)\n> +  p_k, x_k, r_k, r_k_norm, k = lax.while_loop(\n> +      lambda r_k_norm, k: (r_k_norm < full_tol) & (k < maxiter),\n>\n> I'm running into another error that I don't know how to solve, could you\n> please take a look @shoyer <https://github.com/shoyer> :)\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/1821?email_source=notifications&email_token=AAJJFVSCLTEYWMCWKPKWGPLQZUCUTA5CNFSM4JV5RJ3KYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCP7DODY#discussion_r360501612>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAJJFVQU4KPBV7SGQTRJ6BDQZUCUTANCNFSM4JV5RJ3A>\n> .\n>\n' by a COLLABORATOR of type User on 2019-12-20T18:30:30Z\n'> Please post the error message here\r\n\r\nah yes, my apologies here's the error\r\n\r\n```\r\ntests/lax_scipy_sparse_test.py:50: in build_and_solve\r\n    return sparse.cg(matvec=matvec, b=b, maxiter=maxiter)\r\njax/experimental/sparse.py:65: in cg\r\n    x = lax.custom_linear_solve(matvec, b, cg_solve, symmetric=True)\r\njax/lax/lax_control_flow.py:1229: in custom_linear_solve\r\n    partial(solve, matvec), in_args_tree, b_avals)\r\njax/lax/lax_control_flow.py:60: in _initial_style_jaxpr\r\n    jaxpr, out_pvals, consts = pe.trace_to_jaxpr(fun, in_pvals, instantiate=True)\r\njax/interpreters/partial_eval.py:333: in trace_to_jaxpr\r\n    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\r\njax/linear_util.py:153: in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\njax/experimental/sparse.py:64: in <lambda>\r\n    cg_solve = lambda matvec, b: _cg_solve(matvec, b, x0, tol, maxiter)\r\njax/experimental/sparse.py:56: in _cg_solve\r\n    (p_k, x_k, r_k, r_k_norm, k)\r\njax/lax/lax_control_flow.py:192: in while_loop\r\n    cond_jaxpr, cond_consts, cond_tree = _initial_style_jaxpr(cond_fun, in_tree, init_avals)\r\njax/lax/lax_control_flow.py:60: in _initial_style_jaxpr\r\n    jaxpr, out_pvals, consts = pe.trace_to_jaxpr(fun, in_pvals, instantiate=True)\r\njax/interpreters/partial_eval.py:333: in trace_to_jaxpr\r\n    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\r\njax/linear_util.py:153: in call_wrapped\r\n    ans = self.f(*args, **dict(self.params, **kwargs))\r\njax/experimental/sparse.py:54: in <lambda>\r\n    lambda x: (x[-2] < full_tol) & (x[-1] < maxiter),\r\njax/core.py:304: in __lt__\r\n    def __lt__(self, other): return self.aval._lt(self, other)\r\njax/numpy/lax_numpy.py:464: in fn\r\n    x1, x2 =  _promote_args(numpy_fn.__name__, x1, x2)\r\njax/numpy/lax_numpy.py:279: in _promote_args\r\n    _check_arraylike(fun_name, *args)\r\nfun_name = 'less'\r\nargs = (Traced<ShapedArray(int64[]):JaxprTrace(level=1/0)>, None), pos = 1\r\narg = None\r\nmsg = '{} requires ndarray or scalar arguments, got {} at position {}.'\r\n    def _check_arraylike(fun_name, *args):\r\n      \"\"\"Check if all args fit JAX's definition of arraylike (ndarray or scalar).\"\"\"\r\n      if _any(not _arraylike(arg) for arg in args):\r\n        pos, arg = next((i, arg) for i, arg in enumerate(args)\r\n                        if not _arraylike(arg))\r\n        msg = \"{} requires ndarray or scalar arguments, got {} at position {}.\"\r\n>       raise TypeError(msg.format(fun_name, type(arg), pos))\r\nE       TypeError: less requires ndarray or scalar arguments, got <class 'NoneType'> at position 1.\r\n```\r\nEssentially, `r_k_norm`, `k` returned from `lax.while_loop` are `None` and it makes `lambda x: (x[-2] < full_tol) & (x[-1] < maxiter)` invalid. ' by a CONTRIBUTOR of type User on 2019-12-20T19:08:59Z\n'You need to set the initial values passed into the loop with valid arrays of the same dtype/shape as set inside the loop. You might precompute the norm with np.linalg.norm or use a default value of np.inf.' by a COLLABORATOR of type User on 2019-12-20T19:24:21Z\n'@shoyer I was able to solve the `None` issue but the implementation is returning zeros as the final solution for both 1-step and full convergence of conjugate solve. I'm wondering if you could take a quick look to see if anything looks off ? Thank you!! ' by a CONTRIBUTOR of type User on 2020-01-09T05:50:54Z\n'@TuanNguyen27 it's been a while since you worked on this. It OK if I take over and finish up this PR?' by a COLLABORATOR of type User on 2020-03-29T21:33:02Z\n'Please do ! I compared my implementation with TF and fax, all the steps\nseem to match but the tests still failed and I can’t figure out why.\n\nTuan\n\nOn Sun, Mar 29, 2020 at 2:33 PM Stephan Hoyer <notifications@github.com>\nwrote:\n\n> @TuanNguyen27 <https://github.com/TuanNguyen27> it's been a while since\n> you worked on this. It OK if I take over and finish up this PR?\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/google/jax/pull/1821#issuecomment-605705166>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADIMBNJ7DBMQKWF644H2PR3RJ65BVANCNFSM4JV5RJ3A>\n> .\n>\n-- \nSincerely,\n\nTuan Nguyen\nSwarthmore ' 18 | Salesforce Einstein\n610.517.5065\n' by a CONTRIBUTOR of type User on 2020-03-29T22:06:11Z\n\nPR has review comments:\n'This should copy the dtype from `b`, too. np.zeros_like is probably the cleanest way.' by a COLLABORATOR of type User on 2019-12-10T04:52:53Z\n'Let’s only allow 1D x0 for now. In the future we might relax that, but being strict let’s us simply our testing.' by a COLLABORATOR of type User on 2019-12-10T04:54:29Z\n'This needs to be in a class in order for it to be tested. Right now I suspect your tests are not running :)' by a COLLABORATOR of type User on 2019-12-10T05:02:35Z\n'Please match the tolerance behavior from scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.cg.html' by a COLLABORATOR of type User on 2019-12-10T05:03:18Z\n'This should add a conjugate so it works properly on complex valued array. Also please test complex numbers, too. ' by a COLLABORATOR of type User on 2019-12-10T05:04:50Z\n'I would suggest adding another test that compares the result of exactly one step of conjugate gradients, rather than running all the way to convergence, e.g., by setting maxiter=1.' by a COLLABORATOR of type User on 2019-12-10T05:07:56Z\n'can we add the second info return argument like scipy?' by a COLLABORATOR of type User on 2019-12-10T05:09:29Z\n'looks like the way i'm currently making the square matrix doesn't take into account `dtype`. Is there an alternative to using numpy's `randn`?  ' by a CONTRIBUTOR of type User on 2019-12-10T06:31:28Z\n'I think it would be fine to always set `info=0` for now, or maybe `info=None`?\r\n\r\nNote that this signature will be incompatible with `custom_linear_solve`, so you'll have to add it in `cg` for now rather than here. Long term, we probably want to use something like @jekbradbury's tagging transformation from https://github.com/google/jax/pull/1803 but that can come later.' by a COLLABORATOR of type User on 2019-12-10T08:39:28Z\n'Take a look at the `rng_factory` argument from other JAX tests, e.g., in `linalg_test.py`.' by a COLLABORATOR of type User on 2019-12-10T08:42:43Z\n'thanks! how do i \"zero\" out values in my dense matrix to create a sparse one in JAX ? ' by a CONTRIBUTOR of type User on 2019-12-10T16:13:56Z\n'`rng(shape, dtype)` returns a normal NumPy array. So you could modify it in-place if you like, but I don't think it's essential to test this function with actual \"sparse\" arrays. We don't really have sparse arrays in JAX currently, functions that compute a matrix-vector product are probably the closest thing.' by a COLLABORATOR of type User on 2019-12-10T16:34:08Z\n'Do i need to specify `atol`? The documentation has it as `legacy` but the [source code](https://github.com/scipy/scipy/blob/v0.14.0/scipy/sparse/linalg/isolve/iterative.py#L220) doesn't. ' by a CONTRIBUTOR of type User on 2019-12-10T20:54:53Z\n'I think `atol='legacy'` could be avoided but the documented behavior could be supported:\r\n`Tolerances for convergence, norm(residual) <= max(tol*norm(b), atol).`' by a COLLABORATOR of type User on 2019-12-10T21:04:12Z\n'This should be saved between iterations rather than re-computed in each step. Also, note that this quantity is exactly the same as `np.linalg.norm(r)`, so you don't need to recompute the norm in the while condition, either.' by a COLLABORATOR of type User on 2019-12-10T21:07:03Z\n'`cond_fun` needs the same arguments as the `body_fun`\r\n\r\nI'm guessing this would go much faster if you setup a local development environment? See https://jax.readthedocs.io/en/latest/developer.html for instructions. You don't need to build `jaxlib`, it's fine to use a pre-built version.' by a COLLABORATOR of type User on 2019-12-18T23:02:40Z\n'thanks for the suggestion! It does speed things up a lot.' by a CONTRIBUTOR of type User on 2019-12-19T19:57:00Z\n'I'm running into another error that I don't know how to solve, could you please take a look @shoyer :)  ' by a CONTRIBUTOR of type User on 2019-12-20T18:07:04Z\n'I think the first part of this condition is failing in the first iteration: the norm is larger than the tolerance, so the loop does not execute even once. I think it should be flipped?' by a COLLABORATOR of type User on 2020-01-09T06:08:48Z\n'wow, can't believe i miss that! Thanks a lot, Stephan! Now I'm seeing some big discrepancy between my result and what's returned by scipy's cg (both in 1-step and converged result). Should i try implementing pre-conditioning, or do you suggest other things I should try first ? ' by a CONTRIBUTOR of type User on 2020-01-09T17:47:39Z\n'You definitely don't need preconditioning. Out of the box, SciPy does exactly the same algorithm as on the Wikipedia page (though possibly with slightly different operations).\r\n\r\nI would suggest carefully stepping through the algorithm you're written, and comparing it to other conjugate gradient implementations that are known to be correct. For example, you could compare to this one from TensorFlow:\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/solvers/python/ops/linear_equations.py\r\n\r\nOr the version that @gehring wrote in fax:\r\nhttps://github.com/gehring/fax/blob/master/fax/competitive/cg.py' by a COLLABORATOR of type User on 2020-01-09T18:22:46Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 20,
        "num_review_comments": 21,
        "reason_for_closure": "Inconsistent behavior :- PR attempts to enforce NumPy behavior where JAX intentionally diverges."
    },
    {
        "summary": "Pull Request '1803' titled 'Tagging, take two' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-12-03T04:34:06Z, and was closed at 2020-05-20T04:53:41Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This is a rewrite of #1660 that uses explicit scope arguments rather than conflating scopes with values. This leads to a simpler mental model that's also likely to be easier to build on top of (especially for Sonnet-like neural network libraries).\r\n\r\nHere's an example of the more explicit API:\r\n```python\r\ndef bar(scope, key, x):\r\n  y = lax.tag(random.normal(key), scope, \"y\")\r\n  return x + y\r\ndef baz(scope, key, x):\r\n  key1, key2 = random.split(key)\r\n  a = bar(lax.push_scope(scope, 1), key1, x)\r\n  b = bar(lax.push_scope(scope, 2), key2, x)\r\n  return a + b\r\n\r\ncollect(baz)(jax.Scope(), random.PRNGKey(0), 2.)\r\n# (DeviceArray(2.8923516), {1: {'y': DeviceArray(0.14389044)}, 2: {'y': DeviceArray(-1.2515389)}})\r\n```\r\n\r\nThis implementation is still able to support the PRNG-splitting-based approach to tag structure; we would just have to create a type that contains both a key and a scope and splits them together.'\nPR has comments:\n'You need to add somewhere:\r\n```\r\nad_util.jaxval_zeros_likers[tagging.Scope] = lambda x: x\r\n```\r\nOtherwise using gradient fails. Additionally, the following code still fails:\r\n```\r\ndef loss(scope, params, x_in):\r\n  h1 = jnp.matmul(x_in, params[0])\r\n  h1 = lax.tag(h1, lax.push_scope(scope, \"kfac_output_layer_0\"))\r\n  h2 = jnp.matmul(h1, params[1])\r\n  h2 = lax.tag(h2, lax.push_scope(scope, \"kfac_output_layer_1\"))\r\n  return jnp.sum(jnp.mean((h2 - x_in) ** 2, axis=0))\r\n\r\ndef get_what_i_want(f, *args, **kwargs):\r\n    value, orig_tree = jax.collect(f)(jax.Scope(), *args, **kwargs)\r\n    outputs = jax.as_dict((path, val) for path, val in jax.iterpaths(orig_tree)\r\n                          if any(\"kfac_output\" in pi for pi in path))\r\n    def fun_for_grad(tree):\r\n      inject_add = jax.inject(f, tree, accum_fn=lambda old, new: old + new)\r\n      return inject_add(jax.Scope(), *args, **kwargs)\r\n    zeros_tree = jax.tree_map(jnp.zeros_like, outputs)\r\n    grads = jax.grad(fun_for_grad)(zeros_tree)\r\n    return grads\r\n\r\nx = jnp.zeros([10, 784])\r\nparams = [jnp.zeros([784, 31]), jnp.zeros([31, 784])]\r\nget_what_i_want(loss, params, x)\r\n```\r\nWith the error:\r\n```\r\n  File \"/usr/local/google/_blaze_botev/beb44bb70ff62d19ec998d25ef453aa2/execroot/google3/blaze-out/k8-cuda101-py3-opt/bin/experimental/deepmind/botev/jax/curvature/examples/autoencoder_mnist_standalone.runfiles/google3/third_party/py/jax/api.py\", line 358, in grad_f\r\n    _, g = value_and_grad_f(*args, **kwargs)\r\n  File \"/usr/local/google/_blaze_botev/beb44bb70ff62d19ec998d25ef453aa2/execroot/google3/blaze-out/k8-cuda101-py3-opt/bin/experimental/deepmind/botev/jax/curvature/examples/autoencoder_mnist_standalone.runfiles/google3/third_party/py/jax/api.py\", line 423, in value_and_grad_f\r\n    g = vjp_py(onp.ones((), dtype=dtype))\r\n  File \"/usr/local/google/_blaze_botev/beb44bb70ff62d19ec998d25ef453aa2/execroot/google3/blaze-out/k8-cuda101-py3-opt/bin/experimental/deepmind/botev/jax/curvature/examples/autoencoder_mnist_standalone.runfiles/google3/third_party/py/jax/api.py\", line 1229, in _vjp_pullback_wrapper\r\n    ans = fun(*args)\r\n  File \"/usr/local/google/_blaze_botev/beb44bb70ff62d19ec998d25ef453aa2/execroot/google3/blaze-out/k8-cuda101-py3-opt/bin/experimental/deepmind/botev/jax/curvature/examples/autoencoder_mnist_standalone.runfiles/google3/third_party/py/jax/interpreters/ad.py\", line 115, in vjp_\r\n    _, arg_cts = backward_pass(jaxpr, consts, (), dummy_args, dummy_primals_and_cts)\r\n  File \"/usr/local/google/_blaze_botev/beb44bb70ff62d19ec998d25ef453aa2/execroot/google3/blaze-out/k8-cuda101-py3-opt/bin/experimental/deepmind/botev/jax/curvature/examples/autoencoder_mnist_standalone.runfiles/google3/third_party/py/jax/interpreters/ad.py\", line 223, in backward_pass\r\n    cts_out = get_primitive_transpose(eqn.primitive)(cts_in, *invals, **eqn.params)\r\n  File \"/usr/local/google/_blaze_botev/beb44bb70ff62d19ec998d25ef453aa2/execroot/google3/blaze-out/k8-cuda101-py3-opt/bin/experimental/deepmind/botev/jax/curvature/examples/autoencoder_mnist_standalone.runfiles/google3/third_party/py/jax/interpreters/ad.py\", line 403, in linear_transpose\r\n    return zero if cotangent is zero else transpose_rule(cotangent, **kwargs)\r\nTypeError: <lambda>() missing 1 required positional argument: 'scope'\r\n```\r\nThe error comes from:\r\n```\r\nad.deflinear(tag_p, lambda t, scope: [tag(t, scope)])\r\n```\r\n\r\nThe issue arises as the `scope` is an `arg` not a `kwarg` to `tag.bind` hence it does not get passed in the `transpose_rule(cotangent, **kwargs)`. \r\n\r\n\r\n' by a CONTRIBUTOR of type User on 2019-12-10T18:25:02Z\n'Adding this feature into master will be useful for #1922 ' by a NONE of type User on 2020-01-08T01:37:17Z\n'Closing for now, after checking with @jekbradbury he's got some newer designs.' by a COLLABORATOR of type User on 2020-05-20T04:53:41Z\n\nPR has review comments:\n'How does tagging work under JIT? I see from the tests that it does work, but how?' by a COLLABORATOR of type User on 2019-12-03T13:12:34Z\n'typo \"tge\"' by a COLLABORATOR of type User on 2019-12-03T13:12:53Z\n'Maybe add a test with a lax.while_loop? ' by a COLLABORATOR of type User on 2019-12-03T13:13:41Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 3,
        "reason_for_closure": "Closed as infeasible: behavior change deemed undesirable."
    },
    {
        "summary": "Pull Request '1652' titled 'Implements np.unique' was authored by a User, who is associated as a NONE. \nIt was created at 2019-11-09T14:14:28Z, and was closed at 2020-04-01T19:07:20Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Fixes #70 (np.unique)'\nPR has comments:\n'@shoyer @hawkinsp Kindly review the PR at your convenience and please suggest the changes that I can make.' by a NONE of type User on 2019-11-13T09:33:26Z\n'Sir, I am a bit unsure if I have understood you correctly, regarding the casting of Numpy arrays. I wanted to be certain if I am heading in the right direction, regarding the code. Here is what I understood.\r\n\r\n```\r\nfile = numpy.array(trial)\r\nfile_new = list(trial)\r\n```\r\n\r\nIf I return the ___file_new___ in the output, would the Jax tracer be able to iterate through the list and produce the right result or will the Jax tracer not be able to trace the initial Numpy.array, i.e., **_file_**.\r\nIs this the right approach or have I not grasped your point correctly? If not, would you mind being a bit more elaborate.\r\nThank You ' by a NONE of type User on 2019-11-19T13:02:46Z\n'JAX can only trace through functions implemented in terms of its own primitives. So, for example, we wouldn't be able to trace through `numpy.array`.\r\n\r\nThat doesn't mean you can't *use* classic NumPy as part of a JAX program, it just means that if you do so you won't be able to use JAX transformations over that code.\r\n\r\nHope that helps!' by a COLLABORATOR of type User on 2020-04-01T19:07:20Z\n\nPR has review comments:\n'Unfortunately it's not a good idea to cast to a classic NumPy array (here, you call `onp.array` on the input, which yields a NumPy array).\r\n\r\ne.g.:\r\n```\r\nIn [26]: jax.numpy.ones((3, 4)) + 2                                                                       \r\nOut[26]: \r\nDeviceArray([[3., 3., 3., 3.],\r\n             [3., 3., 3., 3.],\r\n             [3., 3., 3., 3.]], dtype=float32)\r\n\r\nIn [27]: asanyarray(jax.numpy.ones((3, 4)) + 2)                                                           \r\nOut[27]: \r\narray([[3., 3., 3., 3.],\r\n       [3., 3., 3., 3.],\r\n       [3., 3., 3., 3.]], dtype=float32)\r\n```\r\nNote the output is a classic NumPy array.\r\n\r\nThe problem with doing this is that JAX's tracer doesn't know understand how to trace through classic NumPy ops, only the `lax` primitives and operations built on top of them. So, for example, you wouldn't be able to batch this operation with `vmap` or compile it with `jit`. So you need to express this operation in terms of the operations in `jax.lax` (or other `jax.numpy` operations).\r\n\r\nI also note that if we *did* want to implement this operation by casting to a classic NumPy array, then there's a much simpler version...\r\n```\r\nunique = onp.unique\r\n```\r\n' by a COLLABORATOR of type User on 2019-11-13T14:32:26Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 1,
        "reason_for_closure": "Incorrect :- PR attempted to enforce NumPy behavior but JAX's superset functionality was preferred."
    },
    {
        "summary": "Pull Request '1637' titled 'implement James Bondbury caching heuristic' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2019-11-07T05:48:10Z, and was closed at 2019-11-09T04:11:49Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This is a follow-up to #1605 to address the issue of recompiling equal (but not identical) callables.\r\n\r\nPython callables can be equal but not identical (in real use cases!), as we learned when we tried to pull #1605 down into Google and saw new test timeouts due to repeating big recompilations. Here's a stylized version of the issue:\r\n\r\n```python\r\nclass partial(object):\r\n  def __init__(self, f, *args):\r\n    self.stuff = (f, args)\r\n  def __hash__(self):\r\n    return hash(self.stuff)\r\n  def __eq__(self, other):\r\n    return type(other) is partial and self.stuff == other.stuff\r\n  def __call__(self, *rest):\r\n    f, args = self.stuff\r\n    return f(*(args + rest))\r\n\r\nf = lambda x, y: x + y\r\nassert partial(f, 2) == partial(f, 2)\r\nassert partial(f, 2) is not partial(f, 2)\r\n\r\njit(partial(f, 2))(3)\r\njit(partial(f, 2))(5)  # cache hit or miss?\r\n```\r\n\r\nBefore #1605, the last line would get a cache hit because the caching was based on equality (acting in effect as if the underlying callable itself was the cache key, ignoring the other components of the cache key for simplicity). After #1605, because the cache became a WeakKeyDictionary, when the first `partial(f, 2)` went out of scope the corresponding cache entry would be deleted.\r\n\r\nThis came up in a real use case, though it involved bound instance methods rather than partials. It relied on something like this at-first-surprising but sensible-in-retrospect Python behavior:\r\n\r\n```python\r\nclass A:\r\n  def foo(self): pass  # an instance method\r\n\r\na1, a2 = A(), A()\r\nprint(id(a1.foo), hash(a1.foo))\r\nprint(id(a2.foo), hash(a2.foo))  # different hashes!\r\n\r\nclass A:\r\n  def foo(self): pass  # an instance method\r\n  def __hash__(self): return 2\r\n  def __eq__(self, other): type(other) is A\r\n\r\na1, a2 = A(), A()\r\nprint(id(a1.foo), hash(a1.foo))\r\nprint(id(a2.foo), hash(a2.foo))  # equal hashes!\r\n```\r\n\r\n```\r\n140018977204896 4768\r\n140018977204896 4796\r\n140018977204896 8751184735374\r\n140018977204896 8751184735374\r\n```\r\n\r\nActually, what we observed were instance methods with _different_ ids but equal hashes (unlike the above for which the ids are all the same), though it involved threading.\r\n\r\nSo should we...\r\n1. ...cache by putting the callables in a (strong-ref) dict, like pre-#1605, which means caching on value semantics? That means we'd get hits in the first example above, but it also means we can't delete cache entries when the corresponding callable objects go out of scope: what if someone creates a brand new but equal function later?\r\n2. ...cache by putting the callables in a a WeakKeyDictionary, like post-#1605, which in terms of persistence is like caching on object identity? Then we'd be able to delete a cache entry when the corresponding callable object goes out of scope, solving the issue in #1605, but it also means we get cache misses in the above example, and in analogous real user code.\r\n\r\nWe don't know the answer yet. But at least as a stop-gap, or possibly a long-term solution if it works well, @jekbradbury proposed a heuristic: start from policy 2 using a WeakKeyDictionary, but keep around a list of callable hashes we've seen before. If we see the same hash again, then hang on to a strong reference to the callable, so as to keep it in the WeakKeyDictionary hash. (We called this the James Bondbury Heuristic because You Only Compile Twice.)\r\n\r\nI verified:\r\n  - [x] the internal tests pass rather than time out\r\n  - [x] the memory leak version of the #1605 issue (with large constants) doesn't blow up\r\n  - [x] the benchmark in #1605 is unaffected'\nPR has comments:\n'We ended up not needing this; we decided to go with policy 2, with no James Bondbury.' by a COLLABORATOR of type User on 2019-11-09T04:11:49Z\n\nPR has review comments:\n'Add a comment noting this saves a strong reference?' by a COLLABORATOR of type User on 2019-11-07T06:48:12Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "Unnecessary: JAX behavior decided to remain a superset of NumPy."
    },
    {
        "summary": "Pull Request '1569' titled 'ResBlock combinator in stax' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-10-26T16:21:22Z, and was closed at 2021-05-12T13:51:28Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'I have seen (and used) the ResNet-style \"split, apply, combine\" design pattern enough that I thought it might be nice to have it built in. This may be trivial, but I thought it made code a little cleaner and could potentially clean up `import` statements for architectures with residual connections.\r\n\r\nI confirmed the resnet50 example exhibited identical losses for each update with and without the new `ResBlock` call.\r\n\r\nPlease let me know if you like the idea, but would prefer some other function name and/or signature--I would be happy to modify it to suite maintainer preferences.'\nPR has comments:\n'Looks reasonable to me, though my preference would be to avoid the final “tail” layer defaulting to ReLU and keep all activations explicit. This would be more consistent with the rest of stax, and in my experience unexpected activations are a common source of bugs.' by a COLLABORATOR of type User on 2019-10-26T18:17:02Z\n'Thanks for the PR! We're leaning towards declining this change even thought it's correct and increases the readability of the ResNet definition, because stax.py is meant more as an example (forking encouraged!) than as a growing or comprehensive library and more full-featured neural network creation APIs are available or coming soon (see https://github.com/google/trax and https://github.com/JuliusKunze/jaxnet for two of them).' by a CONTRIBUTOR of type User on 2019-10-28T18:58:19Z\n'> Thanks for the PR! We're leaning towards declining this change even thought it's correct and increases the readability of the ResNet definition, because stax.py is meant more as an example (forking encouraged!) than as a growing or comprehensive library and more full-featured neural network creation APIs are available or coming soon (see https://github.com/google/trax and https://github.com/JuliusKunze/jaxnet for two of them).\r\n\r\nThanks for the update. I was not aware of either of those two projects. It looks like Trax already has a similar [residual layer](https://github.com/google/trax/blob/fbd88a8eafdbb7ba327c63f44f80cb9798599e04/trax/layers/combinators.py#L519).' by a CONTRIBUTOR of type User on 2019-10-28T20:31:53Z\n'We're no longer evolving stax in favor of other libraries such as Flax, Haiku, and Trax. Closing!' by a COLLABORATOR of type User on 2021-05-12T13:51:27Z\n\nPR has review comments:\n'You can’t use this syntax yet because JAX still supports Python 2 (for a little while longer)' by a COLLABORATOR of type User on 2019-10-26T18:20:15Z\n'> Looks reasonable to me, though my preference would be to avoid the final “tail” layer defaulting to ReLU and keep all activations explicit. This would be more consistent with the rest of stax, and in my experience unexpected activations are a common source of bugs.\r\n\r\nOh interesting--in hindsight I could see that potentially being an issue but I don't think I would have thought of it before you mentioned it. I modified the default `tail` to be `Identity`.\r\n\r\n>You can’t use this syntax yet because JAX still supports Python 2 (for a little while longer)\r\n\r\nI submitted a patch that works for Python 2 and 3 with a `TODO` to replace it with the more readable function head when Python 2 support is dropped.\r\n\r\nPlease feel free to let me know if you would like any further modifications.' by a CONTRIBUTOR of type User on 2019-10-28T16:19:35Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 4,
        "num_review_comments": 2,
        "reason_for_closure": "No longer evolving this library."
    },
    {
        "summary": "Pull Request '1562' titled 'device_put_replicated' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2019-10-23T22:48:28Z, and was closed at 2020-01-14T13:57:40Z by a User.\nThe PR has labels: cla: no - . \nPR has comments:\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1562) for more info**.\n\n<!-- need_author_cla -->' by a COLLABORATOR of type User on 2019-10-23T22:48:32Z\n'A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1562) for more info**.\n\n<!-- cla_yes -->' by a COLLABORATOR of type User on 2019-10-23T22:48:50Z\n'It's okay, googlebot: Anselm Levskaya (levskaya@) is a Googler!' by a COLLABORATOR of type User on 2019-10-23T22:49:05Z\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1562) for more info**.\n\n<!-- need_author_cla -->' by a COLLABORATOR of type User on 2019-10-24T14:00:36Z\n'A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1562) for more info**.\n\n<!-- cla_yes -->' by a COLLABORATOR of type User on 2019-10-24T14:00:54Z\n'We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1562) for more info**.\n\n<!-- need_author_cla -->' by a COLLABORATOR of type User on 2019-10-25T07:05:07Z\n'I removed the tentative jitted constant replication logic since it was incomplete and @mattjj and @skye are working on something correct for the general nested pmap case.' by a COLLABORATOR of type User on 2019-10-31T05:41:08Z\n'We decided to do this with lazy broadcasts instead (which pmap can then do something smart with).' by a COLLABORATOR of type User on 2020-01-14T13:57:58Z\n\nPR has review comments:\n'This might be true on the platforms we test on, but we shouldn't make assumptions about device IDs. I would replace this with self.assertEqual(len(set(b.device().id for b in bufs)), xb.device_count())' by a MEMBER of type User on 2019-10-23T22:54:02Z\n'You're right! Done.' by a COLLABORATOR of type User on 2019-10-24T14:00:39Z\n'If this is public API, it deserves a docstring. ' by a COLLABORATOR of type User on 2019-10-27T16:33:35Z\n'Shouldn't here (and below) devices be a keyword argument, to match the definition of the primitive? This would also help clarify why we do not have to consider the case when devices and backend are abstract, or in the batch rule why devices is not among batch_dims.' by a COLLABORATOR of type User on 2019-10-27T16:53:25Z\n'Well, being a keyword argument or not is something up to the caller; in Python 2 there's no way to force something to be a keyword argument. (That's distinct from the question of whether an argument has a default value.)\r\n\r\nIndeed `devices` here is a parameter of the primitive and, by our convention for `bind`, must be passed as a keyword argument by the caller. But I don't think there's anything that needs changing about this function signature. (Compare e.g. to the [`convert_element_type` primitive](https://github.com/google/jax/blob/b62c3449df4e8226a55a3a04d8432872bceff469/jax/lax/lax.py#L1859-L1876): both `new_dtype` and `old_dtype` are primitive parameters and hence must be passed to `bind` as keyword arguments, but the signature of the primitives' rules can't indicate that.)' by a COLLABORATOR of type User on 2019-10-28T16:15:07Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 5,
        "reason_for_closure": "CLA issue resolved by manual verification and later discussion on implementation details led to closure."
    },
    {
        "summary": "Pull Request '1500' titled 'add documentation for the jarrett api function' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-10-14T12:01:11Z, and was closed at 2020-04-01T15:32:06Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'I've noticed that it's used in `jax.nn`, seems to be generally useful transformation, but naming is a bit unintuitive. I've taken the example from #525.'\nPR has comments:\n'Are we comfortable freezing this as the API (especially the name)?' by a COLLABORATOR of type User on 2019-10-14T20:50:54Z\n'We've removed `jarrett` because of bitrot, so this PR is moot at the moment. (When we revive it, we should include this doc string!)' by a COLLABORATOR of type User on 2020-04-01T15:32:06Z\n\nPR has review comments:\n'These jaxprs appear to be outdated (they contain `pack`, which was removed in detuplification). Can you regenerate?' by a CONTRIBUTOR of type User on 2019-10-14T19:45:28Z\n'Let’s try to stick with Python 3 compatible syntax for all examples.' by a COLLABORATOR of type User on 2019-10-15T04:10:53Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 2,
        "num_review_comments": 2,
        "reason_for_closure": "Inconclusive: The PR was closed because the function was deemed valuable despite numpy's limitations, and there was no consensus on deprecating complex support.  The PR also attempted to add a docstring."
    },
    {
        "summary": "Pull Request '1185' titled 'Jet Finite Differences Test, WIP primitives and symbolic zeros' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-08-14T22:43:48Z, and was closed at 2020-03-29T22:07:42Z by a User.\nThe PR has labels: cla: no - . \nIt has a body of 'I've corrected the test function to use higher order finite differences method. This will now numerically test the jets for terms other than `[1,0,0,...]` which was the case for the `jvp` based tests previously. I'm unsure of how to write a `jvp` based test that does the equivalent.\r\n\r\nI am writing some primitives but am stuck since our previous `make_derivs` required using numpy, e.g. `np.sum` and `np.array`. Since these are now going directly into `lax.py` we cannot import these functions. What is the correct approach here?\r\n\r\nThe symbolic zeros, `ZeroTerm` and `ZeroSeries` are broken. `ZeroTerm` needs `*` with float defined. `ZeroSeries` needs to support iteration. These are two bugs I've run into so far.\r\n\r\nThe `total_zhat_jet` test will be to determine how we can use `jet` to give the \"total derivative of `f(z(t),t)` wrt t\" This uses a known `z(t)` with closed form solution `sol(t)` to test by simply nesting `grad` on the closure with just `t` input. To test we will look at `jet` on the full `f(z_eval,t_eval)`. However, due to the above bugs I am unable to test the simple function `4 * z / t`'\nPR has comments:\n'All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1185) for more info**.\n\n<!-- need_author_consent -->' by a COLLABORATOR of type User on 2019-08-15T15:53:46Z\n'I think this PR can be deleted now that we have jet in master.' by a CONTRIBUTOR of type User on 2020-03-29T15:46:30Z\n'@duvenaud thanks!' by a COLLABORATOR of type User on 2020-03-29T22:07:42Z\n\nPR has review comments:\n'Here you see I need access to `numpy` operations, but living inside `lax.py` I cannot import these. We did not run into this problem with `sin` because I could just directly import this. However, here I need `np.array` (which we can possibly avoid) and `np.sum` because `sum` does not take keyword arguments. \r\n\r\nWe do not want to use `onp` here because I believe it will prevent us from taking `grad` through these operations.' by a CONTRIBUTOR of type User on 2019-08-14T22:46:28Z\n'This should return a `zero_term` however, `ZeroTerm` does not have `*` defined on it and causes errors. This is a workaround until I know how to fix these.' by a CONTRIBUTOR of type User on 2019-08-14T22:47:25Z\n'Again, should be a `zero_term`' by a CONTRIBUTOR of type User on 2019-08-14T22:48:01Z\n'`order` refers to the order of the finite difference method. `deriv` corresponds to the order of the derivative. `order` is related to the accuracy of the method, but I don't have a good way to choose this.' by a CONTRIBUTOR of type User on 2019-08-14T22:49:59Z\n'`atol` because the higher order finite differences become increasingly numerically unstable. `atol` should loosen tolerance as order increases, possibly? Or determine a more principled way to choose `order`.' by a CONTRIBUTOR of type User on 2019-08-14T22:51:01Z\n'This nested `jvp` test function only tests the case where the terms are `[1,0,0,0]`. I have replaced it with a finite differences based numerical test that can test for arbitrary input terms by taking the taylor expansion around the input with the terms corresponding to the directional sensitivity at each order.\r\n\r\nI suspect there is a way to write the correct test for this in terms of `jvp`s but I spent too much time on it with no success. That would be good though, as the fdm approach has numerical issues.' by a CONTRIBUTOR of type User on 2019-08-14T22:53:44Z\n'I can explain this on call, but essentially `sol` provides the closed form solution for `sol(t) = f(z(t),t)` such that `grad` gives the \"total derivative\" wrt t.\r\n\r\nHowever, we need this total derivative without directly having access to `z(t)`, instead we have access to `z_eval` \r\n\r\nThis test will eventually help us determine how to use `jet` on `z_eval` to get the \"total derivative\" wrt z of `f(z_eval, t_eval)` \r\n\r\nHowever, this is stopped currently by bugs in my implementation of `mul` and the symbolic zeros.' by a CONTRIBUTOR of type User on 2019-08-14T23:12:11Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 3,
        "num_review_comments": 7,
        "reason_for_closure": "PR closed as unnecessary: implemented behavior deemed correct and valuable despite differing from NumPy."
    },
    {
        "summary": "Pull Request '1154' titled '[WIP] Implemented an `extract_image_patches` op using `lax` ops.' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-08-09T18:20:41Z, and was closed at 2020-10-12T21:36:02Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'An implementation of an op for extracting image patches. This implementation is a port of TensorFlow's implementation of `tf.image.extract_image_patches`.'\nPR has comments:\n'@hawkinsp Here is a prototype of tensorflow's `extract_image_patches`. This should be an exact port of tensorflow's implementation but a bit more pythonic. I still have to go around and polish it but let me know what you think of this direction. Also, adding some tests would probably a good idea.' by a CONTRIBUTOR of type User on 2019-08-09T18:24:50Z\n'Wow, I was just looking for this, so thanks! :) One thing to note is that this kind of operation used to be called `im2col` back in caffe/torch days, and I believe in matlab too. So it would be nice to mention this in the documentation, if just for discoverability via search.' by a CONTRIBUTOR of type User on 2019-08-14T14:56:48Z\n'While trying this out, my repl dies with SIGABRT whenever I mistakenly pass a `np.float64` for `image`. Not sure if this should be dealt with here or it's not this function's problem. (I'm new to jax.)' by a CONTRIBUTOR of type User on 2019-08-14T15:22:19Z\n'@lucasb-eyer I wasn't able to reproduce your issue on colab (both CPU and GPU). Admittedly, I am not familiar enough with the backend to give a good guess about what might cause this error, but some additional info might help. Are you running with CUDA? Could you provide a minimal example that causes this error? ' by a CONTRIBUTOR of type User on 2019-08-14T16:14:33Z\n'I had a mistake in my error description, it crashes when I pass an integer array, not a float64.\r\n\r\nRunning with CUDA on my local workstation's GPU (a 1080Ti). The minimal example is copy-pasting the function definition from this PR into a python interpreter with jax, then running the following:\r\n\r\n```\r\nextract_image_patches(\r\n    np.arange(25).reshape(1,5,5,1),\r\n    rhs_shape=(1,2,2,1),\r\n    window_strides=(1,1,1,1),\r\n    padding='VALID',\r\n    rhs_dilation=(1,1,1,1),\r\n    data_format='NHWC')\r\n```\r\n\r\nThis crashes, but using `np.arange(25).reshape(1,5,5,1).astype(np.float32)` works nice.' by a CONTRIBUTOR of type User on 2019-08-19T09:22:59Z\n'> I had a mistake in my error description, it crashes when I pass an integer array, not a float64.\r\n\r\n@lucasb-eyer Someone should correct me if I am wrong but I don't believe that integer convolution is supported in JAX at the moment.\r\n' by a CONTRIBUTOR of type User on 2019-08-19T19:57:40Z\n'@hawkinsp When you have some time, could you look through my replies and changes. I'd like to hear your thoughts before making the final changes to the docstrings and function signature.' by a CONTRIBUTOR of type User on 2019-08-19T19:59:51Z\n'Indeed, I didn't expect it to work, I just expected it not to brutally crash with a SIGABRT, though maybe my expectation is wrong? That's also why I initially wrote \"Not sure if this should be dealt with here or it's not this function's problem.\"' by a CONTRIBUTOR of type User on 2019-08-21T09:23:14Z\n'By the way, @hawkinsp is away right now (last week and this coming week) and asked one of us to follow up on this PR, but we haven't had the bandwidth yet. We'll get to it as soon as we can! Thanks for your work here.' by a COLLABORATOR of type User on 2019-08-25T20:10:01Z\n'@mattjj No problem at all! Thanks for the heads up!' by a CONTRIBUTOR of type User on 2019-08-25T21:28:05Z\n'This is great @gehring. I wonder if you have an idea on the simplest way of having the inverse of this, i.e. from patches to the original image?' by a NONE of type User on 2019-09-30T11:57:44Z\n'@gehring I haven't forgotten about reviewing this, but I've been out of office a lot. I'll take a look when I can.\r\n\r\n@MostafaDehghani Usually when you are looking for that kind of transpose operation, you can use the reverse-mode gradient, or VJP.\r\n\r\n`jax.vjp` should let you do what you need here: use `jax.vjp` on `extract_image_patches` with a cotangent containing the image patches to sum. If there are overlapping image patches, the VJP will sum the contributions all of them. To take the mean, which I assume is what you want, you can use `jax.vjp` with a cotangent of all 1s to count the number of contributions to each pixel, then divide the sum by the count. Hope that helps!' by a COLLABORATOR of type User on 2019-09-30T13:09:10Z\n'Hi, \r\n\r\nJust for the record, wanted to post this small example of how to reconstruct the original image given the output of extract_image_patches using  jax.vjp. \r\n\r\n```\r\nim_height = im_width = 6\r\npatch_height = patch_width = 2\r\nim = np.arange(im_height * im_width).reshape(-1, im_height, im_width,1).astype(np.float32)\r\n\r\n\r\ndef i2p(im):\r\n  return extract_image_patches(\r\n          lhs=im.astype(np.float64),\r\n          rhs_shape=(1,patch_height, patch_width,1),\r\n          window_strides=(1,patch_height, patch_width,1), \r\n          padding='VALID',\r\n          rhs_dilation=(1,1,1,1),\r\n          data_format='NHWC')\r\n\r\ndef p2i(im_patched):\r\n    _, fn_vjp = jax.vjp(i2p, np.ones(im.shape))\r\n    overlap_count = fn_vjp(np.ones_like(im_patched))[0]\r\n    return fn_vjp(im_patched)[0] / overlap_cou\r\n\r\nim_patched = i2p(im).squeeze()\r\nrecunstructed_im = p2i(im_patched)\r\nassert np.array_equal(recunstructed_im, im)\r\n```\r\nThanks  again @hawkinsp for the help! :)' by a NONE of type User on 2019-10-07T08:20:03Z\n'This PR is no longer relevant given a more polished version is actively being worked on (#4459).' by a CONTRIBUTOR of type User on 2020-10-12T21:36:02Z\n\nPR has review comments:\n'I might call this file extract_image_patches.py or similar. `extract` is a bit generic.' by a COLLABORATOR of type User on 2019-08-09T19:58:13Z\n'A docstring would be nice.' by a COLLABORATOR of type User on 2019-08-09T19:59:25Z\n'I think the (not publicly documented) `lax.conv_dimension_numbers` has most of what you need here. Perhaps we could refactor it a bit and share that code?\r\n\r\nEither way I wouldn't want to add a new public API for dimension numbers as part of this change.' by a COLLABORATOR of type User on 2019-08-13T15:01:32Z\n'You should probably let the user specify the format.' by a COLLABORATOR of type User on 2019-08-13T15:02:20Z\n'Given that `extract_image_patches` is very similar to convolution, I would try to harmonize the API with `conv_general_dilated` as much as possible (e.g., perhaps `rhs_dilation` instead of `rates` etc, similar parameter orders, etc.)' by a COLLABORATOR of type User on 2019-08-13T15:23:02Z\n'I'd probably use `np.int32` here.' by a COLLABORATOR of type User on 2019-08-13T15:25:33Z\n'You can: `reshape` can do this. But you shouldn't worry about that too much. You'd do better just to wrap a `jit` around the implementation so XLA can see the whole thing.' by a COLLABORATOR of type User on 2019-08-13T15:26:34Z\n'Shouldn't this be `(kernel_size, depth, kernel_size)`?' by a COLLABORATOR of type User on 2019-08-13T15:27:40Z\n'Done' by a CONTRIBUTOR of type User on 2019-08-13T18:47:45Z\n'I stopped short of finishing this because I wanted to see if we wanted to change the API first. It doesn't really make sense to me to require a specifying batch and feature dims for patch sizes, dilation and strides if we don't support !=1 (according to the original TF implementation). Should we change this to make calls simpler?' by a CONTRIBUTOR of type User on 2019-08-13T18:50:56Z\n'Removed the util module and changed to use lax's methods.' by a CONTRIBUTOR of type User on 2019-08-13T18:51:40Z\n'The original implementation had it hard coded but I don't see any reason not to allow this to be specified. I've updated the implementation.' by a CONTRIBUTOR of type User on 2019-08-13T18:58:29Z\n'There isn't really a `rhs` in this case but I pattern matched as well as I could!\r\nLet me know if you have any suggested changes. Also, the original implementation doesn't have any `lhs_dilation`. I think there is nothing inherent about this implementation that would prevent from supporting this. WDYT?' by a CONTRIBUTOR of type User on 2019-08-13T19:02:02Z\n'Done' by a CONTRIBUTOR of type User on 2019-08-13T19:02:07Z\n'Yes! That's what I get for only testing on images with depth 1... Now fixed.' by a CONTRIBUTOR of type User on 2019-08-13T19:02:57Z\n'Sounds good! I removed the `TODO`. I'm assuming you don't expect any performance benefits from doing it with one `reshape` rather than the current form.' by a CONTRIBUTOR of type User on 2019-08-13T19:04:46Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 14,
        "num_review_comments": 16,
        "reason_for_closure": "Superseded: A more polished version of this implementation is being worked on in another PR."
    },
    {
        "summary": "Pull Request '1081' titled 'Add scalar python types to array-like types' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-08-01T00:59:36Z, and was closed at 2020-06-24T15:12:03Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This will make `isinstance` called on scalars return True for both jitted and non-jitted JAX programs. Note that\r\n`isinstance(x, onp.ndarray)` is False for these types, so it breaks this correspondence.\r\n\r\n#1080 '\nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1081) for more info**.\n\n<!-- need_sender_cla -->' by a COLLABORATOR of type User on 2019-08-01T00:59:40Z\n'CLAs look good, thanks!\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F1081) for more info**.\n\n<!-- ok -->' by a COLLABORATOR of type User on 2019-08-01T01:02:28Z\n'Thanks Matt and James, added your suggestions!' by a CONTRIBUTOR of type User on 2019-08-01T01:19:10Z\n'It's a little surprising that types change under `jit`, but it sort of makes sense if you think about it -- and I fear the fix here might be worse. What if I'm using `isinstance` checks to see if I can safely use NumPy attributes, like `dtype` and `shape`?' by a COLLABORATOR of type User on 2019-08-01T02:01:30Z\n'@shoyer let's keep discussing this! Though we should keep in mind we're going for \"best reasonable solution\" rather than \"perfect.\"\r\n\r\nAttributes like `dtype` and `shape` work under a `jit` (because scalars are always 0d arrays) but not without a `jit`. But that doesn't seem so bad because NumPy ascribes dtypes and shapes to Python scalars already (i.e. the output of `onp.result_type` and `onp.shape` respectively). A separate argument is that using `isinstance` to look for attributes arguably goes against the Python duck-typing spirit.\r\n\r\nBut I am concerned if you think this 'fix' is worse. Can you say more about how this might be worse? Is it about diverging further from NumPy/Python scalar behavior, or rather about having different behavior under `jit` (specifically around isinstance and how it relates to attributes being available)?\r\n\r\nMaybe another principle we should include here is the principle of least surprise. Is `isinstance(3, np.ndarray)` surprising?' by a COLLABORATOR of type User on 2019-08-01T02:15:37Z\n'Honestly, I'm a little amazed that JAX manages to make `isinstance` with `jax.numpy.ndarray` work at all :). \r\n\r\nPerhaps we should think about use cases? For the use case of checking for arrays/scalars vs. other non-primitive Python things inside some sort of nested Python data structure, `isinstance(3, np.ndarray)` seems perfectly reasonable. So maybe this is fine after all...' by a COLLABORATOR of type User on 2019-08-01T03:01:11Z\n'I cannot think of circumstances when it would be useful to distinguish between Python scalars and arrays with JAX, with the possible exception of value-based casting (and that can be handled by some separate mechanism). It does seems like something like `np.is_number_or_array()` would be the right way to handle this, but NumPy doesn't provide that API 🤷‍♂ ' by a COLLABORATOR of type User on 2019-08-01T03:05:03Z\n'Closing this PR, since it's fairly stale at this point.\r\n\r\nI think #1080 is to some extent working as intended; `jit` has the effect of promoting scalars to `ndarrays`. So far we are mostly happy with that choice, but it is not completely transparent to introspection tools like `isinstance`.' by a COLLABORATOR of type User on 2020-06-24T15:12:03Z\n\nPR has review comments:\n'```suggestion\r\n# differently (i.e. isinstance(1, onp.ndarray) == False).\r\n```' by a CONTRIBUTOR of type User on 2019-08-01T01:12:18Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 8,
        "num_review_comments": 1,
        "reason_for_closure": "Stale :- PR closed due to staleness and the issue being addressed in a different way."
    },
    {
        "summary": "Pull Request '996' titled 'Add jit internally to a few slow routines in scatter_ops and linalg.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2019-07-08T15:55:25Z, and was closed at 2019-08-09T16:53:50Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'This makes these operations *much* faster when run outside of a jit context.'\nPR has comments:\n'Unfortunately this change caused test failures (Travis GH integration seems down at the moment) because these functions can take arguments that aren't supported by jit (e.g. Ellipsis):\r\n\r\nhttps://travis-ci.org/google/jax/builds/555864962' by a COLLABORATOR of type User on 2019-07-08T16:19:02Z\n'Would you rather try to support `Ellipsis` in `jax.jit` or wrap the lower level ops like `lax.scatter`? I think either would work here.' by a COLLABORATOR of type User on 2019-07-08T17:22:51Z\n'Good question. I just remembered that this works:\r\n\r\n```python\r\nprint(jit(lambda x: x)(None))\r\n```\r\n\r\nbecause [we register None as a pytree](https://github.com/google/jax/blob/76eda746bd28256fdba1b96036e8faa49088feea/jax/tree_util.py#L238). We could do the same with Ellipsis, adding this to tree_util.py (and also a rule for `slice` objects):\r\n\r\n```python\r\nregister_pytree_node(type(Ellipsis), lambda z: ((), None), lambda _, xs: Ellipsis)\r\nregister_pytree_node(slice,\r\n                     lambda s: ((s.start, s.stop, s.step), None),\r\n                     lambda _, xs: slice(*xs))\r\n```\r\n\r\nHowever, that slice solution didn't pass our tests on your branch because of how we use the `slice.indices` method. This implementation does pass all tests on your branch, but assumes there aren't Tracers in the slice fields:\r\n\r\n```python\r\nregister_pytree_node(slice,\r\n                     lambda s: ((), (s.start, s.stop, s.step)),\r\n                     lambda meta, _: slice(*meta))\r\n```\r\n\r\nIf we can get the first solution for `slice` working, we'd be in business! Gotta look at it more.' by a COLLABORATOR of type User on 2019-07-09T15:37:24Z\n'Ah the issue was that these added `@jit`s were then abstracting the contents of slices to the Shaped level, which would mean static slice indices were turned into dynamic slice indices. If we replace the use of `@jit` you added with `@partial(jit, static_argnums=(1,))` that avoids abstracting the indices and everything works!\r\n\r\nActually, I think it'd be better just to add a `@partial(jit, static_argnums=(1, 3))` decoration to `_scatter_update` rather than the wrapper functions.\r\n\r\nThis might be slightly more fine-grained caching than we need: we might be able to get more compilation cache hits if we just don't raise the abstraction level of (the contents of) `slice` instances. But that seems like more trouble than it's worth, since the point here is just to get these things under a `jit`.\r\n\r\n@shoyer wdyt?\r\n\r\n@dougalm any problem with registering `ellipsis` and `slice` as pytrees, like we do `NoneType`?' by a COLLABORATOR of type User on 2019-07-09T15:52:49Z\n'I pushed 8c72e2f to a branch so you can take a look at the changes I proposed above.' by a COLLABORATOR of type User on 2019-07-09T15:54:32Z\n'@mattjj It looks like your branch handles this better than what I had here, so let's just abandon this PR in favor of that. It's also getting stale.' by a COLLABORATOR of type User on 2019-08-09T16:53:50Z\n\nPR has review comments:\n'FWIW, I added this `jit` decorator in PR #1144 for the same reason.' by a COLLABORATOR of type User on 2019-08-09T13:11:43Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 6,
        "num_review_comments": 1,
        "reason_for_closure": "Superseded by design :- JAX functionality generally supersedes NumPy's, thus intentional difference."
    },
    {
        "summary": "Pull Request '734' titled 'Add initial support for concurrent jit-compilation, requested in #677.' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2019-05-20T15:21:20Z, and was closed at 2019-05-25T15:33:11Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Store the core tracer state within a thread-local object.\r\n\r\nThis is enough to make some simple tests pass, but very likely we also need to make the memoize caches thread-safe too.'\nPR has comments:\n'Looks like a good start. We'll run into trouble if we start mixing traces from different threads. It might be a \r\ngood idea to label each trace with its thread id, so we can raise an exception in `find_top_trace` if we encounter traces from different threads.' by a COLLABORATOR of type User on 2019-05-21T19:41:08Z\n\nPR has review comments:\n'Maybe a new class (and a singleton instance of it) that subclasses `threading.local`, instead of the raw `threading.local` instance.' by a COLLABORATOR of type User on 2019-05-20T16:26:36Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 1,
        "reason_for_closure": "PR merged after implementing requested changes and addressing collaborator concerns."
    },
    {
        "summary": "Pull Request '621' titled 'Remove prng maxsize restriction' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2019-04-17T18:19:35Z, and was closed at 2021-05-12T13:49:47Z by a User.\nThe PR has labels: cla: yes - . \nPR has comments:\n'This was subsequently fixed!' by a COLLABORATOR of type User on 2021-05-12T13:49:47Z\n\nPR has review comments:\n'I would have thought that concatenating them was more natural.' by a COLLABORATOR of type User on 2019-04-18T22:41:35Z\n'Perhaps define a helper function `ceil_of_ratio` in the way that XLA does in C++? Would make it clearer.\r\n' by a COLLABORATOR of type User on 2019-04-18T22:48:05Z\n'It was, and that's how I wrote it at first, but then I decided I might roll this into a fori_loop. Then I got lazy... I can put it back to a concatenate.' by a COLLABORATOR of type User on 2019-04-18T22:57:20Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 3,
        "reason_for_closure": "Inconsistent behavior rectified: removed maxsize restriction, aligning with project conventions."
    },
    {
        "summary": "Pull Request '611' titled 'Implement overrides of NumPy's public API on JAX arrays' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2019-04-13T17:37:57Z, and was closed at 2019-10-24T21:29:20Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of '`__array_ufunc__` allows for writing NumPy's ufuncs, e.g., `onp.sin()`.\r\n\r\n`__array_function__` is a new, experimental override for most other functions in NumPy public API, e.g., `onp.concatenate()`. It will be enabled by default in NumPy 1.17, but is also available in NumPy 1.16 if you set the environment variable `NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1` before importing NumPy.\r\n\r\nTogether, these should allow users to stick with `import numpy as np` for use with JAX, instead of requiring `import jax.numpy as np`. I expect this will be particularly useful for projects that want to remain implementation agnostic, e.g., so they can write functions that will run without changes on JAX, CuPy and Dask arrays.\r\n\r\nNote: if you want to test this out in Colab, I think you need to install the development version of NumPy (e.g., `pip install -U git+https://github.com/numpy/numpy.git`). As far as I can tell, it isn't possible to set an environment variable from Colab before importing NumPy.'\nPR has comments:\n'I added `__array_ufunc__` and `__array_function__` to ~~Tracer~~ UnshapedArray, but this could still use an integration test. Any suggestions on where I should put that?' by a COLLABORATOR of type User on 2019-04-13T21:07:56Z\n'NumPy 1.17 is out, so these overrides will work by default now.\r\n\r\nIt would be nice merge this soonish, if only so I don't need to continue to rebase :). Also I can't wait to stop writing `import jax.numpy as np`!' by a COLLABORATOR of type User on 2019-08-02T05:02:05Z\n'Thanks, Stephan!\r\n\r\nWould we still need to `import jax.numpy as np` to use op-by-op JAX execution on standard numpy ndarrays? In that case I guess users will need to use more explicit `jax.device_put` calls (if they don’t import `jax.numpy` as before. \r\n\r\nMore importantly, this would change the behavior for anyone using `import numpy as onp`: where that could previously be used to convert to host-side ndarrays, now it might not, meaning users would need more explicit `jax.device_get` calls. \r\n\r\nThose might be good things, but I just want to make sure I’m understanding. It would be a nontrivial api change.' by a COLLABORATOR of type User on 2019-08-02T14:02:39Z\n'> Would we still need to `import jax.numpy as np` to use op-by-op JAX execution on standard numpy ndarrays? In that case I guess users will need to use more explicit `jax.device_put` calls (if they don’t import `jax.numpy` as before.\r\n\r\nYes, that would work. More practically, the easy way to ensure functions get executed with JAX is to use `jit`.\r\n\r\n> More importantly, this would change the behavior for anyone using `import numpy as onp`: where that could previously be used to convert to host-side ndarrays, now it might not, meaning users would need more explicit `jax.device_get` calls.\r\n\r\nThat's right, this would be a breaking change for such code. Instead, users will need to write `onp.array()` explicitly to convert into NumPy arrays. It might be worth testing this change on our internal codebase to gauge its impact.\r\n\r\nFor the most part I expect this will should be fine -- JAX has mostly equivalent implementations of most commonly used NumPy functions.\r\n\r\nOne noteworthy case are functions like `np.unique()` that exist in NumPy but not JAX. In the current version of this PR, we raise an exception when you call these functions on JAX arrays. Instead, it would probably be more user-friendly to call `jax.device_get()` to load them into memory and issue a warning about functions not implemented by JAX.' by a COLLABORATOR of type User on 2019-08-02T16:49:06Z\n'@mattjj and I decided *not* to merge this until/unless we find someone who really needs it. Please speak up in https://github.com/google/jax/issues/1565 if that's you.' by a COLLABORATOR of type User on 2019-10-24T21:29:20Z\n\nPR has review comments:\n'Alternatively, if you really want to keep with dedicated methods on `DeviceArray` I think this could be solved by implementing `__getattr__` on a metaclass. I'm not sure you want to go down that dark path, though....' by a COLLABORATOR of type User on 2019-04-13T21:49:54Z\n'It seems like the best we could do is forward to `self.aval`, but we'll still need the explicit `__array_ufunc__` and `__array_function__` function methods. It wouldn't do us any good to put these on a metadata, because then they would only have access to the `type` object (and `self.aval` is an instance variable).' by a COLLABORATOR of type User on 2019-04-16T04:41:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 2,
        "reason_for_closure": "Not useful: PR deprecates existing functionality with little benefit, breaking change for negligible NumPy compatibility."
    },
    {
        "summary": "Pull Request '506' titled 'Support subclass inheritance of pytree behavior' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-03-13T23:16:52Z, and was closed at 2021-05-25T12:58:52Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'I'm exploring some ideas around object-oriented layers, where it would be helpful if subclasses could inherit pytree registration from their base classes. This PR makes pytree behavior inheritable by default, but also lets subclasses override any inherited behavior and allows registering a superclass in a non-inheritable way.'\nPR has comments:\n'It would be great if we could tweak this PR to solve #446. IIRC namedtuple instances are subclasses of `tuple` but have slightly different creation signatures, i.e. `TupleSubclass((1, 2, 3))` vs `NamedtupleSubclass(1, 2, 3)`, which might need handling.' by a COLLABORATOR of type User on 2019-03-14T20:14:46Z\n'It's pretty unfortunate that Python namedtuples don't have an abstract base class--in our context, it makes them indistinguishable from core JAX types implemented as tuple subclasses (like `PartialVal` and `JaxTuple`) which need to be treated as leaves for the purposes of `pytree`. Rather than special-casing named tuples, I went ahead and implemented the ability to register a type as a leaf, and registering those two tuple subclasses was enough to get tests to pass.\r\n\r\nI'm not sure if I should also add leaf registration for `AbstractTuple`, `Destructuring`, and `JaxprTracerTuple` (or any others)--I haven't looked into whether any of those can appear in the trees passed to the tree_utils machinery (leaving them out hasn't broken any tests).' by a CONTRIBUTOR of type User on 2019-03-15T00:26:52Z\n'(Also, adding a test was a great idea, because it turns out the code in the original PR didn't actually do anything).' by a CONTRIBUTOR of type User on 2019-03-15T00:28:54Z\n'I like the idea!  I agree that the way namedtuples are implemented in python is pretty annoying.  As an aside, you _can_ make namedtuple equivalents that have a (non-tuple) base class, this is one that should work equivalently in py2/py3 for all the ways we use them in JAX:\r\n```python\r\nfrom collections import Sequence\r\n\r\nclass SaneNamedTuple(Sequence): \r\n    \"\"\"Subclass and define named fields with __slots__.\"\"\"\r\n    __slots__ = ()\r\n    def __init__(self, *args):\r\n        for slot, arg in zip(self.__slots__, args):\r\n            setattr(self, slot, arg)\r\n    def __repr__(self):\r\n        attr_repr = ', '.join(map(lambda x: \"%s=%r\"%(x[0], x[1]),\r\n                         tuple(zip(self.__slots__, tuple(self)))))\r\n        return '{name}({args})'.format(name=type(self).__name__, args=attr_repr)\r\n    def __iter__(self): \r\n        for name in self.__slots__:\r\n            yield getattr(self, name)\r\n    def __getitem__(self, index):\r\n        return getattr(self, self.__slots__[index])\r\n    def __len__(self):\r\n        return len(self.__slots__)\r\n\r\ndef sane_namedtuple(cls_name, attr_names):\r\n  \"Create a named tuple-like class with name cls_name and fields attr_names.\"\r\n  assert isinstance(attr_names, (list, tuple))\r\n  return type(cls_name,\r\n              (SaneNamedTuple,), \r\n              {\"__slots__\": tuple(attr_names)})\r\n\r\nPoint = sane_namedtuple(\"Point\", (\"x\", \"y\"))\r\npt = Point(1, 2)\r\nprint(pt)  # Point(x=1, y=2)\r\nprint(pt.x, pt.y)  # 1 2\r\nprint(isinstance(pt, Point))  # True\r\nprint(isinstance(pt, SaneNamedTuple))  # True\r\nprint(isinstance(pt, Sequence))  # True\r\nprint(isinstance(pt, tuple))  # False\r\nx, y = pt\r\nprint(x, y)  # 1 2\r\n```' by a COLLABORATOR of type User on 2019-03-15T08:45:34Z\n'Added a docstring. Should be ready to merge if folks are okay with my evil reuse of the singleton intended to abstractly represent pytree leaves as a sentinel that represents pytree leaf type registrations.' by a CONTRIBUTOR of type User on 2019-03-15T19:02:43Z\n'(I think Matt would be the best reviewer for this, but he's on vacation so he might take a few days to get back to you.)' by a COLLABORATOR of type User on 2019-03-15T19:04:45Z\n'@levskaya the issue with that is the same limitation I'm running into trying to wrap stax layers in classes: it's not possible to create Python functions that expose a programmatically-defined argument signature (e.g. for tab completion or documentation generation) without using `exec`. Here that means that `SaneNamedTuple`s expose `*args` as their constructor signature rather than the set of fields in the named tuple.' by a CONTRIBUTOR of type User on 2019-03-16T01:42:51Z\n'@jekbradbury - I see what you want.  In python2 this is pretty much true. :(  In python3 we have the tools of inspect.Signature and inspect.Parameter which can set `__signature__` to do this though:\r\n```python\r\n     from inspect import Signature, Parameter\r\n    # ... in base class def:\r\n    @classmethod\r\n    def get_signature(cls):\r\n        parameters = [\r\n          Parameter(name=slot, kind=Parameter.POSITIONAL_OR_KEYWORD) for slot in cls.__slots__\r\n        ]\r\n        return Signature(parameters=parameters)\r\n\r\n    def __init_subclass__(cls, **kwargs):\r\n        # ....\r\n        cls.__slots__ = tuple(slots)\r\n        cls.__signature__ = cls.get_signature()\r\n```\r\nyet another reason to drop py2 given it hasn't been improved in 10 years...' by a COLLABORATOR of type User on 2019-03-16T03:09:42Z\n'Sorry to keep distracting from the main point of this thread :)  to be concrete vis a vis above, this is the implementation of base-classable nametuple that I've seen before for py3.\r\n```python\r\nfrom collections import Sequence\r\nfrom inspect import Signature, Parameter\r\nclass SaneNamedTuple(Sequence): \r\n    __slots__ = ()\r\n    @classmethod\r\n    def get_signature(cls):\r\n        parameters = [Parameter(name=slot, kind=Parameter.POSITIONAL_OR_KEYWORD) \r\n                      for slot in cls.__slots__]\r\n        return Signature(parameters=parameters)\r\n    def __init_subclass__(cls, **kwargs):\r\n        super().__init_subclass__(**kwargs)\r\n        slots = cls.__slots__\r\n        cls.__slots__ = (tuple(slots.split()) if isinstance(slots, str) else tuple(slots))\r\n        cls.__signature__ = cls.get_signature()\r\n        cls.__init__.__signature__ = cls.get_signature()\r\n        cls.__doc__ = ('{cls.__name__}{cls.__signature__}\\n\\n{cls.__doc__}'.format(cls=cls))\r\n    def __new__(cls, *args, **kwargs):\r\n        if cls is SaneNamedTuple:\r\n            raise TypeError(\"Can't instantiate abstract class SaneNamedTuple\")\r\n        return super().__new__(cls)\r\n    @classmethod\r\n    def _get_bound_args(cls, args, kwargs):\r\n        return Signature.bind(cls.__signature__, *args, **kwargs).arguments.items()\r\n    def __init__(self, *args, **kwargs):\r\n        bound_args = self._get_bound_args(args, kwargs)\r\n        for slot, value in bound_args:\r\n            setattr(self, slot, value)\r\n    def __repr__(self):\r\n        return type(self).__name__ + repr(tuple(self))\r\n    def __iter__(self): \r\n        for name in self.__slots__:\r\n            yield getattr(self, name)\r\n    def __getitem__(self, index):\r\n        return getattr(self, self.__slots__[index])\r\n    def __len__(self):\r\n        return len(self.__slots__)\r\n\r\ndef sane_namedtuple(cls_name, attr_names):\r\n  \"Create a named tuple-like class with name cls_name and fields attr_names.\"\r\n  assert isinstance(attr_names, (list, tuple))\r\n  return type(cls_name,\r\n              (SaneNamedTuple,), \r\n              {\"__slots__\": tuple(attr_names)})\r\n```' by a COLLABORATOR of type User on 2019-03-18T06:31:15Z\n'I've also been really interested in dealing with inheritance and pytrees.  I've been inheriting pytree-conversion like this:\r\n\r\nhttps://github.com/NeilGirdhar/efax/blob/master/efax/tensors/tensor_like.py\r\n\r\nYou can see how it's used here:\r\n\r\nhttps://github.com/NeilGirdhar/efax/blob/630b73dadab01271f794119a6d41f5b6fce1bb66/efax/tensors/jax_generator.py#L34\r\n\r\nI have been using it with inheritance too.  When there is no extra information, the subclass inherits the pytree conversion.  When there are extra members, you can override the members, and have the overrides call super:\r\n\r\n```python\r\n    @classmethod\r\n    def from_tensor(cls, values, aux_data):\r\n        super_aux_data, distribution = aux_data\r\n        r = super().from_tensor(values, super_aux_data)\r\n        r.distribution = distribution\r\n        return r\r\n\r\n    def to_tensor(self):\r\n        values, aux_data = super().to_tensor()\r\n        return values, (aux_data, self.distribution)\r\n```\r\n\r\nOne of the advantages of this inheritance approach is that it supports `isinstance` checks, and you can use `TensorLike` in type annotations.  For example, I have the following type annotations:\r\n\r\n```python\r\nShape = Tuple[int, ...]\r\nShapeLike = Union[int, Sequence[int]]\r\nSliceLike = Tuple[Union[int, None, slice], ...]\r\nTensor = Union[np.ndarray, jnp.ndarray]\r\nPyTree = Union[Tensor, 'TensorLike', Container['PyTree']]\r\n```' by a CONTRIBUTOR of type User on 2020-04-20T07:51:36Z\n'I like this a lot! I was unaware of `__init_subclass__`; that's a pretty cool feature.' by a CONTRIBUTOR of type User on 2020-04-21T03:01:49Z\n'I thought I would throw up an issue since it would be nice to have https://github.com/google/jax/issues/2916' by a CONTRIBUTOR of type User on 2020-05-01T13:29:43Z\n'Do we still want to merge this or something along these lines, or should we close this PR?' by a COLLABORATOR of type User on 2020-06-24T15:12:57Z\n\nPR has review comments:\n'Is `register_pytree_leaf` just optional?\r\n\r\nIt seems like it might be redundant: if we just have node types registered, then everything that isn't registered is a leaf. If we register both leaves and nodes, then in principle now unregistered things are a third thing (?), since they're neither a leaf nor a node.\r\n\r\nI might be misunderstanding though. Can you explain the logic?\r\n\r\nIf indeed registering leaves is optional and does potentially break the current \"registered = node, unregistered = leaf\" dichotomy, then maybe we can remove it. What do you think?' by a COLLABORATOR of type User on 2019-03-24T01:19:49Z\n'Wonderful docstring, thank you!' by a COLLABORATOR of type User on 2019-03-24T01:21:03Z\n'Accessing `pytype.__mro__` feels kind of dangerous, like we're going from something really simple and minimal to something that knows a lot about Python inheritance. Could this be fragile? Are there alternatives? For example, it seems like using `isinstance` on all the registered node types might be an alternative; I'm not saying that's better, but I'd like to think through the solution space.\r\n\r\nI'm also interested if this has a big impact on performance. An advantage of the `type`-keyed dict mechanism is that it should be quite fast. I'm not saying the performance _is_ much different here, or that it's actually an important difference, but we should put some thought into that. What do you think?' by a COLLABORATOR of type User on 2019-03-24T01:24:55Z\n'Can we not have a default value here, or else leave the default to `False` so that previous behavior is maintained? In general I think default values can be bad and it's better to force the call site to be explicit.' by a COLLABORATOR of type User on 2019-03-24T01:25:47Z\n'Instead of replacing the previous test, can you add a new one? It can just copy and paste things from the existing test. The more tests, the better!' by a COLLABORATOR of type User on 2019-03-24T01:26:14Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 13,
        "num_review_comments": 5,
        "reason_for_closure": "Here are some concise reasons for closure, similar to the examples:\n\n* **Inconsistent:** Deviates from established JAX behavior as a NumPy superset.\n* **Disruptive:** Deprecates existing functionality without sufficient benefit.\n* **Resolved:** Underlying issue fixed in newer dependency version.\n* **Erroneous:** PR introduced an API visibility issue. \n* **Redundant:** PR addressed an issue that's already working. \n* **Miscommunication:** Author misunderstood the requested change.\n* **Withdrawn:** Author abandoned the PR due to related progress elsewhere.\n* **Unnecessary:** Functionality achievable via alternative approach. \n* **Out of Scope:** PR introduced an issue that is out of scope.\n* **Incomplete:** Further work and discussion necessary."
    },
    {
        "summary": "Pull Request '496' titled 'Add nanmean to lax_numpy' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-03-11T15:24:09Z, and was closed at 2019-09-21T08:17:17Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'Adding np.nanmean since it is listed as un-implemented in https://github.com/google/jax/issues/70'\nPR has comments:\n'\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F496) for more info**.\n\n<!-- need_sender_cla -->' by a COLLABORATOR of type User on 2019-03-11T15:24:21Z\n'I signed it!' by a CONTRIBUTOR of type User on 2019-03-11T15:25:51Z\n'CLAs look good, thanks!\n\nℹ️ **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Fgoogle%2Fjax%2Fpull%2F496) for more info**.\n\n<!-- ok -->' by a COLLABORATOR of type User on 2019-03-11T15:25:55Z\n'@hawkinsp One question on the test, it doesn't appear any of the other nan-functions currently have tests either (I assume due to the fact they will fail in CI due to fastmath). \r\n\r\nShould I create a NaN test function similar to how `testReducer` currently works and test all these functions with this? That seems like the appropriate thing to do to me, but not sure if you want that in this PR.' by a CONTRIBUTOR of type User on 2019-03-14T10:44:41Z\n'Yes, you're right, the other NaN functions are also missing tests for that reason. However, fixing it isn't hard.\r\n\r\nI'm hoping we can turn on correct-by-default NaN handling this week, and then the issue will be moot. Until then, you could just add this code to the test Python program, perhaps somewhere at the start:\r\n\r\n```\r\nimport os\r\nos.environ['XLA_FLAGS'] = '--xla_cpu_enable_fast_math=false'\r\n```' by a COLLABORATOR of type User on 2019-03-14T13:43:06Z\n\nPR has review comments:\n'Nit: you can't refer to `np` here, since this file *is* the implementation of the numpy API. You can sometimes refer to `onp`, which is classic NumPy, but that's not what you want to do here. I think you just meant to write `count_nonzero` with no `np.` prefix.\r\n\r\n' by a COLLABORATOR of type User on 2019-03-12T00:55:22Z\n'Second: using a Python branch on a value like `np.count_nonzero` won't work inside a `jit` decorator, because the value of `a` will be abstracted to a ShapedArray. Branches on abstract values currently have to be done using `lax.select`/`np.where` or `lax.cond`. But you don't need to branch here at all — just follow the algorithm I outlined below.' by a COLLABORATOR of type User on 2019-03-12T01:36:22Z\n'I don't think the logic here is quite correct. Here, you're performing a bool sum reduction to test whether there exists a nan anywhere in the input array. `dtype=bool_` means you get a bool result, i.e., whether there is a nan present or not. If not, you do a regular `mean`. If so, you make a correction to the normalizer. However, that correction is looks like it is either 1 or 0, which won't handle the case where there is more than 1 nan in the input array.\r\n\r\nI think we can simplify this a little bit.\r\n\r\nI would have guessed that the right algorithm here would be to:\r\na) count the number of non-nan values — you can do this something like:\r\n```\r\nnormalizer = sum(logical_not(isnan(a)), axis=axis, dtype=int32, keepdims=keepdims)\r\n```\r\n\r\nb) divide the sum of the non-nan values by the non-nan count?\r\n```\r\nnormalizer = lax.convert_element_type(normalizer, dtype)\r\noutput = lax.div(nansum(a, axis=axis, dtype=dtype, keepdims=keepdims), normalizer)\r\n```\r\n?\r\n\r\n(That's may not be precisely right, especially with respect to type promotion but you get the general idea.)' by a COLLABORATOR of type User on 2019-03-12T01:39:50Z\n'Whoops on the NP front I implemented this function originally in a notebook for my own purposes, and forgot to remove them all. Interesting on the other front, I tested this a good amount but not wrapped in JIT, so thanks for informing me of those side-effects!' by a CONTRIBUTOR of type User on 2019-03-14T10:11:24Z\n'Yeah, that algorithm looks right! I'll throw together this with some tests now.' by a CONTRIBUTOR of type User on 2019-03-14T10:11:58Z\n'If we're going to test and special case integer and bool values anyway, we can simplify this even more. Why not just  move this test to the top of the function, and delegate to `mean` if the input is bool or integer? These types cannot be `NaN`.' by a COLLABORATOR of type User on 2019-03-14T13:40:07Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 5,
        "num_review_comments": 6,
        "reason_for_closure": "Inconsistent :- PR closed as the proposed change would break existing users with little benefit."
    },
    {
        "summary": "Pull Request '393' titled 'override __new__ in dtype classes' was authored by a User, who is associated as a COLLABORATOR. \nIt was created at 2019-02-16T06:37:50Z, and was closed at 2019-02-17T00:07:48Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'fixes #367 '\nPR has comments:\n'I'm glad these tests started failing, because actually I think this solution is fraught with peril! We'd need to support equality testing on these dtype / scalar classes, and make that satisfy e.g. `np.float64 == np.dtype('float64')`. I don't understand all the issues here, so I'm just going to kill this PR. ' by a COLLABORATOR of type User on 2019-02-17T00:07:48Z\n\nPR has review comments:\n'Do you plan to do the float/complex types, too? I would have expected them to have the same issue.' by a COLLABORATOR of type User on 2019-02-16T13:10:43Z\n'You’re right, I’ll do that. ' by a COLLABORATOR of type User on 2019-02-16T14:41:40Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as infeasible: introduces inconsistencies with NumPy and deprecates existing functionality."
    },
    {
        "summary": "Pull Request '269' titled 'add multivariate_normal, change api of jax.random' was authored by a User, who is associated as a CONTRIBUTOR. \nIt was created at 2019-01-19T19:00:27Z, and was closed at 2019-10-04T06:23:10Z by a User.\nThe PR has labels: cla: yes - . \nIt has a body of 'I've just added the multivariate normal sampler, while I'd love to hear about your opinions on the API, specifically on the order of the random samplers' arguments. I think the general rule for this in `np.random` is roughly `np.random.sampler(distribution parameters, shape, dtype)`. For example, they have `np.random.normal(loc=0.0, scale=1.0, size=None)` and `random.randint(low, high=None, size=None, dtype='l')`.\r\n\r\nFollowing the discussion at #260 , we cannot make `jax.random`'s API the same as `np.random`'s, since we need to explicitly specify the key. In this context, I'd suggest we use `jax.random.sampler(key, distribution parameters, shape, dtype)`, i.e. to put the key at the beginning and keep the others similar to `np.random`.\r\n\r\nI've changed the API in this way in the PR. If you agree with this, I will continue to check and modify the examples and tests, since there're things like `jax.random.normal(key, shape)`, where the positional arguments become invalid under the new API. I think this should be the reason why Travis CI failed currently. Or please tell me if you have different attitudes on the API.'\nPR has comments:\n'Closing in favor of #1389. Thanks for the PR, and sorry for letting it languish!' by a CONTRIBUTOR of type User on 2019-10-04T06:23:10Z\n\nPR has review comments:\n'I think that we can keep the current behaviour (without `loc`, `scale`). Many samplers can be transformed by `x -> loc * x + scale` so it is unnecessary to add `loc`, `scale` for all of them.' by a CONTRIBUTOR of type User on 2019-03-30T21:24:09Z\n'I guess this is `np.dot(L, vector)` instead of `np.dot(vector, L.T)`? In addition, we can simplify the implementation by taking an `eps = normal(key, shape, dtype)` instead of reimplementing.' by a CONTRIBUTOR of type User on 2019-03-30T21:32:42Z",
        "has_locked_reason": false,
        "merged": false,
        "num_comments": 1,
        "num_review_comments": 2,
        "reason_for_closure": "Closed as unnecessary: JAX behavior is correct and NumPy's limitation shouldn't be replicated."
    }
]